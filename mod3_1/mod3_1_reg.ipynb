{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do one model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try some regularization \n",
    "\n",
    "I thought I dialed in good a C but it did not score better on the holdout.  But I overdid it. __C=0.01 improves things considerably; C=0.10 is even better__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here are the defaults for LogisticRegression:\n",
    "```\n",
    "LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, \n",
    "                   class_weight=None, random_state=None, solver='liblinear', max_iter=100, multi_class='ovr', \n",
    "                   verbose=0, warm_start=False, n_jobs=1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Imports/setup\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "pd.set_option('display.max_columns', 60)\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "# for the pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "# for the selectors\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler\n",
    "# for gluing preprocessed text and numbers together\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "# for nans in the numeric data\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "# Import classifiers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# Import CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer\n",
    "\n",
    "# metrics\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
    "\n",
    "# unflattener\n",
    "import python.flat_to_labels as ftl\n",
    "\n",
    "#### Set up a train-test split making sure we have all labels in both splits\n",
    "from python.multilabel import multilabel_train_test_split\n",
    "\n",
    "from python.dd_mmll import multi_multi_log_loss, BOX_PLOTS_COLUMN_INDICES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Function</th>\n",
       "      <th>Use</th>\n",
       "      <th>Sharing</th>\n",
       "      <th>Reporting</th>\n",
       "      <th>Student_Type</th>\n",
       "      <th>Position_Type</th>\n",
       "      <th>Object_Type</th>\n",
       "      <th>Pre_K</th>\n",
       "      <th>Operating_Status</th>\n",
       "      <th>Object_Description</th>\n",
       "      <th>Text_2</th>\n",
       "      <th>SubFund_Description</th>\n",
       "      <th>Job_Title_Description</th>\n",
       "      <th>Text_3</th>\n",
       "      <th>Text_4</th>\n",
       "      <th>Sub_Object_Description</th>\n",
       "      <th>Location_Description</th>\n",
       "      <th>FTE</th>\n",
       "      <th>Function_Description</th>\n",
       "      <th>Facility_or_Department</th>\n",
       "      <th>Position_Extra</th>\n",
       "      <th>Total</th>\n",
       "      <th>Program_Description</th>\n",
       "      <th>Fund_Description</th>\n",
       "      <th>Text_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>134338</th>\n",
       "      <td>Teacher Compensation</td>\n",
       "      <td>Instruction</td>\n",
       "      <td>School Reported</td>\n",
       "      <td>School</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>Teacher</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>PreK-12 Operating</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Teacher-Elementary</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>KINDERGARTEN</td>\n",
       "      <td>50471.810</td>\n",
       "      <td>KINDERGARTEN</td>\n",
       "      <td>General Fund</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206341</th>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>Non-Operating</td>\n",
       "      <td>CONTRACTOR SERVICES</td>\n",
       "      <td>BOND EXPENDITURES</td>\n",
       "      <td>BUILDING FUND</td>\n",
       "      <td>(blank)</td>\n",
       "      <td>Regular</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RGN  GOB</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UNDESIGNATED</td>\n",
       "      <td>3477.860</td>\n",
       "      <td>BUILDING IMPROVEMENT SERVICES</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BUILDING IMPROVEMENT SERVICES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326408</th>\n",
       "      <td>Teacher Compensation</td>\n",
       "      <td>Instruction</td>\n",
       "      <td>School Reported</td>\n",
       "      <td>School</td>\n",
       "      <td>Unspecified</td>\n",
       "      <td>Teacher</td>\n",
       "      <td>Base Salary/Compensation</td>\n",
       "      <td>Non PreK</td>\n",
       "      <td>PreK-12 Operating</td>\n",
       "      <td>Personal Services - Teachers</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TCHER 2ND GRADE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Regular Instruction</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TEACHER</td>\n",
       "      <td>62237.130</td>\n",
       "      <td>Instruction - Regular</td>\n",
       "      <td>General Purpose School</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364634</th>\n",
       "      <td>Substitute Compensation</td>\n",
       "      <td>Instruction</td>\n",
       "      <td>School Reported</td>\n",
       "      <td>School</td>\n",
       "      <td>Unspecified</td>\n",
       "      <td>Substitute</td>\n",
       "      <td>Benefits</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>PreK-12 Operating</td>\n",
       "      <td>EMPLOYEE BENEFITS</td>\n",
       "      <td>TEACHER SUBS</td>\n",
       "      <td>GENERAL FUND</td>\n",
       "      <td>Teacher, Short Term Sub</td>\n",
       "      <td>Regular</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UNALLOC BUDGETS/SCHOOLS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PROFESSIONAL-INSTRUCTIONAL</td>\n",
       "      <td>22.300</td>\n",
       "      <td>GENERAL MIDDLE/JUNIOR HIGH SCH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>REGULAR INSTRUCTION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47683</th>\n",
       "      <td>Substitute Compensation</td>\n",
       "      <td>Instruction</td>\n",
       "      <td>School Reported</td>\n",
       "      <td>School</td>\n",
       "      <td>Unspecified</td>\n",
       "      <td>Teacher</td>\n",
       "      <td>Substitute Compensation</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>PreK-12 Operating</td>\n",
       "      <td>TEACHER COVERAGE FOR TEACHER</td>\n",
       "      <td>TEACHER SUBS</td>\n",
       "      <td>GENERAL FUND</td>\n",
       "      <td>Teacher, Secondary (High)</td>\n",
       "      <td>Alternative</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NON-PROJECT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PROFESSIONAL-INSTRUCTIONAL</td>\n",
       "      <td>54.166</td>\n",
       "      <td>GENERAL HIGH SCHOOL EDUCATION</td>\n",
       "      <td>NaN</td>\n",
       "      <td>REGULAR INSTRUCTION</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Function          Use          Sharing Reporting  \\\n",
       "134338     Teacher Compensation  Instruction  School Reported    School   \n",
       "206341                 NO_LABEL     NO_LABEL         NO_LABEL  NO_LABEL   \n",
       "326408     Teacher Compensation  Instruction  School Reported    School   \n",
       "364634  Substitute Compensation  Instruction  School Reported    School   \n",
       "47683   Substitute Compensation  Instruction  School Reported    School   \n",
       "\n",
       "       Student_Type Position_Type               Object_Type     Pre_K  \\\n",
       "134338     NO_LABEL       Teacher                  NO_LABEL  NO_LABEL   \n",
       "206341     NO_LABEL      NO_LABEL                  NO_LABEL  NO_LABEL   \n",
       "326408  Unspecified       Teacher  Base Salary/Compensation  Non PreK   \n",
       "364634  Unspecified    Substitute                  Benefits  NO_LABEL   \n",
       "47683   Unspecified       Teacher   Substitute Compensation  NO_LABEL   \n",
       "\n",
       "         Operating_Status            Object_Description             Text_2  \\\n",
       "134338  PreK-12 Operating                           NaN                NaN   \n",
       "206341      Non-Operating           CONTRACTOR SERVICES  BOND EXPENDITURES   \n",
       "326408  PreK-12 Operating  Personal Services - Teachers                NaN   \n",
       "364634  PreK-12 Operating             EMPLOYEE BENEFITS       TEACHER SUBS   \n",
       "47683   PreK-12 Operating  TEACHER COVERAGE FOR TEACHER       TEACHER SUBS   \n",
       "\n",
       "       SubFund_Description       Job_Title_Description       Text_3  \\\n",
       "134338                 NaN         Teacher-Elementary           NaN   \n",
       "206341       BUILDING FUND                     (blank)      Regular   \n",
       "326408                 NaN             TCHER 2ND GRADE          NaN   \n",
       "364634        GENERAL FUND    Teacher, Short Term Sub       Regular   \n",
       "47683         GENERAL FUND  Teacher, Secondary (High)   Alternative   \n",
       "\n",
       "                     Text_4 Sub_Object_Description Location_Description  FTE  \\\n",
       "134338                  NaN                    NaN                  NaN  1.0   \n",
       "206341                  NaN                    NaN                  NaN  NaN   \n",
       "326408  Regular Instruction                    NaN                  NaN  1.0   \n",
       "364634                  NaN                    NaN                  NaN  NaN   \n",
       "47683                   NaN                    NaN                  NaN  NaN   \n",
       "\n",
       "           Function_Description Facility_or_Department  \\\n",
       "134338                      NaN                    NaN   \n",
       "206341                 RGN  GOB                    NaN   \n",
       "326408                      NaN                    NaN   \n",
       "364634  UNALLOC BUDGETS/SCHOOLS                    NaN   \n",
       "47683               NON-PROJECT                    NaN   \n",
       "\n",
       "                    Position_Extra      Total             Program_Description  \\\n",
       "134338               KINDERGARTEN   50471.810                    KINDERGARTEN   \n",
       "206341                UNDESIGNATED   3477.860   BUILDING IMPROVEMENT SERVICES   \n",
       "326408                     TEACHER  62237.130           Instruction - Regular   \n",
       "364634  PROFESSIONAL-INSTRUCTIONAL     22.300  GENERAL MIDDLE/JUNIOR HIGH SCH   \n",
       "47683   PROFESSIONAL-INSTRUCTIONAL     54.166   GENERAL HIGH SCHOOL EDUCATION   \n",
       "\n",
       "              Fund_Description                         Text_1  \n",
       "134338            General Fund                            NaN  \n",
       "206341                     NaN  BUILDING IMPROVEMENT SERVICES  \n",
       "326408  General Purpose School                            NaN  \n",
       "364634                     NaN            REGULAR INSTRUCTION  \n",
       "47683                      NaN            REGULAR INSTRUCTION  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get data\n",
    "the_data = pd.read_csv('data/TrainingData.csv', index_col=0)\n",
    "\n",
    "# take a look\n",
    "the_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Encode the targets as categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function            category\n",
      "Object_Type         category\n",
      "Operating_Status    category\n",
      "Position_Type       category\n",
      "Pre_K               category\n",
      "Reporting           category\n",
      "Sharing             category\n",
      "Student_Type        category\n",
      "Use                 category\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "### bind variable LABELS - these are actually the targets and we're going to one-hot encode them...\n",
    "LABELS = ['Function',  'Use',  'Sharing',  'Reporting',  'Student_Type',  'Position_Type', \n",
    "          'Object_Type',  'Pre_K',  'Operating_Status']\n",
    "\n",
    "### This turns out to be key.  Submission requires the dummy versions of these vars to be in this order.\n",
    "LABELS.sort()\n",
    "\n",
    "# Define the lambda function: categorize_label\n",
    "categorize_label = lambda x: x.astype('category')\n",
    "\n",
    "# Convert df[LABELS] to a categorical type\n",
    "the_data[LABELS] = the_data[LABELS].apply(categorize_label, axis=0)\n",
    "\n",
    "# Print the converted dtypes\n",
    "print(the_data[LABELS].dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#### Save the unique labels for each output (category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Instruction',\n",
       " 'NO_LABEL',\n",
       " 'O&M',\n",
       " 'Pupil Services & Enrichment',\n",
       " 'ISPD',\n",
       " 'Leadership',\n",
       " 'Business Services',\n",
       " 'Untracked Budget Set-Aside']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build a dictionary\n",
    "the_labels = {col : the_data[col].unique().tolist() for col in the_data[LABELS].columns}\n",
    "# take a look at one entry\n",
    "the_labels['Use']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change fraction to suit.\n",
    "Note: small fractions will have a hard time ensuring labels in both splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downsize it or not\n",
    "# df = the_data.sample(frac=0.10, random_state=777) # this seed gets a split with enough labels in both sets\n",
    "df = the_data.sample(frac=1.0, random_state=777)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get targets as set of one-hot encoded columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name these columns\n",
    "NUMERIC_COLUMNS = ['FTE', 'Total']\n",
    "\n",
    "# Get labels and convert to dummy variables: label_dummies\n",
    "label_dummies = pd.get_dummies(df[LABELS])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up a train-test split  for modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ======================== Begin Mod3_1  (add bigrams) with regularization ==================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some things to note about the default CountVectorizer and HashingVectorizer:\n",
    "1. All strings are downcased\n",
    "2. The default setting selects tokens of 2 or more alphanumeric characters (punctuation is completely ignored and always treated as a token separator).  This means single letter or digit tokens are ignored.\n",
    "3. If the vectorizer is used to transform another input (e.g. test), any tokens not in the original corpus are ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define combine_text_columns()\n",
    "def combine_text_columns(df, to_drop=NUMERIC_COLUMNS + LABELS):\n",
    "    \"\"\" converts all text columns in each row of df to single string \"\"\"\n",
    "    # Drop non-text columns that are in the df\n",
    "    to_drop = set(to_drop) & set(df.columns.tolist())\n",
    "    text_data = df.drop(to_drop, axis=1)  \n",
    "    # Replace nans with blanks\n",
    "    text_data.fillna('', inplace=True)    \n",
    "    # Join all text items in a row that have a space in between\n",
    "    return text_data.apply(lambda x: \" \".join(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import FunctionTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# Get the dummy encoding of the labels\n",
    "dummy_labels = pd.get_dummies(df[LABELS])\n",
    "\n",
    "# Get the features in the data\n",
    "NON_LABELS = [c for c in df.columns if c not in LABELS]\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = multilabel_train_test_split(df[NON_LABELS],\n",
    "                                                               dummy_labels,\n",
    "                                                               0.2, \n",
    "                                                               seed=123)\n",
    "# Preprocess the text data: get_text_data\n",
    "get_text_data = FunctionTransformer(combine_text_columns, validate=False)\n",
    "\n",
    "# Use all 0s instead of noise: get_numeric_data\n",
    "get_numeric_data_hack = FunctionTransformer(lambda x: np.zeros(x[NUMERIC_COLUMNS].shape, dtype=np.float), validate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit time: 220.05 seconds\n"
     ]
    }
   ],
   "source": [
    "#### Build the pipeline\n",
    "mod_reg = Pipeline([\n",
    "        ('union', FeatureUnion(\n",
    "            transformer_list = [\n",
    "                ('numeric_features', get_numeric_data_hack),\n",
    "                ('text_features', Pipeline([('selector', get_text_data),\n",
    "                                            ('vectorizer', CountVectorizer(ngram_range=(1,2)))]))\n",
    "             ])),\n",
    "    # no scaler here  \n",
    "    ('clf', OneVsRestClassifier(LogisticRegression(C=0.0009), n_jobs=-1))\n",
    "    ])\n",
    "\n",
    "start = timer()\n",
    "# Fit to the training data\n",
    "mod_reg.fit(X_train, y_train)\n",
    "end = timer()\n",
    "print('fit time: {:0.2f} seconds'.format(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### regularization results on 1/10 of the data\n",
    "\n",
    "* C=1: 43 sec; train: 0.0522 test: 0.1074\n",
    "* C=0.5: 39 sec; train: 0.0678 test: 0.1162\n",
    "* C=0.75: 41 sec; train: 0.0580 test: 0.1103\n",
    "* C=0.25:  34 sec; train: 0.0897 test: 0.1318\n",
    "* C=0.125: 31 sec; train: 0.1195 test: 0.1562\n",
    "* C=0.0625 28 sec; train: 0.1589 test: 0.1913\n",
    "* C=0.030 27 sec; train: 0.2131 test: 0.2421\n",
    "* C=0.016 24 sec; train: 0.2721 test: 0.2987\n",
    "* C=0.010 23 sec; train: 0.3250  test: 0.3502\n",
    "* C=0.0125 24 sec; train: 0.2989  test: 0.3247\n",
    "\n",
    "#### regularization results on all the data, C=0.0125\n",
    "log loss on training set: 0.1386, log loss on test set: 0.1412\n",
    "\n",
    "#### regularization results on all the data, C=0.0100, 329sec\n",
    "log loss on training set: 0.1386, log loss on test set: 0.1412\n",
    "\n",
    "#### regularization results on all the data, C=0.0050, 282 sec\n",
    "log loss on training set: 0.1897 log loss on test set: 0.1921\n",
    "\n",
    "#### regularization results on all the data, C=0.0050, 282 sec\n",
    "log loss on training set: 0.1897 log loss on test set: 0.1921\n",
    "\n",
    "#### regularization results on all the data, C=0.0010, 219 sec\n",
    "log loss on training set: 0.3366 log loss on test set: 0.3386\n",
    "\n",
    "#### regularization results on all the data, C=0.0030, 265 sec\n",
    "log loss on training set: 0.0.2274 log loss on test set: 0.2297\n",
    "\n",
    "#### regularization results on all the data, C=0.0001, 159 sec\n",
    "log loss on training set: 0.7229 log loss on test set: 0.7237\n",
    "\n",
    "#### regularization results on all the data, C=0.0005, 203 sec\n",
    "log loss on training set: 0.4291 log loss on test set: 0.4308\n",
    "\n",
    "#### regularization results on all the data, C=0.0006, 210 sec\n",
    "log loss on training set: 0.4028 log loss on test set: 0.4046\n",
    "\n",
    "#### regularization results on all the data, C=0.0007, 210 sec\n",
    "log loss on training set: 0.4028 log loss on test set: 0.4046\n",
    "\n",
    "#### regularization results on all the data, C=0.0008, 214 sec\n",
    "log loss on training set: 0.3642 log loss on test set: 0.3661\n",
    "\n",
    "### regularization results on all the data, C=0.0009, 214 sec  - this is the thing to submit...\n",
    "```\n",
    "log loss on training set: 0.3494 log loss on test set: 0.3514;\n",
    "log loss on training set: 0.3493, log loss on test set: 0.3518; rs=999\n",
    "log loss on training set: 0.3489, log loss on test set: 0.3534; rs=123\n",
    "log loss on training set: 0.3497, log loss on test set: 0.3504, rs=1111\n",
    "log loss on training set: 0.3491, log loss on test set: 0.3527; rs=5851\n",
    "log loss on training set: 0.3499, log loss on test set: 0.3494; rs=2559\n",
    "log loss on training set: 0.3495, log loss on test set: 0.3512; rs=3097\n",
    "log loss on training set: 0.3495, log loss on test set: 0.0.3514; rs=20001207\n",
    "log loss on training set: 0.3495, log loss on test set: 0.0.3514; rs=37\n",
    "log loss on training set: 0.3494, log loss on test set: 0.3514; rs=777\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For log loss we need the probabilities, not the predicted labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict.proba time: 24.95 seconds\n"
     ]
    }
   ],
   "source": [
    "# get probas\n",
    "start = timer()\n",
    "mod_reg_train_probas = mod_reg.predict_proba(X_train)\n",
    "mod_reg_test_probas = mod_reg.predict_proba(X_test)\n",
    "end = timer()\n",
    "print('Predict.proba time: {:0.2f} seconds'.format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log loss on training set: 0.3494\n",
      "log loss on test set: 0.3514\n"
     ]
    }
   ],
   "source": [
    "print('log loss on training set: {:0.4f}'.format(multi_multi_log_loss(mod_reg_train_probas, \n",
    "                                                                      y_train.values, BOX_PLOTS_COLUMN_INDICES)))\n",
    "print('log loss on test set: {:0.4f}'.format(multi_multi_log_loss(mod_reg_test_probas, \n",
    "                                                                      y_test.values, BOX_PLOTS_COLUMN_INDICES)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_f1(true, pred):\n",
    "    the_scores = []\n",
    "    for target in range(len(LABELS)):\n",
    "        the_score = f1_score(true[:, target], pred[:, target], average='weighted')\n",
    "        print('F1 score for target {}: {:.3f}'.format(LABELS[target], the_score))\n",
    "        the_scores.append(the_score)\n",
    "    print('Average F1 score for all targets : {:.3f}'.format(np.mean(the_scores)))\n",
    "\n",
    "def report_accuracy(true, pred):\n",
    "    the_scores = []\n",
    "    for target in range(len(LABELS)):\n",
    "        the_score = accuracy_score(true[:, target], pred[:, target])\n",
    "        print('Accuracy score for target {}: {:.3f}'.format(LABELS[target], the_score))\n",
    "        the_scores.append(the_score)\n",
    "    print('Average accuracy score for all targets : {:.3f}'.format(np.mean(the_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ftl wants ndarray, not pd.Dataframe\n",
    "the_ys = ftl.flat_to_labels(y_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saus\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for target Function: 0.849\n",
      "F1 score for target Object_Type: 0.939\n",
      "F1 score for target Operating_Status: 0.966\n",
      "F1 score for target Position_Type: 0.911\n",
      "F1 score for target Pre_K: 0.975\n",
      "F1 score for target Reporting: 0.934\n",
      "F1 score for target Sharing: 0.899\n",
      "F1 score for target Student_Type: 0.933\n",
      "F1 score for target Use: 0.889\n",
      "Average F1 score for all targets : 0.922\n",
      "Accuracy score for target Function: 0.861\n",
      "Accuracy score for target Object_Type: 0.941\n",
      "Accuracy score for target Operating_Status: 0.968\n",
      "Accuracy score for target Position_Type: 0.916\n",
      "Accuracy score for target Pre_K: 0.976\n",
      "Accuracy score for target Reporting: 0.935\n",
      "Accuracy score for target Sharing: 0.904\n",
      "Accuracy score for target Student_Type: 0.935\n",
      "Accuracy score for target Use: 0.894\n",
      "Average accuracy score for all targets : 0.925\n"
     ]
    }
   ],
   "source": [
    "report_f1(the_ys, ftl.flat_to_labels(mod_reg_test_probas))\n",
    "\n",
    "report_accuracy(the_ys, ftl.flat_to_labels(mod_reg_test_probas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### =========================== predict on holdout set =================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saus\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2785: DtypeWarning: Columns (5,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# Load the holdout data: holdout\n",
    "### Over here the file is TestData.csv\n",
    "holdout = pd.read_csv('data/TestData.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict time: 3.068298013462197 seconds\n"
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "# Generate predictions: predictions\n",
    "mod_reg_predictions = mod_reg.predict_proba(holdout)\n",
    "end = timer()\n",
    "print('predict time: {} seconds'.format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_mod_reg = pd.DataFrame(columns=pd.get_dummies(df[LABELS], prefix_sep='__').columns, \n",
    "                             index=holdout.index,\n",
    "                             data=mod_reg_predictions)\n",
    "\n",
    "pred_mod_reg.to_csv('pred_mod_reg.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ====================== End of mod_reg; score: 0.6943 ============================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  ============ Try another submission with C=0.01.  I might have overdone it. ========================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit time: 368.27 seconds\n"
     ]
    }
   ],
   "source": [
    "#### Build the pipeline\n",
    "mod_reg_01 = Pipeline([\n",
    "        ('union', FeatureUnion(\n",
    "            transformer_list = [\n",
    "                ('numeric_features', get_numeric_data_hack),\n",
    "                ('text_features', Pipeline([('selector', get_text_data),\n",
    "                                            ('vectorizer', CountVectorizer(ngram_range=(1,2)))]))\n",
    "             ])),\n",
    "        ('clf', OneVsRestClassifier(LogisticRegression(C=0.01), n_jobs=-1))\n",
    "    ])\n",
    "\n",
    "start = timer()\n",
    "# Fit to the training data\n",
    "mod_reg_01.fit(X_train, y_train)\n",
    "end = timer()\n",
    "print('fit time: {:0.2f} seconds'.format(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### =========================== predict on holdout set =================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saus\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2785: DtypeWarning: Columns (5,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# Load the holdout data: holdout\n",
    "### Over here the file is TestData.csv\n",
    "holdout = pd.read_csv('data/TestData.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict time: 2.7244055672565537 seconds\n"
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "# Generate predictions: predictions\n",
    "mod_reg_01_predictions = mod_reg_01.predict_proba(holdout)\n",
    "end = timer()\n",
    "print('predict time: {} seconds'.format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_mod_reg_01 = pd.DataFrame(columns=pd.get_dummies(df[LABELS], prefix_sep='__').columns, \n",
    "                             index=holdout.index,\n",
    "                             data=mod_reg_01_predictions)\n",
    "\n",
    "pred_mod_reg_01.to_csv('pred_mod_reg_01.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  ============ end mod_reg_tenth.  0.5400: 4th place ========================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  ============ Try another submission with C=0.1, reg_mod_tenth.  I might have overdone it. ========================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit time: 506.39 seconds\n"
     ]
    }
   ],
   "source": [
    "#### Build the pipeline\n",
    "mod_reg_tenth = Pipeline([\n",
    "        ('union', FeatureUnion(\n",
    "            transformer_list = [\n",
    "                ('numeric_features', get_numeric_data_hack),\n",
    "                ('text_features', Pipeline([('selector', get_text_data),\n",
    "                                            ('vectorizer', CountVectorizer(ngram_range=(1,2)))]))\n",
    "             ])),\n",
    "        ('clf', OneVsRestClassifier(LogisticRegression(C=0.1), n_jobs=-1))\n",
    "    ])\n",
    "\n",
    "start = timer()\n",
    "# Fit to the training data\n",
    "mod_reg_tenth.fit(X_train, y_train)\n",
    "end = timer()\n",
    "print('fit time: {:0.2f} seconds'.format(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### =========================== predict on holdout set =================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saus\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2785: DtypeWarning: Columns (5,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# Load the holdout data: holdout\n",
    "### Over here the file is TestData.csv\n",
    "holdout = pd.read_csv('data/TestData.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50064, 16), (400277, 25))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "holdout.shape, the_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict time: 4.0949001371336635 seconds\n"
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "# Generate predictions: predictions\n",
    "mod_reg_tenth_predictions = mod_reg_tenth.predict_proba(holdout)\n",
    "end = timer()\n",
    "print('predict time: {} seconds'.format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_mod_reg_tenth = pd.DataFrame(columns=pd.get_dummies(df[LABELS], prefix_sep='__').columns, \n",
    "                             index=holdout.index,\n",
    "                             data=mod_reg_tenth_predictions)\n",
    "\n",
    "pred_mod_reg_tenth.to_csv('pred_mod_reg_tenth.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  ============ end mod_reg_tenth 0.5341.   4th  ========================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  ============ Try another submission with C=0.33.    not run yet... ========================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note: typo here: s.b. C=0.3333; separate file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit time: 354.80 seconds\n"
     ]
    }
   ],
   "source": [
    "#### Build the pipeline\n",
    "mod_reg_third = Pipeline([\n",
    "        ('union', FeatureUnion(\n",
    "            transformer_list = [\n",
    "                ('numeric_features', get_numeric_data_hack),\n",
    "                ('text_features', Pipeline([('selector', get_text_data),\n",
    "                                            ('vectorizer', CountVectorizer(ngram_range=(1,2)))]))\n",
    "             ])),\n",
    "        ('clf', OneVsRestClassifier(LogisticRegression(C=0.0333), n_jobs=-1))\n",
    "    ])\n",
    "\n",
    "start = timer()\n",
    "# Fit to the training data\n",
    "mod_reg_third.fit(X_train, y_train)\n",
    "end = timer()\n",
    "print('fit time: {:0.2f} seconds'.format(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### =========================== predict on holdout set =================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saus\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2785: DtypeWarning: Columns (5,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# Load the holdout data: holdout\n",
    "### Over here the file is TestData.csv\n",
    "holdout = pd.read_csv('data/TestData.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50064, 16), (400277, 25))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "holdout.shape, the_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict time: 4.815847237900016 seconds\n"
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "# Generate predictions: predictions\n",
    "mod_reg_third_predictions = mod_reg_third.predict_proba(holdout)\n",
    "end = timer()\n",
    "print('predict time: {} seconds'.format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_mod_reg_third = pd.DataFrame(columns=pd.get_dummies(df[LABELS], prefix_sep='__').columns, \n",
    "                             index=holdout.index,\n",
    "                             data=mod_reg_third_predictions)\n",
    "\n",
    "pred_mod_reg_third.to_csv('pred_mod_reg_third.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  =================== end mod_reg_third  ========================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Just for yucks let's try a test set that's a similar size to the hold out and see if scores get closer to holdout scores..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note: train/test has just been screwed with.  Need to fix if we need the old split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import FunctionTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# Get the dummy encoding of the labels\n",
    "dummy_labels = pd.get_dummies(df[LABELS])\n",
    "\n",
    "# Get the features in the data\n",
    "NON_LABELS = [c for c in df.columns if c not in LABELS]\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = multilabel_train_test_split(df[NON_LABELS],\n",
    "                                                               dummy_labels,\n",
    "                                                               0.2, \n",
    "                                                               seed=123)\n",
    "# Preprocess the text data: get_text_data\n",
    "get_text_data = FunctionTransformer(combine_text_columns, validate=False)\n",
    "\n",
    "# Use all 0s instead of noise: get_numeric_data\n",
    "get_numeric_data_hack = FunctionTransformer(lambda x: np.zeros(x[NUMERIC_COLUMNS].shape, dtype=np.float), validate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit time: 499.28 seconds\n"
     ]
    }
   ],
   "source": [
    "#### Build the pipeline\n",
    "mod_reg_tenth = Pipeline([\n",
    "        ('union', FeatureUnion(\n",
    "            transformer_list = [\n",
    "                ('numeric_features', get_numeric_data_hack),\n",
    "                ('text_features', Pipeline([('selector', get_text_data),\n",
    "                                            ('vectorizer', CountVectorizer(ngram_range=(1,2)))]))\n",
    "             ])),\n",
    "        ('clf', OneVsRestClassifier(LogisticRegression(C=0.1), n_jobs=-1))\n",
    "    ])\n",
    "\n",
    "start = timer()\n",
    "# Fit to the training data\n",
    "mod_reg_tenth.fit(X_train, y_train)\n",
    "end = timer()\n",
    "print('fit time: {:0.2f} seconds'.format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict.proba time: 24.92 seconds\n"
     ]
    }
   ],
   "source": [
    "# get probas\n",
    "start = timer()\n",
    "mod_reg_tenth_train_probas = mod_reg_tenth.predict_proba(X_train)\n",
    "mod_reg_tenth_test_probas = mod_reg_tenth.predict_proba(X_test)\n",
    "end = timer()\n",
    "print('Predict.proba time: {:0.2f} seconds'.format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log loss on training set: 0.0743\n",
      "log loss on test set: 0.0808\n"
     ]
    }
   ],
   "source": [
    "print('log loss on training set: {:0.4f}'.format(multi_multi_log_loss(mod_reg_tenth_train_probas, \n",
    "                                                                      y_train.values, BOX_PLOTS_COLUMN_INDICES)))\n",
    "print('log loss on test set: {:0.4f}'.format(multi_multi_log_loss(mod_reg_tenth_test_probas, \n",
    "                                                                      y_test.values, BOX_PLOTS_COLUMN_INDICES)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Moving to a 90/10 split doesn't seem to move results any closer to holdout results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
