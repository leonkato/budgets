{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rebuild the models from first_models.ipynb, but  write out train and test probabilities and values for each model to files so as not to need to re-fit these models.  We rename the classifiers from first_models.ipynb for consistency.\n",
    "\n",
    "#### These datafiles can then be used to expore different metrics for all the models (see standard_metrics.ipynb).\n",
    "\n",
    "#### The predictions on the holdout set are already saved (also renamed for consistency). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifier names\n",
    "\n",
    "| name in first_models | name in this file  | comment\n",
    "|----------------------|--------------------|----------------\n",
    "|  clf                 |  mod0      |  numerical features only\n",
    "|  pl                  |  mod1      |  add pipeline; use text and numbers\n",
    "|  pl_rf               |  mod2      |  swap in RandomForest for classifier\n",
    "|  pl_03               |  mod3      |  back to logistic regression; add bigrams\n",
    "|  pl_05               |  mod4      |  swap in HashingVectorizer; add feature interaction        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train/test probabilities and values, variable and file names\n",
    "\n",
    "* mod0\n",
    "    * np.save('fmm_out/mod0_train_probas', mod0_yhat_train_probas)\n",
    "    * np.save('fmm_out/mod0_test_probas', mod0_yhat_test_probas)\n",
    "    * np.save('fmm_out/mod0_y_train',       y_train.values)\n",
    "    * np.save('fmm_out/mod0_y_test',       y_test.values)\n",
    "\n",
    "* mod1    \n",
    "    * np.save('fmm_out/mod1_train_probas', mod1_yhat_train_probas)\n",
    "    * np.save('fmm_out/mod1_test_probas', mod1_yhat_test_probas)\n",
    "    * np.save('fmm_out/mod1_y_train',       y_train.values)\n",
    "    * np.save('fmm_out/mod1_y_test',       y_test.values)\n",
    "    \n",
    "* mod2\n",
    "\n",
    "    * np.save('fmm_out/mod1_train_probas', mod1_yhat_train_probas)\n",
    "    * np.save('fmm_out/mod1_test_probas', mod1_yhat_test_probas)\n",
    "    * np.save('fmm_out/mod1_y_train',       y_train.values)\n",
    "    * np.save('fmm_out/mod1_y_test',       y_test.values)\n",
    "\n",
    "* mod3\n",
    "\n",
    "    * np.save('fmm_out/mod3_f_train_probas', mod3_train_probas)\n",
    "    * np.save('fmm_out/mod3_test_probas_flat', mod3_test_probas)\n",
    "    * np.save('fmm_out/mod3_y_train',       y_train.values)\n",
    "    * np.save('fmm_out/mod3_y_test',       y_test.values)\n",
    "\n",
    "* mod4\n",
    "\n",
    "    * np.save('fmm_out/mod4_f_train_probas', mod4_train_probas)\n",
    "    * np.save('fmm_out/mod4_test_probas_flat', mod4_test_probas)\n",
    "    * np.save('fmm_out/mod4_y_train',       y_train.values)\n",
    "    * np.save('fmm_out/mod4_y_test',       y_test.values)\n",
    "    \n",
    "Predictions on holdout set; these were all written by first_models (not this file).\n",
    "\n",
    "##### Note: The probabilty prediction files written by  first_models.ipynb have been renamed and moved to the 'submitted' directory.\n",
    "\n",
    "|old name | new name |  model |          comment\n",
    "|---------|----------|--------------------------------\n",
    "|predictions.csv     |   t_00.csv                   | 1st model | numerical features only\n",
    "|sub2.csv            |   t_01.csv                   | 2nd model | add pipeline; use text and numbers\n",
    "|sub_rf_defaults.csv |   t_02.csv                   | 3rd model | swap in RandomForest for classifier\n",
    "|sub3.csv            |   t_03.csv                   | 4th model | back to logistic regression; add bigrams\n",
    "|subm4.csv           |   t_04.csv                   | 5th model | swap in HashingVectorizer; add feature interaction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports/setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "pd.set_option('display.max_columns', 60)\n",
    "\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data\n",
    "df = pd.read_csv('data/TrainingData.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Function</th>\n",
       "      <th>Use</th>\n",
       "      <th>Sharing</th>\n",
       "      <th>Reporting</th>\n",
       "      <th>Student_Type</th>\n",
       "      <th>Position_Type</th>\n",
       "      <th>Object_Type</th>\n",
       "      <th>Pre_K</th>\n",
       "      <th>Operating_Status</th>\n",
       "      <th>Object_Description</th>\n",
       "      <th>Text_2</th>\n",
       "      <th>SubFund_Description</th>\n",
       "      <th>Job_Title_Description</th>\n",
       "      <th>Text_3</th>\n",
       "      <th>Text_4</th>\n",
       "      <th>Sub_Object_Description</th>\n",
       "      <th>Location_Description</th>\n",
       "      <th>FTE</th>\n",
       "      <th>Function_Description</th>\n",
       "      <th>Facility_or_Department</th>\n",
       "      <th>Position_Extra</th>\n",
       "      <th>Total</th>\n",
       "      <th>Program_Description</th>\n",
       "      <th>Fund_Description</th>\n",
       "      <th>Text_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>134338</th>\n",
       "      <td>Teacher Compensation</td>\n",
       "      <td>Instruction</td>\n",
       "      <td>School Reported</td>\n",
       "      <td>School</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>Teacher</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>PreK-12 Operating</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Teacher-Elementary</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>KINDERGARTEN</td>\n",
       "      <td>50471.810</td>\n",
       "      <td>KINDERGARTEN</td>\n",
       "      <td>General Fund</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206341</th>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>Non-Operating</td>\n",
       "      <td>CONTRACTOR SERVICES</td>\n",
       "      <td>BOND EXPENDITURES</td>\n",
       "      <td>BUILDING FUND</td>\n",
       "      <td>(blank)</td>\n",
       "      <td>Regular</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RGN  GOB</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UNDESIGNATED</td>\n",
       "      <td>3477.860</td>\n",
       "      <td>BUILDING IMPROVEMENT SERVICES</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BUILDING IMPROVEMENT SERVICES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326408</th>\n",
       "      <td>Teacher Compensation</td>\n",
       "      <td>Instruction</td>\n",
       "      <td>School Reported</td>\n",
       "      <td>School</td>\n",
       "      <td>Unspecified</td>\n",
       "      <td>Teacher</td>\n",
       "      <td>Base Salary/Compensation</td>\n",
       "      <td>Non PreK</td>\n",
       "      <td>PreK-12 Operating</td>\n",
       "      <td>Personal Services - Teachers</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TCHER 2ND GRADE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Regular Instruction</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TEACHER</td>\n",
       "      <td>62237.130</td>\n",
       "      <td>Instruction - Regular</td>\n",
       "      <td>General Purpose School</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364634</th>\n",
       "      <td>Substitute Compensation</td>\n",
       "      <td>Instruction</td>\n",
       "      <td>School Reported</td>\n",
       "      <td>School</td>\n",
       "      <td>Unspecified</td>\n",
       "      <td>Substitute</td>\n",
       "      <td>Benefits</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>PreK-12 Operating</td>\n",
       "      <td>EMPLOYEE BENEFITS</td>\n",
       "      <td>TEACHER SUBS</td>\n",
       "      <td>GENERAL FUND</td>\n",
       "      <td>Teacher, Short Term Sub</td>\n",
       "      <td>Regular</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UNALLOC BUDGETS/SCHOOLS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PROFESSIONAL-INSTRUCTIONAL</td>\n",
       "      <td>22.300</td>\n",
       "      <td>GENERAL MIDDLE/JUNIOR HIGH SCH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>REGULAR INSTRUCTION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47683</th>\n",
       "      <td>Substitute Compensation</td>\n",
       "      <td>Instruction</td>\n",
       "      <td>School Reported</td>\n",
       "      <td>School</td>\n",
       "      <td>Unspecified</td>\n",
       "      <td>Teacher</td>\n",
       "      <td>Substitute Compensation</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>PreK-12 Operating</td>\n",
       "      <td>TEACHER COVERAGE FOR TEACHER</td>\n",
       "      <td>TEACHER SUBS</td>\n",
       "      <td>GENERAL FUND</td>\n",
       "      <td>Teacher, Secondary (High)</td>\n",
       "      <td>Alternative</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NON-PROJECT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PROFESSIONAL-INSTRUCTIONAL</td>\n",
       "      <td>54.166</td>\n",
       "      <td>GENERAL HIGH SCHOOL EDUCATION</td>\n",
       "      <td>NaN</td>\n",
       "      <td>REGULAR INSTRUCTION</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Function          Use          Sharing Reporting  \\\n",
       "134338     Teacher Compensation  Instruction  School Reported    School   \n",
       "206341                 NO_LABEL     NO_LABEL         NO_LABEL  NO_LABEL   \n",
       "326408     Teacher Compensation  Instruction  School Reported    School   \n",
       "364634  Substitute Compensation  Instruction  School Reported    School   \n",
       "47683   Substitute Compensation  Instruction  School Reported    School   \n",
       "\n",
       "       Student_Type Position_Type               Object_Type     Pre_K  \\\n",
       "134338     NO_LABEL       Teacher                  NO_LABEL  NO_LABEL   \n",
       "206341     NO_LABEL      NO_LABEL                  NO_LABEL  NO_LABEL   \n",
       "326408  Unspecified       Teacher  Base Salary/Compensation  Non PreK   \n",
       "364634  Unspecified    Substitute                  Benefits  NO_LABEL   \n",
       "47683   Unspecified       Teacher   Substitute Compensation  NO_LABEL   \n",
       "\n",
       "         Operating_Status            Object_Description             Text_2  \\\n",
       "134338  PreK-12 Operating                           NaN                NaN   \n",
       "206341      Non-Operating           CONTRACTOR SERVICES  BOND EXPENDITURES   \n",
       "326408  PreK-12 Operating  Personal Services - Teachers                NaN   \n",
       "364634  PreK-12 Operating             EMPLOYEE BENEFITS       TEACHER SUBS   \n",
       "47683   PreK-12 Operating  TEACHER COVERAGE FOR TEACHER       TEACHER SUBS   \n",
       "\n",
       "       SubFund_Description       Job_Title_Description       Text_3  \\\n",
       "134338                 NaN         Teacher-Elementary           NaN   \n",
       "206341       BUILDING FUND                     (blank)      Regular   \n",
       "326408                 NaN             TCHER 2ND GRADE          NaN   \n",
       "364634        GENERAL FUND    Teacher, Short Term Sub       Regular   \n",
       "47683         GENERAL FUND  Teacher, Secondary (High)   Alternative   \n",
       "\n",
       "                     Text_4 Sub_Object_Description Location_Description  FTE  \\\n",
       "134338                  NaN                    NaN                  NaN  1.0   \n",
       "206341                  NaN                    NaN                  NaN  NaN   \n",
       "326408  Regular Instruction                    NaN                  NaN  1.0   \n",
       "364634                  NaN                    NaN                  NaN  NaN   \n",
       "47683                   NaN                    NaN                  NaN  NaN   \n",
       "\n",
       "           Function_Description Facility_or_Department  \\\n",
       "134338                      NaN                    NaN   \n",
       "206341                 RGN  GOB                    NaN   \n",
       "326408                      NaN                    NaN   \n",
       "364634  UNALLOC BUDGETS/SCHOOLS                    NaN   \n",
       "47683               NON-PROJECT                    NaN   \n",
       "\n",
       "                    Position_Extra      Total             Program_Description  \\\n",
       "134338               KINDERGARTEN   50471.810                    KINDERGARTEN   \n",
       "206341                UNDESIGNATED   3477.860   BUILDING IMPROVEMENT SERVICES   \n",
       "326408                     TEACHER  62237.130           Instruction - Regular   \n",
       "364634  PROFESSIONAL-INSTRUCTIONAL     22.300  GENERAL MIDDLE/JUNIOR HIGH SCH   \n",
       "47683   PROFESSIONAL-INSTRUCTIONAL     54.166   GENERAL HIGH SCHOOL EDUCATION   \n",
       "\n",
       "              Fund_Description                         Text_1  \n",
       "134338            General Fund                            NaN  \n",
       "206341                     NaN  BUILDING IMPROVEMENT SERVICES  \n",
       "326408  General Purpose School                            NaN  \n",
       "364634                     NaN            REGULAR INSTRUCTION  \n",
       "47683                      NaN            REGULAR INSTRUCTION  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take a look\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### take a quick look at the dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Function                   object\n",
       "Use                        object\n",
       "Sharing                    object\n",
       "Reporting                  object\n",
       "Student_Type               object\n",
       "Position_Type              object\n",
       "Object_Type                object\n",
       "Pre_K                      object\n",
       "Operating_Status           object\n",
       "Object_Description         object\n",
       "Text_2                     object\n",
       "SubFund_Description        object\n",
       "Job_Title_Description      object\n",
       "Text_3                     object\n",
       "Text_4                     object\n",
       "Sub_Object_Description     object\n",
       "Location_Description       object\n",
       "FTE                       float64\n",
       "Function_Description       object\n",
       "Facility_or_Department     object\n",
       "Position_Extra             object\n",
       "Total                     float64\n",
       "Program_Description        object\n",
       "Fund_Description           object\n",
       "Text_1                     object\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Encode the targets as categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function            category\n",
      "Object_Type         category\n",
      "Operating_Status    category\n",
      "Position_Type       category\n",
      "Pre_K               category\n",
      "Reporting           category\n",
      "Sharing             category\n",
      "Student_Type        category\n",
      "Use                 category\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "### bind variable LABELS - these are actually the targets and we're going to one-hot encode them...\n",
    "LABELS = ['Function',  'Use',  'Sharing',  'Reporting',  'Student_Type',  'Position_Type', \n",
    "          'Object_Type',  'Pre_K',  'Operating_Status']\n",
    "\n",
    "### This turns out to be key.  Submission requires the dummy versions of these vars to be in this order.\n",
    "LABELS.sort()\n",
    "\n",
    "# Define the lambda function: categorize_label\n",
    "categorize_label = lambda x: x.astype('category')\n",
    "\n",
    "# Convert df[LABELS] to a categorical type\n",
    "df[LABELS] = df[LABELS].apply(categorize_label, axis=0)\n",
    "\n",
    "# Print the converted dtypes\n",
    "print(df[LABELS].dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Counting unique labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculate number of unique values for each label: num_unique_labels\n",
    "num_unique_labels = df[LABELS].apply(pd.Series.nunique, axis=0)\n",
    "\n",
    "# Plot number of unique values for each label\n",
    "num_unique_labels.plot(kind='bar')\n",
    "\n",
    "# Label the axes\n",
    "plt.xlabel('Labels')\n",
    "plt.ylabel('Number of unique values')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Function            37\n",
       "Object_Type         11\n",
       "Operating_Status     3\n",
       "Position_Type       25\n",
       "Pre_K                3\n",
       "Reporting            3\n",
       "Sharing              5\n",
       "Student_Type         9\n",
       "Use                  8\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_unique_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(num_unique_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's save the unique labels for each output (category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a dictionary\n",
    "cols_dict = {col : df[col].unique().tolist() for col in df[LABELS].columns}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Teacher Compensation',\n",
       " 'NO_LABEL',\n",
       " 'Substitute Compensation',\n",
       " 'Facilities & Maintenance',\n",
       " 'Instructional Materials & Supplies',\n",
       " 'Food Services',\n",
       " 'Security & Safety',\n",
       " 'Utilities',\n",
       " 'Student Transportation',\n",
       " 'Parent & Community Relations',\n",
       " 'Extended Time & Tutoring',\n",
       " 'Enrichment',\n",
       " 'Special Population Program Management & Support',\n",
       " 'School Supervision',\n",
       " 'Data Processing & Information Services',\n",
       " 'Aides Compensation',\n",
       " 'Physical Health & Services',\n",
       " 'Career & Academic Counseling',\n",
       " 'Library & Media',\n",
       " 'Professional Development',\n",
       " 'School Administration',\n",
       " 'Other Non-Compensation',\n",
       " 'Social & Emotional',\n",
       " 'Finance, Budget, Purchasing & Distribution',\n",
       " 'Human Resources',\n",
       " 'Curriculum Development',\n",
       " 'Legal',\n",
       " 'Other Compensation',\n",
       " 'Student Assignment',\n",
       " 'Governance',\n",
       " 'Development & Fundraising',\n",
       " 'Research & Accountability',\n",
       " 'Recruitment',\n",
       " 'Insurance',\n",
       " 'Untracked Budget Set-Aside',\n",
       " 'Communications',\n",
       " 'Facilities Planning']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_dict['Function']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up a train-test split  for modeling\n",
    "\n",
    "The first step is to split the data into a training set and a test set. Some labels don't occur very often, but we want to make sure that they appear in both the training and the test sets. The function multilabel_train_test_split (in this directory) will make sure at least *min_count* examples of each label appear in each split.\n",
    "\n",
    "We start with a simple model that uses just the numeric columns of the datawhen calling multilabel_train_test_split. \n",
    "\n",
    "Also, we one-hot encode the labels as advised in the tutorial.  This creates 104 columns of binary label columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from python.multilabel import multilabel_train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMERIC_COLUMNS = ['FTE', 'Total']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 320222 entries, 134338 to 415831\n",
      "Data columns (total 2 columns):\n",
      "FTE      320222 non-null float64\n",
      "Total    320222 non-null float64\n",
      "dtypes: float64(2)\n",
      "memory usage: 7.3 MB\n",
      "None\n",
      "\n",
      "X_test info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 80055 entries, 206341 to 413949\n",
      "Data columns (total 2 columns):\n",
      "FTE      80055 non-null float64\n",
      "Total    80055 non-null float64\n",
      "dtypes: float64(2)\n",
      "memory usage: 1.8 MB\n",
      "None\n",
      "\n",
      "y_train info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 320222 entries, 134338 to 415831\n",
      "Columns: 104 entries, Function_Aides Compensation to Use_Untracked Budget Set-Aside\n",
      "dtypes: uint8(104)\n",
      "memory usage: 34.2 MB\n",
      "None\n",
      "\n",
      "y_test info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 80055 entries, 206341 to 413949\n",
      "Columns: 104 entries, Function_Aides Compensation to Use_Untracked Budget Set-Aside\n",
      "dtypes: uint8(104)\n",
      "memory usage: 8.6 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Create the new DataFrame: numeric_data_only\n",
    "numeric_data_only = df[NUMERIC_COLUMNS].fillna(-1000)\n",
    "\n",
    "# Get labels and convert to dummy variables: label_dummies\n",
    "label_dummies = pd.get_dummies(df[LABELS])\n",
    "\n",
    "# Create training and test sets\n",
    "X_train, X_test, y_train, y_test = multilabel_train_test_split(numeric_data_only,\n",
    "                                                               label_dummies,\n",
    "                                                               size=0.2, \n",
    "                                                               seed=123)\n",
    "\n",
    "# Print the info\n",
    "print(\"X_train info:\")\n",
    "print(X_train.info())\n",
    "print(\"\\nX_test info:\")  \n",
    "print(X_test.info())\n",
    "print(\"\\ny_train info:\")  \n",
    "print(y_train.info())\n",
    "print(\"\\ny_test info:\")  \n",
    "print(y_test.info()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400277, 2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[NUMERIC_COLUMNS].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start with a simple model\n",
    "\n",
    "The first model ignores everything but the two numeric columns  to get started and check for correct format (104 columns of probability predictions).  Create a multi-label classifier clf by placing LogisticRegression() inside OneVsRestClassifier()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit time: 100.04255839071291 seconds\n"
     ]
    }
   ],
   "source": [
    "# Import classifiers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# Create the DataFrame: numeric_data_only\n",
    "numeric_data_only = df[NUMERIC_COLUMNS].fillna(-1000)\n",
    "\n",
    "# Get labels and convert to dummy variables: label_dummies\n",
    "label_dummies = pd.get_dummies(df[LABELS])\n",
    "\n",
    "# Create training and test sets\n",
    "X_train, X_test, y_train, y_test = multilabel_train_test_split(numeric_data_only,\n",
    "                                                               label_dummies,\n",
    "                                                               size=0.2, \n",
    "                                                               seed=123)\n",
    "# Instantiate the classifier: clf\n",
    "mod0 = OneVsRestClassifier(LogisticRegression(), n_jobs=-1)\n",
    "\n",
    "start = timer()\n",
    "# Fit the classifier to the training data\n",
    "mod0.fit(X_train, y_train)\n",
    "end = timer()\n",
    "print('fit time: {} seconds'.format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the accuracy scores\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# get the predictions\n",
    "mod0_yhat_train = mod0.predict(X_train)\n",
    "mod0_yhat_test = mod0.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((320222, 104), (80055, 104))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod0_yhat_train.shape, mod0_yhat_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Not very accurate but correct format.\n",
    "\n",
    "The sklearn.metrics.accuracy function doesn't do the expected thing because the predictions of this model (and the y values they are compared to) are somewhat different than the usual situation. To summarize: the model is better than this - see standard_metrics.ipynb for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.000012\n",
      "test accuracy: 0.000012\n"
     ]
    }
   ],
   "source": [
    "print('train accuracy: {:.6f}'.format(accuracy_score(mod0_yhat_train, y_train)))\n",
    "print('test accuracy: {:.6f}'.format(accuracy_score(mod0_yhat_train, y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "\n",
    "Try the different log_loss metrics I have, knowing that DD scored this model at 1.3314"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Official loss metric\n",
    "### From https://github.com/drivendataorg/metrics/blob/master/metrics.py\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, r2_score\n",
    "\n",
    "# Defined for your convenience; these are the\n",
    "# class_column_indices for the Box-Plots for Education competition\n",
    "# www.drivendata.org/competitions/4/\n",
    "BOX_PLOTS_COLUMN_INDICES = [range(37),\n",
    "                            range(37, 48),\n",
    "                            range(48, 51),\n",
    "                            range(51, 76),\n",
    "                            range(76, 79),\n",
    "                            range(79, 82),\n",
    "                            range(82, 87),\n",
    "                            range(87, 96),\n",
    "                            range(96, 104)]\n",
    "\n",
    "BPCI = BOX_PLOTS_COLUMN_INDICES\n",
    "\n",
    "def multi_multi_log_loss(predicted, actual, class_column_indices, eps=1e-15):\n",
    "    \"\"\"Multi class, multi-label version of Logarithmic Loss metric.\n",
    "    :param predicted: a 2d numpy array of the predictions that are probabilities [0, 1]\n",
    "    :param actual: a 2d numpy array of the same shape as your predictions. 1 for the actual labels, 0 elsewhere\n",
    "    :return: The multi-multi log loss score for this set of predictions\n",
    "    \"\"\"\n",
    "    class_scores = np.ones(len(class_column_indices), dtype=np.float64)\n",
    "\n",
    "    # calculate log loss for each set of columns that belong to a class:\n",
    "    for k, this_class_indices in enumerate(class_column_indices):\n",
    "        # get just the columns for this class\n",
    "        preds_k = predicted[:, this_class_indices]\n",
    "\n",
    "        # normalize so probabilities sum to one (unless sum is zero, then we clip)\n",
    "        preds_k /= np.clip(preds_k.sum(axis=1).reshape(-1, 1), eps, np.inf)\n",
    "\n",
    "        actual_k = actual[:, this_class_indices]\n",
    "\n",
    "        # shrink predictions\n",
    "        y_hats = np.clip(preds_k, eps, 1 - eps)\n",
    "        sum_logs = np.sum(actual_k * np.log(y_hats))\n",
    "        class_scores[k] = (-1.0 / actual.shape[0]) * sum_logs\n",
    "\n",
    "    return np.average(class_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### First we need the probabilities, not the predicted labels\n",
    "mod0_yhat_train_probas = mod0.predict_proba(X_train)\n",
    "mod0_yhat_test_probas = mod0.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.353521736853036"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_multi_log_loss(mod0_yhat_train_probas, y_train.values, BOX_PLOTS_COLUMN_INDICES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3557282270290794"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_multi_log_loss(mod0_yhat_test_probas, y_test.values, BOX_PLOTS_COLUMN_INDICES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We only need to save probability predictions and y values.\n",
    "\n",
    "As proved in flat_to_labels.ipynb, the predictions of 1vs103 approach are not usable without post processing.  But the probability output can be turned into standard multi-target labels with the function, flat_to_labels.\n",
    "\n",
    "The implication is that it doesn't make sense to keep yhats, only y_probas.  The reconstituted predictions are the same as you would get from 9 multi-label classifiers (one per target) and they can be used to run the standard sklearn metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('fmm_out/mod0_train_probas', mod0_yhat_train_probas)\n",
    "np.save('fmm_out/mod0_test_probas', mod0_yhat_test_probas)\n",
    "np.save('fmm_out/mod0_y_train',       y_train.values)\n",
    "np.save('fmm_out/mod0_y_test',       y_test.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submitted prediction file and scored 1.33.  Best prediction is 0.37 (log_loss)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__================ End of Model 0 ==================__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add text processing to the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combining text columns for tokenization\n",
    "\n",
    "The tutorial recommends combining all the text columns into a single string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define combine_text_columns()\n",
    "def combine_text_columns(df, to_drop=NUMERIC_COLUMNS + LABELS):\n",
    "    \"\"\" converts all text columns in each row of df to single string \"\"\"\n",
    "    # Drop non-text columns that are in the df\n",
    "    to_drop = set(to_drop) & set(df.columns.tolist())\n",
    "    text_data = df.drop(to_drop, axis=1)  \n",
    "    # Replace nans with blanks\n",
    "    text_data.fillna('', inplace=True)    \n",
    "    # Join all text items in a row that have a space in between\n",
    "    return text_data.apply(lambda x: \" \".join(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "397349    SUPPLIES                         TEACHER, MATH...\n",
       "181594    EMPLOYEE BENEFITS  GENERAL FUND Teacher, Short...\n",
       "178741       Teacher-Art       ART  PRIMARY GRADES PROGR...\n",
       "127259    OTHER PERSONAL SERVICES          SUB TEACHER A...\n",
       "312022    SALARIES OF PART TIME EMPLOYEE  DISTRICT SPECI...\n",
       "207135    SALARIES OF PART TIME EMPLOYEE  DISTRICT SPECI...\n",
       "425110    ADDITIONAL/EXTRA DUTY PAY/STIP  GENERAL FUND T...\n",
       "223981    Personal Services - Substitute Teachers Certif...\n",
       "430816    SALARIES OF REGULAR EMPLOYEES  LEVY OVERRIDE E...\n",
       "212038    CONTRA BENEFITS  GENERAL FUND Teacher, Element...\n",
       "dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test it\n",
    "combine_text_columns(df.sample(n=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What's in a token?\n",
    "Now you will use combine_text_columns to convert all training text data in your DataFrame to a single vector that can be passed to the vectorizer object and made into a bag-of-words using the .fit_transform() method.\n",
    "\n",
    "You'll compare the effect of tokenizing using any non-whitespace characters as a token and using only alphanumeric characters as a token.\n",
    "\n",
    "##### INSTRUCTIONS\n",
    "\n",
    "Import CountVectorizer from sklearn.feature_extraction.text.\n",
    "Instantiate vec_basic and vec_alphanumeric using, respectively, the TOKENS_BASIC and TOKENS_ALPHANUMERIC patterns.\n",
    "Create the text vector by using the combine_text_columns() function on df.\n",
    "Using the .fit_transform() method with text_vector, fit and transform first vec_basic and then vec_alphanumeric. Print the number of tokens they contain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4757 tokens in the dataset\n",
      "There are 3284 alpha-numeric tokens in the dataset\n"
     ]
    }
   ],
   "source": [
    "# Import the CountVectorizer\n",
    "from  sklearn.feature_extraction.text import CountVectorizer\n",
    "# Create the basic token pattern; separates based on whitespace\n",
    "TOKENS_BASIC = '\\\\S+(?=\\\\s+)'\n",
    "# Create the alphanumeric token pattern\n",
    "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'\n",
    "# Instantiate basic CountVectorizer: vec_basic\n",
    "vec_basic = CountVectorizer(token_pattern=TOKENS_BASIC)\n",
    "# Instantiate alphanumeric CountVectorizer: vec_alphanumeric\n",
    "vec_alphanumeric = CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC)\n",
    "# Create the text vector\n",
    "text_vector = combine_text_columns(df)\n",
    "# Fit and transform vec_basic\n",
    "vec_basic.fit_transform(text_vector)\n",
    "# Print number of tokens of vec_basic\n",
    "print(\"There are {} tokens in the dataset\".format(len(vec_basic.get_feature_names())))\n",
    "# Fit and transform vec_alphanumeric\n",
    "vec_alphanumeric.fit_transform(text_vector)\n",
    "# Print number of tokens of vec_alphanumeric\n",
    "print(\"There are {} alpha-numeric tokens in the dataset\".format(len(vec_alphanumeric.get_feature_names())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"(year', '\"building', '\"guidance,', '\"human', '\"maintenance,', '\"multilingual', '\"performing', '\"software,', '\"technology', '\"title', '%', '&', '&materials', '&program', '&wildlife', '(']\n"
     ]
    }
   ],
   "source": [
    "print(vec_basic.get_feature_names()[:16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00a', '12', '1st', '2nd', '3rd', '4th', '5', '56', '5th', '6', '60', '60hrs', '6th', '70', '70h', '70hr']\n"
     ]
    }
   ],
   "source": [
    "print(vec_alphanumeric.get_feature_names()[:16])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add pipeline\n",
    "\n",
    "The tutorial introducing a pipeline to consolidate preprocessing and modeling in a single classifier/estimator and to facilitate experimentation with both preprocessing and different models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rebinding X/y train/test...\n",
    "\n",
    "It needs to be done because X is a different feature subset.  It's likely that the indices are the same as before.  I'd like to check it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import FunctionTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# Get the dummy encoding of the labels\n",
    "dummy_labels = pd.get_dummies(df[LABELS])\n",
    "\n",
    "# Get the columns that are features in the original df\n",
    "NON_LABELS = [c for c in df.columns if c not in LABELS]\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = multilabel_train_test_split(df[NON_LABELS],\n",
    "                                                               dummy_labels,\n",
    "                                                               0.2, \n",
    "                                                               seed=123)\n",
    "# Preprocess the text data: get_text_data\n",
    "get_text_data = FunctionTransformer(combine_text_columns, validate=False)\n",
    "\n",
    "# Preprocess the numeric data: get_numeric_data\n",
    "get_numeric_data = FunctionTransformer(lambda x: x[NUMERIC_COLUMNS], validate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "# for the selectors\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "# for gluing preprocessed text and numbers together\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "# for nans in the numeric data\n",
    "from sklearn.preprocessing import Imputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit time: 418.17933918888946 seconds\n"
     ]
    }
   ],
   "source": [
    "# Complete the pipeline: pl\n",
    "mod1 = Pipeline([\n",
    "        ('union', FeatureUnion(\n",
    "            transformer_list = [\n",
    "                ('numeric_features', Pipeline([('selector', get_numeric_data),\n",
    "                                               ('imputer', Imputer())])),\n",
    "                ('text_features', Pipeline([('selector', get_text_data),\n",
    "                                            ('vectorizer', CountVectorizer())]))\n",
    "             ])),\n",
    "        ('clf', OneVsRestClassifier(LogisticRegression(), n_jobs=-1))\n",
    "    ])\n",
    "\n",
    "start = timer()\n",
    "# Fit to the training data\n",
    "mod1.fit(X_train, y_train)\n",
    "end = timer()\n",
    "print('fit time: {} seconds'.format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the accuracy scores, but see note above on using accuracy on this output.\n",
    "mod1_yhat_train = mod1.predict(X_train)\n",
    "mod1_yhat_test = mod1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.3493\n",
      "test accuracy: 0.3493\n"
     ]
    }
   ],
   "source": [
    "print('train accuracy: {:.4f}'.format(accuracy_score(mod1_yhat_train, y_train)))\n",
    "print('test accuracy: {:.4f}'.format(accuracy_score(mod1_yhat_train, y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "### For log loss we need the probabilities, not the predicted labels\n",
    "mod1_yhat_train_probas = mod1.predict_proba(X_train)\n",
    "mod1_yhat_test_probas = mod1.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5110256169364263"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_multi_log_loss(mod1_yhat_train_probas, y_train.values, BOX_PLOTS_COLUMN_INDICES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5117387856670895"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_multi_log_loss(mod1_yhat_test_probas, y_test.values, BOX_PLOTS_COLUMN_INDICES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('fmm_out/mod1_train_probas', mod1_yhat_train_probas)\n",
    "np.save('fmm_out/mod1_test_probas', mod1_yhat_test_probas)\n",
    "np.save('fmm_out/mod1_y_train',       y_train.values)\n",
    "np.save('fmm_out/mod1_y_test',       y_test.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submitted and scored with log-loss of 0.75."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ==========================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try a different class of model, RandomForestClassifier\n",
    "\n",
    "Unlike the model above,  the RFCs predictions come out in a different format (list of 104 2-d arrays with a probability for both 1 and 0).  This needs to be reshaped for submission.\n",
    "\n",
    "This model is overfit, which is to be expected with default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit time: 133.67552510826226 seconds\n"
     ]
    }
   ],
   "source": [
    "# Import random forest classifer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Edit model step in pipeline\n",
    "pl_rf = Pipeline([\n",
    "        ('union', FeatureUnion(\n",
    "            transformer_list = [\n",
    "                ('numeric_features', Pipeline([\n",
    "                    ('selector', get_numeric_data),\n",
    "                    ('imputer', Imputer())\n",
    "                ])),\n",
    "                ('text_features', Pipeline([\n",
    "                    ('selector', get_text_data),\n",
    "                    ('vectorizer', CountVectorizer())\n",
    "                ]))\n",
    "             ]\n",
    "        )),\n",
    "        ('clf', RandomForestClassifier(n_jobs=-1))\n",
    "    ])\n",
    "start = timer()\n",
    "# Fit to the training data\n",
    "pl_rf.fit(X_train, y_train)\n",
    "end = timer()\n",
    "print('fit time: {} seconds'.format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and print accuracy\n",
    "# Print the accuracy scores\n",
    "pl_rf_yhat_train = pl_rf.predict(X_train)\n",
    "pl_rf_yhat_test = pl_rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.9854\n",
      "test accuracy: 0.9056\n"
     ]
    }
   ],
   "source": [
    "print('train accuracy: {:.4f}'.format(accuracy_score(pl_rf_yhat_train, y_train)))\n",
    "print('test accuracy: {:.4f}'.format(accuracy_score(pl_rf_yhat_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get train/test probas in correct format for dd_mmll and flat_to_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_rf_train_probas = pl_rf.predict_proba(X_train)\n",
    "pl_rf_test_probas = pl_rf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check type.  It's list, not array\n",
    "type(pl_rf_train_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(320222, 2)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what is the shape of elements?  2 probabilities, 1st is for 0, 2nd is for 1\n",
    "pl_rf_train_probas[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab the probabilities for 1 and turn into array of the correct shape\n",
    "pl_rf_train_probas_flat = np.concatenate([x[:, 1].reshape(-1,1) for x in pl_rf_train_probas], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(320222, 104)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check shape\n",
    "pl_rf_train_probas_flat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look at some of the probabilties.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1.,\n",
       "       0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       0., 0.])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl_rf_train_probas_flat[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# capture all the unique values and their counts\n",
    "da_probs = np.unique(pl_rf_train_probas_flat, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x19da6f69940>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the values and log of the counts; most common are 0 and 1\n",
    "plt.scatter(da_probs[0], np.log(da_probs[1]), marker='.', s=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict on the holdout set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_rf_test_probas_flat = np.concatenate([x[:, 1].reshape(-1,1) for x in pl_rf_test_probas], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the dd_mmll for train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.016416194134729067"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_multi_log_loss(pl_rf_train_probas_flat, y_train.values, BOX_PLOTS_COLUMN_INDICES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.27993204928582904"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_multi_log_loss(pl_rf_test_probas_flat, y_test.values, BOX_PLOTS_COLUMN_INDICES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### save results\n",
    "\n",
    "np.save('fmm_out/pl_rf_train_probas_flat', pl_rf_train_probas_flat)\n",
    "np.save('fmm_out/pl_rf_test_probas_flat', pl_rf_test_probas_flat)\n",
    "np.save('fmm_out/pl_rf_y_train',       y_train.values)\n",
    "np.save('fmm_out/pl_rf_y_test',       y_test.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submitted 28may2018\n",
    "\n",
    "##### Score on holdout set predictions is 1.6491. Much worse even though accuracy and log loss is great.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### =================================== End of Mod02 ============================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mod03: Add features to the pipeline\n",
    "\n",
    "1. Add bigrams to CountVectorizer and tokenize on alphanumeric (previously was used with default settings)\n",
    "2. Dimension reduction with SelectKBest using chi-squared (300 features).\n",
    "3. Scale all features to range \\[-1, 1\\] with MaxAbsScaler\n",
    "4. Go back to OneVsAll(LogisticRegression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pipeline\n",
    "# from sklearn.pipeline import Pipeline\n",
    "\n",
    "# # Import classifiers\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# # Import CountVectorizer\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Import other preprocessing modules\n",
    "# from sklearn.preprocessing import Imputer\n",
    "from sklearn.feature_selection import chi2, SelectKBest\n",
    "\n",
    "# Select 300 best features\n",
    "chi_k = 300\n",
    "\n",
    "# Import functional utilities\n",
    "from sklearn.preprocessing import FunctionTransformer, MaxAbsScaler\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "# Perform preprocessing\n",
    "get_text_data = FunctionTransformer(combine_text_columns, validate=False)\n",
    "get_numeric_data = FunctionTransformer(lambda x: x[NUMERIC_COLUMNS], validate=False)\n",
    "\n",
    "# Create the token pattern: TOKENS_ALPHANUMERIC\n",
    "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'\n",
    "\n",
    "# Instantiate pipeline: pl\n",
    "pl_03 = Pipeline([\n",
    "        ('union', FeatureUnion(\n",
    "            transformer_list = [\n",
    "                ('numeric_features', Pipeline([\n",
    "                    ('selector', get_numeric_data),\n",
    "                    ('imputer', Imputer())\n",
    "                ])),\n",
    "                ('text_features', Pipeline([\n",
    "                    ('selector', get_text_data),\n",
    "                    # soup up vectorizer a bit\n",
    "                    ('vectorizer', CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC,\n",
    "                                                   ngram_range=(1, 2))),\n",
    "                    ('dim_red', SelectKBest(chi2, chi_k))\n",
    "                ]))\n",
    "             ]\n",
    "        )),\n",
    "        ('scale', MaxAbsScaler()),\n",
    "        ('clf', OneVsRestClassifier(LogisticRegression(), n_jobs=-1))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### They haven't fitted or predicted with this model in the course.  It seems natural to do that.  Here we go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit time: 253.41003526521558 seconds\n"
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "# Fit to the training data\n",
    "pl_03.fit(X_train, y_train)\n",
    "end = timer()\n",
    "print('fit time: {} seconds'.format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and print accuracy\n",
    "# Print the accuracy scores\n",
    "yhat_train = pl_03.predict(X_train)\n",
    "yhat_test = pl_03.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.5489\n",
      "test accuracy: 0.5489\n"
     ]
    }
   ],
   "source": [
    "print('train accuracy: {:.4f}'.format(accuracy_score(yhat_train, y_train)))\n",
    "print('test accuracy: {:.4f}'.format(accuracy_score(yhat_train, y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod3_train_probas = pl_03.predict_proba(X_train)\n",
    "mod3_test_probas = pl_03.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### save results\n",
    "np.save('fmm_out/mod3_f_train_probas', mod3_train_probas)\n",
    "np.save('fmm_out/mod3_test_probas_flat', mod3_test_probas)\n",
    "np.save('fmm_out/mod3_y_train',       y_train.values)\n",
    "np.save('fmm_out/mod3_y_test',       y_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3560730331295568"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_multi_log_loss(mod3_train_probas, y_train.values, BOX_PLOTS_COLUMN_INDICES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35718663072807594"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_multi_log_loss(mod3_test_probas, y_test.values, BOX_PLOTS_COLUMN_INDICES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submitted model with bigrams, dimension reduction and scaling - score is 0.8174.  Not as good as 2nd model, but  pretty close.\n",
    "\n",
    "It may be that tossing all those features (there are 29k of them) is overdoing it. Also, maybe not the best choice of dimension reduction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ================================ End of mod3 ========================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mod4: Add feature interactions \n",
    "\n",
    "The tutorial provides a utility transformer called SparseInteractions. This add features to the model based on the combination of each feature pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from python.sparse_interactions import SparseInteractions\n",
    "\n",
    "# # Instantiate pipeline: pl\n",
    "# pl = Pipeline([\n",
    "#         ('union', FeatureUnion(\n",
    "#             transformer_list = [\n",
    "#                 ('numeric_features', Pipeline([\n",
    "#                     ('selector', get_numeric_data),\n",
    "#                     ('imputer', Imputer())\n",
    "#                 ])),\n",
    "#                 ('text_features', Pipeline([\n",
    "#                     ('selector', get_text_data),\n",
    "#                     ('vectorizer', CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC,\n",
    "#                                                    ngram_range=(1, 2))),  \n",
    "#                     ('dim_red', SelectKBest(chi2, chi_k))\n",
    "#                 ]))\n",
    "#              ]\n",
    "#         )),\n",
    "#         # Now add the interaction features to the selected feature set\n",
    "#         ('int', SparseInteractions(degree=2)),\n",
    "#         ('scale', MaxAbsScaler()),\n",
    "#         ('clf', OneVsRestClassifier(LogisticRegression()))\n",
    "#     ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Also use hashing vectorizer instead of CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the hashing vectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "# Instantiate the winning model pipeline: pl\n",
    "mod4 = Pipeline([\n",
    "        ('union', FeatureUnion(\n",
    "            transformer_list = [\n",
    "                ('numeric_features', Pipeline([\n",
    "                    ('selector', get_numeric_data),\n",
    "                    ('imputer', Imputer())\n",
    "                ])),\n",
    "                ('text_features', Pipeline([\n",
    "                    ('selector', get_text_data),\n",
    "                    ('vectorizer', HashingVectorizer(token_pattern=TOKENS_ALPHANUMERIC,\n",
    "                                                     non_negative=True, norm=None, binary=False,\n",
    "                                                     ngram_range=(1,2))),\n",
    "                    ('dim_red', SelectKBest(chi2, chi_k))\n",
    "                ]))\n",
    "             ]\n",
    "        )),\n",
    "        ('int', SparseInteractions(degree=2)),\n",
    "        ('scale', MaxAbsScaler()),\n",
    "        ('clf', OneVsRestClassifier(LogisticRegression(), n_jobs=-1))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The final model in the tutorial.  Fit and see how it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saus\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n",
      "C:\\Users\\saus\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit time: 3143.0284152797362 seconds\n"
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "# Fit to the training data\n",
    "mod4.fit(X_train, y_train)\n",
    "end = timer()\n",
    "print('fit time: {} seconds'.format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saus\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy on budget dataset:  0.7799762663169071\n"
     ]
    }
   ],
   "source": [
    "# Compute and print accuracy\n",
    "accuracy = mod4.score(X_test, y_test)\n",
    "print(\"\\nAccuracy on budget dataset: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3277 seconds is 54.6167 minutes.\n"
     ]
    }
   ],
   "source": [
    "print('3277 seconds is {:.4f} minutes.'.format(3277/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### That took a long time.  Accuracy is better.  Now submit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saus\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n",
      "C:\\Users\\saus\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict time: 284.69087487424804 seconds\n"
     ]
    }
   ],
   "source": [
    "# Compute and print accuracy\n",
    "# Print the accuracy scores\n",
    "start = timer()\n",
    "mod4_yhat_train = mod4.predict(X_train)\n",
    "mod4_yhat_test = mod4.predict(X_test)\n",
    "end = timer()\n",
    "print('predict time: {} seconds'.format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.7885\n",
      "test accuracy: 0.7885\n"
     ]
    }
   ],
   "source": [
    "print('train accuracy: {:.4f}'.format(accuracy_score(y_train, mod4_yhat_train,)))\n",
    "print('test accuracy: {:.4f}'.format(accuracy_score(y_train, mod4_yhat_train, )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saus\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n",
      "C:\\Users\\saus\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict time: 282.34185255747707 seconds\n"
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "mod4_train_probas = mod4.predict_proba(X_train)\n",
    "mod4_test_probas = mod4.predict_proba(X_test)\n",
    "end = timer()\n",
    "print('predict time: {} seconds'.format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### save results\n",
    "np.save('fmm_out/mod4_f_train_probas', mod4_train_probas)\n",
    "np.save('fmm_out/mod4_test_probas_flat', mod4_test_probas)\n",
    "np.save('fmm_out/mod4_y_train',       y_train.values)\n",
    "np.save('fmm_out/mod4_y_test',       y_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16776716010090734"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_multi_log_loss(mod4_train_probas, y_train.values, BOX_PLOTS_COLUMN_INDICES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17960486116593985"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_multi_log_loss(mod4_test_probas, y_test.values, BOX_PLOTS_COLUMN_INDICES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Final model as presented in course scores 0.8893.  This is considerably worse than much simpler early model.  Disappointing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
