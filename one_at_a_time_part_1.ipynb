{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add feature engineering one piece at a time and monitor performance.  This notebook goes through 1st and 2nd model with small changes.  See part 2 for later models.\n",
    "\n",
    "| model | agg. log loss  |  agg. F1 score |  comment\n",
    "|-------|:--------------:|:--------------:|----------\n",
    "| mod0  |  1.356         |    0.441       | numerical features only\n",
    "|mod0_1 |  1.323         |    0.441       | same as mod0, but use standard scaler before prediction\n",
    "|mod0_1a|  1.295         |    0.454       | scaling + convert total to absolute value\n",
    "| mod0_2|  1.362         |    0.406       | same as mod0 but use standard scaler and default imputer before prediction\n",
    "| mod1  |  0.512         | 0.853          | pipeline, numerical features and text features (fillna with empty string; combine all text columns within row; default count vectorizer)\n",
    "|mod1_1 | 0.094          | 0.974          | same as mod1 but ignore numerical data\n",
    "|mod1_1_1 | 0.094          | 0.974          | same as mod1_1; work around n_jobs=-1 bug for faster fit\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note: Several problems have happened having to do with using n_jobs=-1 (all processors) with OneVsAll.  At times, it works well.  Other times it hangs.  The code runs fine without the parameter, just slower than if it were working.   Upgraded to latest in attempt to fix, but that actually made things fail that were working before the upgrade.  Can't recreate with smaller problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports/setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "pd.set_option('display.max_columns', 60)\n",
    "\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
    "# unflattener\n",
    "import python.flat_to_labels as ftl\n",
    "# drivendata's spltter: ensures train and test both have enough of all the labels\n",
    "from python.multilabel import multilabel_train_test_split\n",
    "# drivendata's log loss metric\n",
    "from python.dd_mmll import multi_multi_log_loss, BOX_PLOTS_COLUMN_INDICES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "# for the selectors\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler\n",
    "# for gluing preprocessed text and numbers together\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "# for nans in the numeric data\n",
    "from sklearn.preprocessing import Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import classifiers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# Import CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data\n",
    "df = pd.read_csv('data/TrainingData.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Function</th>\n",
       "      <th>Use</th>\n",
       "      <th>Sharing</th>\n",
       "      <th>Reporting</th>\n",
       "      <th>Student_Type</th>\n",
       "      <th>Position_Type</th>\n",
       "      <th>Object_Type</th>\n",
       "      <th>Pre_K</th>\n",
       "      <th>Operating_Status</th>\n",
       "      <th>Object_Description</th>\n",
       "      <th>Text_2</th>\n",
       "      <th>SubFund_Description</th>\n",
       "      <th>Job_Title_Description</th>\n",
       "      <th>Text_3</th>\n",
       "      <th>Text_4</th>\n",
       "      <th>Sub_Object_Description</th>\n",
       "      <th>Location_Description</th>\n",
       "      <th>FTE</th>\n",
       "      <th>Function_Description</th>\n",
       "      <th>Facility_or_Department</th>\n",
       "      <th>Position_Extra</th>\n",
       "      <th>Total</th>\n",
       "      <th>Program_Description</th>\n",
       "      <th>Fund_Description</th>\n",
       "      <th>Text_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>134338</th>\n",
       "      <td>Teacher Compensation</td>\n",
       "      <td>Instruction</td>\n",
       "      <td>School Reported</td>\n",
       "      <td>School</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>Teacher</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>PreK-12 Operating</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Teacher-Elementary</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>KINDERGARTEN</td>\n",
       "      <td>50471.810</td>\n",
       "      <td>KINDERGARTEN</td>\n",
       "      <td>General Fund</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206341</th>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>Non-Operating</td>\n",
       "      <td>CONTRACTOR SERVICES</td>\n",
       "      <td>BOND EXPENDITURES</td>\n",
       "      <td>BUILDING FUND</td>\n",
       "      <td>(blank)</td>\n",
       "      <td>Regular</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RGN  GOB</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UNDESIGNATED</td>\n",
       "      <td>3477.860</td>\n",
       "      <td>BUILDING IMPROVEMENT SERVICES</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BUILDING IMPROVEMENT SERVICES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326408</th>\n",
       "      <td>Teacher Compensation</td>\n",
       "      <td>Instruction</td>\n",
       "      <td>School Reported</td>\n",
       "      <td>School</td>\n",
       "      <td>Unspecified</td>\n",
       "      <td>Teacher</td>\n",
       "      <td>Base Salary/Compensation</td>\n",
       "      <td>Non PreK</td>\n",
       "      <td>PreK-12 Operating</td>\n",
       "      <td>Personal Services - Teachers</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TCHER 2ND GRADE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Regular Instruction</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TEACHER</td>\n",
       "      <td>62237.130</td>\n",
       "      <td>Instruction - Regular</td>\n",
       "      <td>General Purpose School</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364634</th>\n",
       "      <td>Substitute Compensation</td>\n",
       "      <td>Instruction</td>\n",
       "      <td>School Reported</td>\n",
       "      <td>School</td>\n",
       "      <td>Unspecified</td>\n",
       "      <td>Substitute</td>\n",
       "      <td>Benefits</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>PreK-12 Operating</td>\n",
       "      <td>EMPLOYEE BENEFITS</td>\n",
       "      <td>TEACHER SUBS</td>\n",
       "      <td>GENERAL FUND</td>\n",
       "      <td>Teacher, Short Term Sub</td>\n",
       "      <td>Regular</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UNALLOC BUDGETS/SCHOOLS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PROFESSIONAL-INSTRUCTIONAL</td>\n",
       "      <td>22.300</td>\n",
       "      <td>GENERAL MIDDLE/JUNIOR HIGH SCH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>REGULAR INSTRUCTION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47683</th>\n",
       "      <td>Substitute Compensation</td>\n",
       "      <td>Instruction</td>\n",
       "      <td>School Reported</td>\n",
       "      <td>School</td>\n",
       "      <td>Unspecified</td>\n",
       "      <td>Teacher</td>\n",
       "      <td>Substitute Compensation</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>PreK-12 Operating</td>\n",
       "      <td>TEACHER COVERAGE FOR TEACHER</td>\n",
       "      <td>TEACHER SUBS</td>\n",
       "      <td>GENERAL FUND</td>\n",
       "      <td>Teacher, Secondary (High)</td>\n",
       "      <td>Alternative</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NON-PROJECT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PROFESSIONAL-INSTRUCTIONAL</td>\n",
       "      <td>54.166</td>\n",
       "      <td>GENERAL HIGH SCHOOL EDUCATION</td>\n",
       "      <td>NaN</td>\n",
       "      <td>REGULAR INSTRUCTION</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Function          Use          Sharing Reporting  \\\n",
       "134338     Teacher Compensation  Instruction  School Reported    School   \n",
       "206341                 NO_LABEL     NO_LABEL         NO_LABEL  NO_LABEL   \n",
       "326408     Teacher Compensation  Instruction  School Reported    School   \n",
       "364634  Substitute Compensation  Instruction  School Reported    School   \n",
       "47683   Substitute Compensation  Instruction  School Reported    School   \n",
       "\n",
       "       Student_Type Position_Type               Object_Type     Pre_K  \\\n",
       "134338     NO_LABEL       Teacher                  NO_LABEL  NO_LABEL   \n",
       "206341     NO_LABEL      NO_LABEL                  NO_LABEL  NO_LABEL   \n",
       "326408  Unspecified       Teacher  Base Salary/Compensation  Non PreK   \n",
       "364634  Unspecified    Substitute                  Benefits  NO_LABEL   \n",
       "47683   Unspecified       Teacher   Substitute Compensation  NO_LABEL   \n",
       "\n",
       "         Operating_Status            Object_Description             Text_2  \\\n",
       "134338  PreK-12 Operating                           NaN                NaN   \n",
       "206341      Non-Operating           CONTRACTOR SERVICES  BOND EXPENDITURES   \n",
       "326408  PreK-12 Operating  Personal Services - Teachers                NaN   \n",
       "364634  PreK-12 Operating             EMPLOYEE BENEFITS       TEACHER SUBS   \n",
       "47683   PreK-12 Operating  TEACHER COVERAGE FOR TEACHER       TEACHER SUBS   \n",
       "\n",
       "       SubFund_Description       Job_Title_Description       Text_3  \\\n",
       "134338                 NaN         Teacher-Elementary           NaN   \n",
       "206341       BUILDING FUND                     (blank)      Regular   \n",
       "326408                 NaN             TCHER 2ND GRADE          NaN   \n",
       "364634        GENERAL FUND    Teacher, Short Term Sub       Regular   \n",
       "47683         GENERAL FUND  Teacher, Secondary (High)   Alternative   \n",
       "\n",
       "                     Text_4 Sub_Object_Description Location_Description  FTE  \\\n",
       "134338                  NaN                    NaN                  NaN  1.0   \n",
       "206341                  NaN                    NaN                  NaN  NaN   \n",
       "326408  Regular Instruction                    NaN                  NaN  1.0   \n",
       "364634                  NaN                    NaN                  NaN  NaN   \n",
       "47683                   NaN                    NaN                  NaN  NaN   \n",
       "\n",
       "           Function_Description Facility_or_Department  \\\n",
       "134338                      NaN                    NaN   \n",
       "206341                 RGN  GOB                    NaN   \n",
       "326408                      NaN                    NaN   \n",
       "364634  UNALLOC BUDGETS/SCHOOLS                    NaN   \n",
       "47683               NON-PROJECT                    NaN   \n",
       "\n",
       "                    Position_Extra      Total             Program_Description  \\\n",
       "134338               KINDERGARTEN   50471.810                    KINDERGARTEN   \n",
       "206341                UNDESIGNATED   3477.860   BUILDING IMPROVEMENT SERVICES   \n",
       "326408                     TEACHER  62237.130           Instruction - Regular   \n",
       "364634  PROFESSIONAL-INSTRUCTIONAL     22.300  GENERAL MIDDLE/JUNIOR HIGH SCH   \n",
       "47683   PROFESSIONAL-INSTRUCTIONAL     54.166   GENERAL HIGH SCHOOL EDUCATION   \n",
       "\n",
       "              Fund_Description                         Text_1  \n",
       "134338            General Fund                            NaN  \n",
       "206341                     NaN  BUILDING IMPROVEMENT SERVICES  \n",
       "326408  General Purpose School                            NaN  \n",
       "364634                     NaN            REGULAR INSTRUCTION  \n",
       "47683                      NaN            REGULAR INSTRUCTION  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take a look\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Encode the targets as categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function            category\n",
      "Object_Type         category\n",
      "Operating_Status    category\n",
      "Position_Type       category\n",
      "Pre_K               category\n",
      "Reporting           category\n",
      "Sharing             category\n",
      "Student_Type        category\n",
      "Use                 category\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "### bind variable LABELS - these are actually the targets and we're going to one-hot encode them...\n",
    "LABELS = ['Function',  'Use',  'Sharing',  'Reporting',  'Student_Type',  'Position_Type', \n",
    "          'Object_Type',  'Pre_K',  'Operating_Status']\n",
    "\n",
    "### This turns out to be key.  Submission requires the dummy versions of these vars to be in this order.\n",
    "LABELS.sort()\n",
    "\n",
    "# Define the lambda function: categorize_label\n",
    "categorize_label = lambda x: x.astype('category')\n",
    "\n",
    "# Convert df[LABELS] to a categorical type\n",
    "df[LABELS] = df[LABELS].apply(categorize_label, axis=0)\n",
    "\n",
    "# Print the converted dtypes\n",
    "print(df[LABELS].dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's save the unique labels for each output (category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a dictionary\n",
    "the_labels = {col : df[col].unique().tolist() for col in df[LABELS].columns}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Instruction',\n",
       " 'NO_LABEL',\n",
       " 'O&M',\n",
       " 'Pupil Services & Enrichment',\n",
       " 'ISPD',\n",
       " 'Leadership',\n",
       " 'Business Services',\n",
       " 'Untracked Budget Set-Aside']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "the_labels['Use']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up a train-test split  for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMERIC_COLUMNS = ['FTE', 'Total']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the new DataFrame: numeric_data_only\n",
    "numeric_data_only = df[NUMERIC_COLUMNS].fillna(-1000)\n",
    "\n",
    "# Get labels and convert to dummy variables: label_dummies\n",
    "label_dummies = pd.get_dummies(df[LABELS])\n",
    "\n",
    "# Create training and test sets\n",
    "X_train, X_test, y_train, y_test = multilabel_train_test_split(numeric_data_only,\n",
    "                                                               label_dummies,\n",
    "                                                               size=0.2, \n",
    "                                                               seed=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__======================== Begin Model 0 =========================__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start with a simple model\n",
    "\n",
    "The first model ignores everything but the two numeric columns to get started and check for correct format (104 columns of predictions).  Create a multi-label classifier clf by placing LogisticRegression() inside OneVsRestClassifier()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit time: 126.77165587835282 seconds\n"
     ]
    }
   ],
   "source": [
    "# Make the classifier\n",
    "mod0 = OneVsRestClassifier(LogisticRegression(), n_jobs=-1)\n",
    "\n",
    "start = timer()\n",
    "# Fit the classifier to the training data\n",
    "mod0.fit(X_train, y_train)\n",
    "end = timer()\n",
    "print('fit time: {:0.2f} seconds'.format(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The accuracy metric is not applicable here because the predictions (mod.predict(X_test) from this model are not correct. ftl.flat_to_labels(probas) gets the predictions that it should produce.\n",
    "\n",
    "The overall model should be constrained to predict the highest probabilty label within a target. 1-vs-103 doesn't work that way; it produces a set of 104 classifiers that are independent.\n",
    "\n",
    "##### The call to flat_to_labels produces 9 columns of targets, each populated with the appropriate per-target labels.   We also need to apply it to the y_test since that has been one-hot encoded. \n",
    "##### FTL restores the original Y data.  We then produce the actual predictions from the predicted probabilities.  Within a single target the most likely label is asserted as the prediction.  \n",
    "\n",
    "##### With data in this format the standard sklearn metrics can be applied to each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ftl wants ndarray, not pd.Dataframe\n",
    "the_ys = ftl.flat_to_labels(y_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### First we need the probabilities, not the predicted labels\n",
    "mod0_train_probas = mod0.predict_proba(X_train)\n",
    "mod0_test_probas = mod0.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2759602773093498"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check accuracy on first column (target: Function)\n",
    "accuracy_score(the_ys[:, 0], ftl.flat_to_labels(mod0_test_probas)[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saus\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1996178882196799"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check F1 on first column (target: Function)\n",
    "f1_score(the_ys[:, 0], ftl.flat_to_labels(mod0_test_probas)[:, 0], average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show metrics for each target and average for all targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_f1(true, pred):\n",
    "    the_scores = []\n",
    "    for target in range(len(LABELS)):\n",
    "        the_score = f1_score(true[:, target], pred[:, target], average='weighted')\n",
    "        print('F1 score for target {}: {:.3f}'.format(LABELS[target], the_score))\n",
    "        the_scores.append(the_score)\n",
    "    print('Average F1 score for all targets : {:.3f}'.format(np.mean(the_scores)))\n",
    "\n",
    "def report_accuracy(true, pred):\n",
    "    the_scores = []\n",
    "    for target in range(len(LABELS)):\n",
    "        the_score = accuracy_score(true[:, target], pred[:, target])\n",
    "        print('Accuracy score for target {}: {:.3f}'.format(LABELS[target], the_score))\n",
    "        the_scores.append(the_score)\n",
    "    print('Average accuracy score for all targets : {:.3f}'.format(np.mean(the_scores)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for target Function: 0.276\n",
      "Accuracy score for target Object_Type: 0.425\n",
      "Accuracy score for target Operating_Status: 0.859\n",
      "Accuracy score for target Position_Type: 0.373\n",
      "Accuracy score for target Pre_K: 0.766\n",
      "Accuracy score for target Reporting: 0.641\n",
      "Accuracy score for target Sharing: 0.634\n",
      "Accuracy score for target Student_Type: 0.557\n",
      "Accuracy score for target Use: 0.508\n",
      "Average accuracy score for all targets : 0.560\n"
     ]
    }
   ],
   "source": [
    "report_accuracy(the_ys, ftl.flat_to_labels(mod0_test_probas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saus\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for target Function: 0.200\n",
      "F1 score for target Object_Type: 0.297\n",
      "F1 score for target Operating_Status: 0.794\n",
      "F1 score for target Position_Type: 0.277\n",
      "F1 score for target Pre_K: 0.665\n",
      "F1 score for target Reporting: 0.501\n",
      "F1 score for target Sharing: 0.492\n",
      "F1 score for target Student_Type: 0.399\n",
      "F1 score for target Use: 0.343\n",
      "Average F1 score for all targets : 0.441\n"
     ]
    }
   ],
   "source": [
    "report_f1(the_ys, ftl.flat_to_labels(mod0_test_probas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Log loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.353521736853036"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_multi_log_loss(mod0_train_probas, y_train.values, BOX_PLOTS_COLUMN_INDICES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3557282270290794"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_multi_log_loss(mod0_test_probas, y_test.values, BOX_PLOTS_COLUMN_INDICES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submitted prediction file.  Scored 1.33."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__================ End of Model 0 ==================__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__================ Begin Mod0_1 *(add scaling)* ==================__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit time: 37.78306902934091 seconds\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the classifier\n",
    "mod0_1 = Pipeline([('scale', StandardScaler()),\n",
    "                   ('clf',   OneVsRestClassifier(LogisticRegression(), n_jobs=-1))\n",
    "                  ])\n",
    "\n",
    "start = timer()\n",
    "# Fit the classifier to the training data\n",
    "mod0_1.fit(X_train, y_train)\n",
    "end = timer()\n",
    "print('fit time: {:0.2f} seconds'.format(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fits much faster with scaled numerical input.  Classification is not improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict probilities\n",
    "mod0_1_test_probas = mod0_1.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for target Function: 0.283\n",
      "Accuracy score for target Object_Type: 0.417\n",
      "Accuracy score for target Operating_Status: 0.859\n",
      "Accuracy score for target Position_Type: 0.383\n",
      "Accuracy score for target Pre_K: 0.766\n",
      "Accuracy score for target Reporting: 0.641\n",
      "Accuracy score for target Sharing: 0.634\n",
      "Accuracy score for target Student_Type: 0.557\n",
      "Accuracy score for target Use: 0.508\n",
      "Average accuracy score for all targets : 0.561\n"
     ]
    }
   ],
   "source": [
    "# get accuracy.  The ys are the same as before (test set hasn't changed)\n",
    "report_accuracy(the_ys, ftl.flat_to_labels(mod0_1_test_probas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saus\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for target Function: 0.206\n",
      "F1 score for target Object_Type: 0.281\n",
      "F1 score for target Operating_Status: 0.794\n",
      "F1 score for target Position_Type: 0.289\n",
      "F1 score for target Pre_K: 0.665\n",
      "F1 score for target Reporting: 0.501\n",
      "F1 score for target Sharing: 0.492\n",
      "F1 score for target Student_Type: 0.399\n",
      "F1 score for target Use: 0.343\n",
      "Average F1 score for all targets : 0.441\n"
     ]
    }
   ],
   "source": [
    "report_f1(the_ys, ftl.flat_to_labels(mod0_1_test_probas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3230752955144411"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_multi_log_loss(mod0_1_test_probas, y_test.values, BOX_PLOTS_COLUMN_INDICES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__================ End of Mod0_1 ==================__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__================ Begin Mod0_1a *(add scaling; convert total to absolute value)* ==================__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rectify_total(in_df):\n",
    "    # gets a copy of the desired columns and fills nans with 0\n",
    "    rval = in_df[['FTE','Total']].fillna(0) \n",
    "    # now munge the copy and return it\n",
    "    rval.loc[:, 'Total'] = np.abs(rval['Total'])\n",
    "    return rval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert X_train.FTE to abs val before fitting; also fill both with 0 == fill with -9999 does bad things to mean later\n",
    "get_numeric_data_abs = FunctionTransformer(lambda x: rectify_total(x[NUMERIC_COLUMNS]), validate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit time: 34.34 seconds\n"
     ]
    }
   ],
   "source": [
    "# make a classifier; abs val Total before fitting\n",
    "\n",
    "mod0_1a = Pipeline([('getnum', get_numeric_data_abs),\n",
    "                   ('scale', StandardScaler()),\n",
    "                   ('clf',   OneVsRestClassifier(LogisticRegression(), n_jobs=-1))\n",
    "                  ])\n",
    "\n",
    "start = timer()\n",
    "# Fit the classifier to the training data\n",
    "mod0_1a.fit(X_train, y_train)\n",
    "end = timer()\n",
    "print('fit time: {:0.2f} seconds'.format(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification is improved slightly by using absolute value of Total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict probilities\n",
    "mod0_1a_test_probas = mod0_1a.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for target Function: 0.292\n",
      "Accuracy score for target Object_Type: 0.447\n",
      "Accuracy score for target Operating_Status: 0.859\n",
      "Accuracy score for target Position_Type: 0.392\n",
      "Accuracy score for target Pre_K: 0.766\n",
      "Accuracy score for target Reporting: 0.643\n",
      "Accuracy score for target Sharing: 0.636\n",
      "Accuracy score for target Student_Type: 0.563\n",
      "Accuracy score for target Use: 0.517\n",
      "Average accuracy score for all targets : 0.568\n"
     ]
    }
   ],
   "source": [
    "# get accuracy.  The ys are the same as before (test set hasn't changed)\n",
    "report_accuracy(the_ys, ftl.flat_to_labels(mod0_1a_test_probas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saus\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for target Function: 0.215\n",
      "F1 score for target Object_Type: 0.332\n",
      "F1 score for target Operating_Status: 0.795\n",
      "F1 score for target Position_Type: 0.299\n",
      "F1 score for target Pre_K: 0.665\n",
      "F1 score for target Reporting: 0.506\n",
      "F1 score for target Sharing: 0.498\n",
      "F1 score for target Student_Type: 0.416\n",
      "F1 score for target Use: 0.363\n",
      "Average F1 score for all targets : 0.454\n"
     ]
    }
   ],
   "source": [
    "report_f1(the_ys, ftl.flat_to_labels(mod0_1a_test_probas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2952599960510929"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_multi_log_loss(mod0_1a_test_probas, y_test.values, BOX_PLOTS_COLUMN_INDICES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__================ End of Mod0_1a ==================__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__================ Begin Mod0_2 *(add scaling, default imputer)* ==================__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Redo train/test split so we can use Imputer (instead of replacing NaNs by -9999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = multilabel_train_test_split(df[NUMERIC_COLUMNS],\n",
    "#                                                                label_dummies,\n",
    "#                                                                size=0.2, \n",
    "#                                                                seed=123)\n",
    "### Work on the full input data; we'll select numerics in the pipeline.\n",
    "X_train, X_test, y_train, y_test = multilabel_train_test_split(df,\n",
    "                                                               label_dummies,\n",
    "                                                               size=0.2, \n",
    "                                                               seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit time: 3.8e+01 seconds\n"
     ]
    }
   ],
   "source": [
    "# make a FunctionTransformer and tell the Pipeline we'll deal with the NaNs (Imputer does it)\n",
    "\n",
    "get_numeric_data = FunctionTransformer(lambda x: x[NUMERIC_COLUMNS], validate=False)\n",
    "\n",
    "# Instantiate the classifier: clf\n",
    "mod0_2 = Pipeline([('select num', get_numeric_data),\n",
    "                   ('imputer', Imputer()),\n",
    "                   ('scale', StandardScaler()),\n",
    "                   ('clf',   OneVsRestClassifier(LogisticRegression(), n_jobs=-1))\n",
    "                  ])\n",
    "\n",
    "start = timer()\n",
    "# Fit the classifier to the training data\n",
    "mod0_2.fit(X_train, y_train)\n",
    "end = timer()\n",
    "print('fit time: {:0.2} seconds'.format(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fits 10% faster with mean imputation for numerical input.  Classification is somewhat degraded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict probabilties\n",
    "mod0_2_test_probas = mod0_2.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for target Function: 0.281\n",
      "Accuracy score for target Object_Type: 0.197\n",
      "Accuracy score for target Operating_Status: 0.859\n",
      "Accuracy score for target Position_Type: 0.322\n",
      "Accuracy score for target Pre_K: 0.766\n",
      "Accuracy score for target Reporting: 0.641\n",
      "Accuracy score for target Sharing: 0.634\n",
      "Accuracy score for target Student_Type: 0.557\n",
      "Accuracy score for target Use: 0.508\n",
      "Average accuracy score for all targets : 0.529\n"
     ]
    }
   ],
   "source": [
    "report_accuracy(the_ys, ftl.flat_to_labels(mod0_2_test_probas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saus\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for target Function: 0.163\n",
      "F1 score for target Object_Type: 0.098\n",
      "F1 score for target Operating_Status: 0.793\n",
      "F1 score for target Position_Type: 0.201\n",
      "F1 score for target Pre_K: 0.665\n",
      "F1 score for target Reporting: 0.501\n",
      "F1 score for target Sharing: 0.492\n",
      "F1 score for target Student_Type: 0.398\n",
      "F1 score for target Use: 0.342\n",
      "Average F1 score for all targets : 0.406\n"
     ]
    }
   ],
   "source": [
    "report_f1(the_ys, ftl.flat_to_labels(mod0_2_test_probas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3624418801241875"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    multi_multi_log_loss(mod0_2_test_probas, y_test.values, BOX_PLOTS_COLUMN_INDICES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__================ End of Mod0_2 ==================__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__================ Beginning of Mod1 ==================__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add text processing to the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combining text columns for tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge all text feature strings into one string.\n",
    "def combine_text_columns(df, to_drop=NUMERIC_COLUMNS + LABELS):\n",
    "    \"\"\" converts all text columns in each row of df to single string \"\"\"\n",
    "    # Drop non-text columns that are in the df\n",
    "    to_drop = set(to_drop) & set(df.columns.tolist())\n",
    "    text_data = df.drop(to_drop, axis=1)  \n",
    "    # Replace nans with blanks\n",
    "    text_data.fillna('', inplace=True)    \n",
    "    # Join all text items in a row that have a space in between\n",
    "    return text_data.apply(lambda x: \" \".join(x), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rebinding X/y train/test...\n",
    "\n",
    "It needs to be done because X is a different feature subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dummy encoding of the labels\n",
    "dummy_labels = pd.get_dummies(df[LABELS])\n",
    "\n",
    "# Get the columns that are features in the original df\n",
    "NON_LABELS = [c for c in df.columns if c not in LABELS]\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = multilabel_train_test_split(df[NON_LABELS],\n",
    "                                                               dummy_labels,\n",
    "                                                               0.2, \n",
    "                                                               seed=123)\n",
    "# Preprocess the text data: get_text_data\n",
    "get_text_data = FunctionTransformer(combine_text_columns, validate=False)\n",
    "\n",
    "# Preprocess the numeric data: get_numeric_data\n",
    "get_numeric_data = FunctionTransformer(lambda x: x[NUMERIC_COLUMNS], validate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit time: 469.1 seconds\n"
     ]
    }
   ],
   "source": [
    "# Complete the pipeline: pl\n",
    "mod1 = Pipeline([\n",
    "        ('union', FeatureUnion(\n",
    "            transformer_list = [\n",
    "                ('numeric_features', Pipeline([('selector', get_numeric_data),\n",
    "                                               ('imputer', Imputer())])),\n",
    "                ('text_features', Pipeline([('selector', get_text_data),\n",
    "                                            ('vectorizer', CountVectorizer())]))\n",
    "             ])),\n",
    "        ('clf', OneVsRestClassifier(LogisticRegression(), n_jobs=-1))\n",
    "    ])\n",
    "\n",
    "start = timer()\n",
    "# Fit to the training data\n",
    "mod1.fit(X_train, y_train)\n",
    "end = timer()\n",
    "print('fit time: {:0.1f} seconds'.format(end - start))\n",
    "# old 489 sec; new machine 418 sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict.proba time: 17.28 seconds\n"
     ]
    }
   ],
   "source": [
    "### For log loss we need the probabilities, not the predicted labels\n",
    "start = timer()\n",
    "mod1_train_probas = mod1.predict_proba(X_train)\n",
    "mod1_test_probas = mod1.predict_proba(X_test)\n",
    "end = timer()\n",
    "print('Predict.proba time: {:0.2f} seconds'.format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log loss on training set: 0.5110\n",
      "log loss on training set: 0.5117\n"
     ]
    }
   ],
   "source": [
    "print('log loss on training set: {:0.4f}'.format(multi_multi_log_loss(mod1_train_probas, \n",
    "                                                                      y_train.values, BOX_PLOTS_COLUMN_INDICES)))\n",
    "print('log loss on test set: {:0.4f}'.format(multi_multi_log_loss(mod1_test_probas, \n",
    "                                                                      y_test.values, BOX_PLOTS_COLUMN_INDICES)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for target Function: 0.785\n",
      "F1 score for target Object_Type: 0.869\n",
      "F1 score for target Operating_Status: 0.931\n",
      "F1 score for target Position_Type: 0.834\n",
      "F1 score for target Pre_K: 0.951\n",
      "F1 score for target Reporting: 0.858\n",
      "F1 score for target Sharing: 0.793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saus\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for target Student_Type: 0.867\n",
      "F1 score for target Use: 0.791\n",
      "Average F1 score for all targets : 0.853\n"
     ]
    }
   ],
   "source": [
    "report_f1(the_ys, ftl.flat_to_labels(mod1_test_probas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for target Function: 0.798\n",
      "Accuracy score for target Object_Type: 0.873\n",
      "Accuracy score for target Operating_Status: 0.936\n",
      "Accuracy score for target Position_Type: 0.842\n",
      "Accuracy score for target Pre_K: 0.953\n",
      "Accuracy score for target Reporting: 0.865\n",
      "Accuracy score for target Sharing: 0.815\n",
      "Accuracy score for target Student_Type: 0.873\n",
      "Accuracy score for target Use: 0.803\n",
      "Average accuracy score for all targets : 0.862\n"
     ]
    }
   ],
   "source": [
    "report_accuracy(the_ys, ftl.flat_to_labels(mod1_test_probas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicted, submitted and scored with log-loss of 0.75."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### =============================== End of Mod1 ============================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ================= Beginning of Mod1_1; just the text features (no numerics) ==================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funny thing, but when I simplify the pipeline (remove feature union and selection/preprocessing for numeric data), OneVsRest fails with n_jobs=-1.  Runs without it, but slow (~2x)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the pipeline, but ignore numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit time: 826.2440557586237 seconds\n"
     ]
    }
   ],
   "source": [
    "# Complete the pipeline: pl\n",
    "mod1_1 = Pipeline([('selector', get_text_data),\n",
    "                   ('vectorizer', CountVectorizer()),\n",
    "                   ('clf', OneVsRestClassifier(LogisticRegression()))])\n",
    "start = timer()\n",
    "# Fit to the training data\n",
    "mod1_1.fit(X_train, y_train)\n",
    "end = timer()\n",
    "print('fit time: {:0.2f} seconds'.format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict.proba time: 17.47 seconds\n"
     ]
    }
   ],
   "source": [
    "### For log loss we need the probabilities, not the predicted labels\n",
    "start = timer()\n",
    "mod1_1_train_probas = mod1_1.predict_proba(X_train)\n",
    "mod1_1_test_probas = mod1_1.predict_proba(X_test)\n",
    "end = timer()\n",
    "print('Predict.proba time: {:0.2f} seconds'.format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log loss on training set: 0.0874\n",
      "log loss on test set: 0.0940\n"
     ]
    }
   ],
   "source": [
    "print('log loss on training set: {:0.4f}'.format(multi_multi_log_loss(mod1_1_train_probas, \n",
    "                                                                      y_train.values, BOX_PLOTS_COLUMN_INDICES)))\n",
    "print('log loss on test set: {:0.4f}'.format(multi_multi_log_loss(mod1_1_test_probas, \n",
    "                                                                      y_test.values, BOX_PLOTS_COLUMN_INDICES)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for target Function: 0.955\n",
      "F1 score for target Object_Type: 0.984\n",
      "F1 score for target Operating_Status: 0.984\n",
      "F1 score for target Position_Type: 0.982\n",
      "F1 score for target Pre_K: 0.990\n",
      "F1 score for target Reporting: 0.972\n",
      "F1 score for target Sharing: 0.962\n",
      "F1 score for target Student_Type: 0.973\n",
      "F1 score for target Use: 0.961\n",
      "Average F1 score for all targets : 0.974\n"
     ]
    }
   ],
   "source": [
    "report_f1(the_ys, ftl.flat_to_labels(mod1_1_test_probas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for target Function: 0.955\n",
      "Accuracy score for target Object_Type: 0.984\n",
      "Accuracy score for target Operating_Status: 0.985\n",
      "Accuracy score for target Position_Type: 0.983\n",
      "Accuracy score for target Pre_K: 0.990\n",
      "Accuracy score for target Reporting: 0.973\n",
      "Accuracy score for target Sharing: 0.962\n",
      "Accuracy score for target Student_Type: 0.973\n",
      "Accuracy score for target Use: 0.961\n",
      "Average accuracy score for all targets : 0.974\n"
     ]
    }
   ],
   "source": [
    "report_accuracy(the_ys, ftl.flat_to_labels(mod1_1_test_probas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict on holdout set and create submission file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # Load the holdout data: holdout\n",
    "# ### Over here the file is TestData.csv\n",
    "# holdout = pd.read_csv('data/TestData.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = timer()\n",
    "# # Generate predictions: predictions\n",
    "# mod1_1_predictions = mod1_1.predict_proba(holdout)\n",
    "# end = timer()\n",
    "# print('predict time: {} seconds'.format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_mod1_1 = pd.DataFrame(columns=pd.get_dummies(df[LABELS], prefix_sep='__').columns, \n",
    "#                              index=holdout.index,\n",
    "#                              data=mod1_1_predictions)\n",
    "\n",
    "# pred_mod1_1.to_csv('pred_mod1_1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 finish!!  0.6827 on holdout set at Drivendata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ======================== End of Mod1_1 ==================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ==================== Begin Mod1_1_1 - work around the n_jobs problem ================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Okay, the numeric data is not helpful.\n",
    "\n",
    "Between mod1 and mod3 (mod2 only changes the classifier to RandomForest, leaving preprocessing alone) the changes are:\n",
    "\n",
    "1. tokenize on alphanumeric (instead of default)\n",
    "2. Add bigrams to CountVectorizer (previously was used with default settings).  This doubles the size of the wordvec space.\n",
    "3. Dimension reduction with SelectKBest using chi-squared (300 features)\n",
    "\n",
    "Some things to note about the default CountVectorizer:\n",
    "1) All strings are downcased\n",
    "2) The default setting selects tokens of 2 or more alphanumeric characters (punctuation is completely ignored and always treated as a token separator).  This means single letter or digit tokens are ignored.\n",
    "3) If the vectorizer is used to transform another input (e.g. test), any tokens not in the original corpus are ignored.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One other way to work around bug exposed with CountVectorizer/OneVsRest/Logistic would be to replace all the numeric values with 0.  The classifiers  should ignore (and might work with n_jobs=-1).\n",
    "\n",
    "Yes, this works well and uses all processors yielding the same results as the slower, 1-processor version above.  Fits in 464 sec instead of 827 sec."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "why am I recreating X_train, etc. here?  I was doing it because I used sample to downsize data set....\n",
    "I'll leave it here for now.  So without the bigrams this should take about 7 minutes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dummy encoding of the labels\n",
    "dummy_labels = pd.get_dummies(df[LABELS])\n",
    "\n",
    "# Get the columns that are features in the original df\n",
    "NON_LABELS = [c for c in df.columns if c not in LABELS]\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = multilabel_train_test_split(df[NON_LABELS],\n",
    "                                                               dummy_labels,\n",
    "                                                               0.2, \n",
    "                                                               seed=123)\n",
    "# Preprocess the text data: get_text_data\n",
    "get_text_data = FunctionTransformer(combine_text_columns, validate=False)\n",
    "\n",
    "# Use all 0s instead of noise: get_numeric_data\n",
    "get_numeric_data_hack = FunctionTransformer(lambda x: np.zeros(x[NUMERIC_COLUMNS].shape, dtype=np.float), validate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit time: 384.25 seconds\n"
     ]
    }
   ],
   "source": [
    "# Complete the pipeline: pl\n",
    "mod_1_1_1 = Pipeline([\n",
    "        ('union', FeatureUnion(\n",
    "            transformer_list = [\n",
    "                ('numeric_features', Pipeline([('selector', get_numeric_data_hack),\n",
    "                                               ('imputer', Imputer())])),\n",
    "                ('text_features', Pipeline([('selector', get_text_data),\n",
    "                                            ('vectorizer', CountVectorizer())]))\n",
    "             ])),\n",
    "        ('clf', OneVsRestClassifier(LogisticRegression(), n_jobs=-1))\n",
    "    ])\n",
    "\n",
    "start = timer()\n",
    "# Fit to the training data\n",
    "mod_1_1_1.fit(X_train, y_train)\n",
    "end = timer()\n",
    "print('fit time: {:0.2f} seconds'.format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict.proba time: 16.33 seconds\n"
     ]
    }
   ],
   "source": [
    "### For log loss we need the probabilities, not the predicted labels\n",
    "start = timer()\n",
    "mod_1_1_1_train_probas = mod_1_1_1.predict_proba(X_train)\n",
    "mod_1_1_1_test_probas = mod_1_1_1.predict_proba(X_test)\n",
    "end = timer()\n",
    "print('Predict.proba time: {:0.2f} seconds'.format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log loss on training set: 0.0874\n",
      "log loss on training set: 0.0940\n"
     ]
    }
   ],
   "source": [
    "print('log loss on training set: {:0.4f}'.format(multi_multi_log_loss(mod_1_1_1_train_probas, \n",
    "                                                                      y_train.values, BOX_PLOTS_COLUMN_INDICES)))\n",
    "print('log loss on training set: {:0.4f}'.format(multi_multi_log_loss(mod_1_1_1_test_probas, \n",
    "                                                                      y_test.values, BOX_PLOTS_COLUMN_INDICES)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for target Function: 0.955\n",
      "F1 score for target Object_Type: 0.984\n",
      "F1 score for target Operating_Status: 0.984\n",
      "F1 score for target Position_Type: 0.982\n",
      "F1 score for target Pre_K: 0.990\n",
      "F1 score for target Reporting: 0.972\n",
      "F1 score for target Sharing: 0.962\n",
      "F1 score for target Student_Type: 0.973\n",
      "F1 score for target Use: 0.961\n",
      "Average F1 score for all targets : 0.974\n"
     ]
    }
   ],
   "source": [
    "report_f1(the_ys, ftl.flat_to_labels(mod_1_1_1_test_probas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for target Function: 0.955\n",
      "Accuracy score for target Object_Type: 0.984\n",
      "Accuracy score for target Operating_Status: 0.985\n",
      "Accuracy score for target Position_Type: 0.983\n",
      "Accuracy score for target Pre_K: 0.990\n",
      "Accuracy score for target Reporting: 0.973\n",
      "Accuracy score for target Sharing: 0.963\n",
      "Accuracy score for target Student_Type: 0.973\n",
      "Accuracy score for target Use: 0.961\n",
      "Average accuracy score for all targets : 0.974\n"
     ]
    }
   ],
   "source": [
    "report_accuracy(the_ys, ftl.flat_to_labels(mod_1_1_1_test_probas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### =============================== End of mod_1_1_1 ============================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ====================== Beginning of mod_1_1_2: now add bigrams ======================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Super strange:  I can run this with 90% of the data and it works very well.  If I use all the data, it seems to fit and then never comes back. Even though machine is not busy, it refuses to be interrupted.  Have broken this out into its own file for experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Complete the pipeline: pl\n",
    "# mod_1_1_2 = Pipeline([\n",
    "#         ('union', FeatureUnion(\n",
    "#             transformer_list = [\n",
    "#                 ('numeric_features', Pipeline([('selector', get_numeric_data_hack),\n",
    "#                                                ('imputer', Imputer())])),\n",
    "#                 ('text_features', Pipeline([('selector', get_text_data),\n",
    "#                                             ('vectorizer', CountVectorizer(ngram_range=(1,2)))]))\n",
    "#              ])),\n",
    "#         ('clf', OneVsRestClassifier(LogisticRegression(), n_jobs=-1))\n",
    "#     ])\n",
    "\n",
    "# start = timer()\n",
    "# # Fit to the training data\n",
    "# mod_1_1_2.fit(X_train, y_train)\n",
    "# end = timer()\n",
    "# print('fit time: {:0.2f} seconds'.format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict.proba time: 16.33 seconds\n"
     ]
    }
   ],
   "source": [
    "# ### For log loss we need the probabilities, not the predicted labels\n",
    "# start = timer()\n",
    "# mod_1_1_2_train_probas = mod_1_1_2.predict_proba(X_train)\n",
    "# mod_1_1_2_test_probas = mod_1_1_2.predict_proba(X_test)\n",
    "# end = timer()\n",
    "# print('Predict.proba time: {:0.2f} seconds'.format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log loss on training set: 0.0874\n",
      "log loss on training set: 0.0940\n"
     ]
    }
   ],
   "source": [
    "# print('log loss on training set: {:0.4f}'.format(multi_multi_log_loss(mod_1_1_2_train_probas, \n",
    "#                                                                       y_train.values, BOX_PLOTS_COLUMN_INDICES)))\n",
    "# print('log loss on training set: {:0.4f}'.format(multi_multi_log_loss(mod_1_1_2_test_probas, \n",
    "#                                                                       y_test.values, BOX_PLOTS_COLUMN_INDICES)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for target Function: 0.955\n",
      "F1 score for target Object_Type: 0.984\n",
      "F1 score for target Operating_Status: 0.984\n",
      "F1 score for target Position_Type: 0.982\n",
      "F1 score for target Pre_K: 0.990\n",
      "F1 score for target Reporting: 0.972\n",
      "F1 score for target Sharing: 0.962\n",
      "F1 score for target Student_Type: 0.973\n",
      "F1 score for target Use: 0.961\n",
      "Average F1 score for all targets : 0.974\n"
     ]
    }
   ],
   "source": [
    "# report_f1(the_ys, ftl.flat_to_labels(mod_1_1_2_test_probas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for target Function: 0.955\n",
      "Accuracy score for target Object_Type: 0.984\n",
      "Accuracy score for target Operating_Status: 0.985\n",
      "Accuracy score for target Position_Type: 0.983\n",
      "Accuracy score for target Pre_K: 0.990\n",
      "Accuracy score for target Reporting: 0.973\n",
      "Accuracy score for target Sharing: 0.963\n",
      "Accuracy score for target Student_Type: 0.973\n",
      "Accuracy score for target Use: 0.961\n",
      "Average accuracy score for all targets : 0.974\n"
     ]
    }
   ],
   "source": [
    "# report_accuracy(the_ys, ftl.flat_to_labels(mod_1_1_2_test_probas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ======================== End of mod_1_1_2: now add bigrams ========================================="
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
