{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter 1\n",
    "\n",
    "#### Summarizing the data\n",
    "You'll continue your EDA in this exercise by computing summary statistics for the numeric data in the dataset. The data has been pre-loaded into a DataFrame called df.\n",
    "\n",
    "You can use df.info() in the IPython Shell to determine which columns of the data are numeric, specifically type float64. You'll notice that there are two numeric columns, called FTE and Total.\n",
    "\n",
    "FTE: Stands for \"full-time equivalent\". If the budget item is associated to an employee, this number tells us the percentage of full-time that the employee works. A value of 1 means the associated employee works for the school full-time. A value close to 0 means the item is associated to a part-time or contracted employee.\n",
    "Total: Stands for the total cost of the expenditure. This number tells us how much the budget item cost.\n",
    "After printing summary statistics for the numeric data, your job is to plot a histogram of the non-null FTE column to see the distribution of part-time and full-time employees in the dataset.\n",
    "\n",
    "##### INSTRUCTIONS\n",
    "\n",
    "Print summary statistics of the numeric columns in the DataFrame df using the .describe() method.\n",
    "Import matplotlib.pyplot as plt.\n",
    "Create a histogram of the non-null 'FTE' column. You can do this by passing df['FTE'].dropna() to plt.hist().\n",
    "The title has been specified and axes have been labeled, so hit 'Submit Answer' to see how often school employees work full-time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "pd.set_option('display.max_columns', 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The data here is different from what they use in the tutorial.  In particular, plots don't look the same.\n",
    "\n",
    "The tutorial dataset is downsized to 1500 rows.  Don't know if it's specially selected but I assume they made sure labels were all populated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/TrainingData.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Function</th>\n",
       "      <th>Use</th>\n",
       "      <th>Sharing</th>\n",
       "      <th>Reporting</th>\n",
       "      <th>Student_Type</th>\n",
       "      <th>Position_Type</th>\n",
       "      <th>Object_Type</th>\n",
       "      <th>Pre_K</th>\n",
       "      <th>Operating_Status</th>\n",
       "      <th>Object_Description</th>\n",
       "      <th>Text_2</th>\n",
       "      <th>SubFund_Description</th>\n",
       "      <th>Job_Title_Description</th>\n",
       "      <th>Text_3</th>\n",
       "      <th>Text_4</th>\n",
       "      <th>Sub_Object_Description</th>\n",
       "      <th>Location_Description</th>\n",
       "      <th>FTE</th>\n",
       "      <th>Function_Description</th>\n",
       "      <th>Facility_or_Department</th>\n",
       "      <th>Position_Extra</th>\n",
       "      <th>Total</th>\n",
       "      <th>Program_Description</th>\n",
       "      <th>Fund_Description</th>\n",
       "      <th>Text_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>134338</th>\n",
       "      <td>Teacher Compensation</td>\n",
       "      <td>Instruction</td>\n",
       "      <td>School Reported</td>\n",
       "      <td>School</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>Teacher</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>PreK-12 Operating</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Teacher-Elementary</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>KINDERGARTEN</td>\n",
       "      <td>50471.810</td>\n",
       "      <td>KINDERGARTEN</td>\n",
       "      <td>General Fund</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206341</th>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>Non-Operating</td>\n",
       "      <td>CONTRACTOR SERVICES</td>\n",
       "      <td>BOND EXPENDITURES</td>\n",
       "      <td>BUILDING FUND</td>\n",
       "      <td>(blank)</td>\n",
       "      <td>Regular</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RGN  GOB</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UNDESIGNATED</td>\n",
       "      <td>3477.860</td>\n",
       "      <td>BUILDING IMPROVEMENT SERVICES</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BUILDING IMPROVEMENT SERVICES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326408</th>\n",
       "      <td>Teacher Compensation</td>\n",
       "      <td>Instruction</td>\n",
       "      <td>School Reported</td>\n",
       "      <td>School</td>\n",
       "      <td>Unspecified</td>\n",
       "      <td>Teacher</td>\n",
       "      <td>Base Salary/Compensation</td>\n",
       "      <td>Non PreK</td>\n",
       "      <td>PreK-12 Operating</td>\n",
       "      <td>Personal Services - Teachers</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TCHER 2ND GRADE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Regular Instruction</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TEACHER</td>\n",
       "      <td>62237.130</td>\n",
       "      <td>Instruction - Regular</td>\n",
       "      <td>General Purpose School</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364634</th>\n",
       "      <td>Substitute Compensation</td>\n",
       "      <td>Instruction</td>\n",
       "      <td>School Reported</td>\n",
       "      <td>School</td>\n",
       "      <td>Unspecified</td>\n",
       "      <td>Substitute</td>\n",
       "      <td>Benefits</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>PreK-12 Operating</td>\n",
       "      <td>EMPLOYEE BENEFITS</td>\n",
       "      <td>TEACHER SUBS</td>\n",
       "      <td>GENERAL FUND</td>\n",
       "      <td>Teacher, Short Term Sub</td>\n",
       "      <td>Regular</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UNALLOC BUDGETS/SCHOOLS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PROFESSIONAL-INSTRUCTIONAL</td>\n",
       "      <td>22.300</td>\n",
       "      <td>GENERAL MIDDLE/JUNIOR HIGH SCH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>REGULAR INSTRUCTION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47683</th>\n",
       "      <td>Substitute Compensation</td>\n",
       "      <td>Instruction</td>\n",
       "      <td>School Reported</td>\n",
       "      <td>School</td>\n",
       "      <td>Unspecified</td>\n",
       "      <td>Teacher</td>\n",
       "      <td>Substitute Compensation</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>PreK-12 Operating</td>\n",
       "      <td>TEACHER COVERAGE FOR TEACHER</td>\n",
       "      <td>TEACHER SUBS</td>\n",
       "      <td>GENERAL FUND</td>\n",
       "      <td>Teacher, Secondary (High)</td>\n",
       "      <td>Alternative</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NON-PROJECT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PROFESSIONAL-INSTRUCTIONAL</td>\n",
       "      <td>54.166</td>\n",
       "      <td>GENERAL HIGH SCHOOL EDUCATION</td>\n",
       "      <td>NaN</td>\n",
       "      <td>REGULAR INSTRUCTION</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Function          Use          Sharing Reporting  \\\n",
       "134338     Teacher Compensation  Instruction  School Reported    School   \n",
       "206341                 NO_LABEL     NO_LABEL         NO_LABEL  NO_LABEL   \n",
       "326408     Teacher Compensation  Instruction  School Reported    School   \n",
       "364634  Substitute Compensation  Instruction  School Reported    School   \n",
       "47683   Substitute Compensation  Instruction  School Reported    School   \n",
       "\n",
       "       Student_Type Position_Type               Object_Type     Pre_K  \\\n",
       "134338     NO_LABEL       Teacher                  NO_LABEL  NO_LABEL   \n",
       "206341     NO_LABEL      NO_LABEL                  NO_LABEL  NO_LABEL   \n",
       "326408  Unspecified       Teacher  Base Salary/Compensation  Non PreK   \n",
       "364634  Unspecified    Substitute                  Benefits  NO_LABEL   \n",
       "47683   Unspecified       Teacher   Substitute Compensation  NO_LABEL   \n",
       "\n",
       "         Operating_Status            Object_Description             Text_2  \\\n",
       "134338  PreK-12 Operating                           NaN                NaN   \n",
       "206341      Non-Operating           CONTRACTOR SERVICES  BOND EXPENDITURES   \n",
       "326408  PreK-12 Operating  Personal Services - Teachers                NaN   \n",
       "364634  PreK-12 Operating             EMPLOYEE BENEFITS       TEACHER SUBS   \n",
       "47683   PreK-12 Operating  TEACHER COVERAGE FOR TEACHER       TEACHER SUBS   \n",
       "\n",
       "       SubFund_Description       Job_Title_Description       Text_3  \\\n",
       "134338                 NaN         Teacher-Elementary           NaN   \n",
       "206341       BUILDING FUND                     (blank)      Regular   \n",
       "326408                 NaN             TCHER 2ND GRADE          NaN   \n",
       "364634        GENERAL FUND    Teacher, Short Term Sub       Regular   \n",
       "47683         GENERAL FUND  Teacher, Secondary (High)   Alternative   \n",
       "\n",
       "                     Text_4 Sub_Object_Description Location_Description  FTE  \\\n",
       "134338                  NaN                    NaN                  NaN  1.0   \n",
       "206341                  NaN                    NaN                  NaN  NaN   \n",
       "326408  Regular Instruction                    NaN                  NaN  1.0   \n",
       "364634                  NaN                    NaN                  NaN  NaN   \n",
       "47683                   NaN                    NaN                  NaN  NaN   \n",
       "\n",
       "           Function_Description Facility_or_Department  \\\n",
       "134338                      NaN                    NaN   \n",
       "206341                 RGN  GOB                    NaN   \n",
       "326408                      NaN                    NaN   \n",
       "364634  UNALLOC BUDGETS/SCHOOLS                    NaN   \n",
       "47683               NON-PROJECT                    NaN   \n",
       "\n",
       "                    Position_Extra      Total             Program_Description  \\\n",
       "134338               KINDERGARTEN   50471.810                    KINDERGARTEN   \n",
       "206341                UNDESIGNATED   3477.860   BUILDING IMPROVEMENT SERVICES   \n",
       "326408                     TEACHER  62237.130           Instruction - Regular   \n",
       "364634  PROFESSIONAL-INSTRUCTIONAL     22.300  GENERAL MIDDLE/JUNIOR HIGH SCH   \n",
       "47683   PROFESSIONAL-INSTRUCTIONAL     54.166   GENERAL HIGH SCHOOL EDUCATION   \n",
       "\n",
       "              Fund_Description                         Text_1  \n",
       "134338            General Fund                            NaN  \n",
       "206341                     NaN  BUILDING IMPROVEMENT SERVICES  \n",
       "326408  General Purpose School                            NaN  \n",
       "364634                     NaN            REGULAR INSTRUCTION  \n",
       "47683                      NaN            REGULAR INSTRUCTION  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.Function.value_counts().plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.Use.value_counts().plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 FTE         Total\n",
      "count  126071.000000  3.957220e+05\n",
      "mean        0.426794  1.310586e+04\n",
      "std         0.573576  3.682254e+05\n",
      "min        -0.087551 -8.746631e+07\n",
      "25%         0.000792  7.379770e+01\n",
      "50%         0.130927  4.612300e+02\n",
      "75%         1.000000  3.652662e+03\n",
      "max        46.800000  1.297000e+08\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Print the summary statistics\n",
    "print(df.describe())\n",
    "\n",
    "# Import matplotlib.pyplot as plt\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# Create the histogram\n",
    "plt.hist(df.FTE.dropna())\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('Distribution of %full-time \\n employee works')\n",
    "plt.xlabel('% of full-time')\n",
    "plt.ylabel('num employees')\n",
    "plt.ylim(0, 20)\n",
    "\n",
    "# Display the histogram\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21003"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.FTE.dropna().value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46.8"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df.FTE.dropna()[df.FTE.dropna().argmax()]\n",
    "# argmax deprecated - use idxmax instead\n",
    "df.FTE.dropna()[df.FTE.dropna().idxmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Function</th>\n",
       "      <th>Use</th>\n",
       "      <th>Sharing</th>\n",
       "      <th>Reporting</th>\n",
       "      <th>Student_Type</th>\n",
       "      <th>Position_Type</th>\n",
       "      <th>Object_Type</th>\n",
       "      <th>Pre_K</th>\n",
       "      <th>Operating_Status</th>\n",
       "      <th>Object_Description</th>\n",
       "      <th>Text_2</th>\n",
       "      <th>SubFund_Description</th>\n",
       "      <th>Job_Title_Description</th>\n",
       "      <th>Text_3</th>\n",
       "      <th>Text_4</th>\n",
       "      <th>Sub_Object_Description</th>\n",
       "      <th>Location_Description</th>\n",
       "      <th>FTE</th>\n",
       "      <th>Function_Description</th>\n",
       "      <th>Facility_or_Department</th>\n",
       "      <th>Position_Extra</th>\n",
       "      <th>Total</th>\n",
       "      <th>Program_Description</th>\n",
       "      <th>Fund_Description</th>\n",
       "      <th>Text_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>135371</th>\n",
       "      <td>Food Services</td>\n",
       "      <td>O&amp;M</td>\n",
       "      <td>Shared Services</td>\n",
       "      <td>School</td>\n",
       "      <td>Unspecified</td>\n",
       "      <td>Other</td>\n",
       "      <td>Base Salary/Compensation</td>\n",
       "      <td>Non PreK</td>\n",
       "      <td>PreK-12 Operating</td>\n",
       "      <td>Personal Services - Food Services - Cafeteria</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>46.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>528191.986943</td>\n",
       "      <td>Food Services</td>\n",
       "      <td>Central Cafeteria</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Function  Use          Sharing Reporting Student_Type  \\\n",
       "135371  Food Services  O&M  Shared Services    School  Unspecified   \n",
       "\n",
       "       Position_Type               Object_Type     Pre_K   Operating_Status  \\\n",
       "135371         Other  Base Salary/Compensation  Non PreK  PreK-12 Operating   \n",
       "\n",
       "                                   Object_Description Text_2  \\\n",
       "135371  Personal Services - Food Services - Cafeteria    NaN   \n",
       "\n",
       "       SubFund_Description Job_Title_Description Text_3 Text_4  \\\n",
       "135371                 NaN                   NaN    NaN    NaN   \n",
       "\n",
       "       Sub_Object_Description Location_Description   FTE Function_Description  \\\n",
       "135371                    NaN                  NaN  46.8                  NaN   \n",
       "\n",
       "       Facility_or_Department Position_Extra          Total  \\\n",
       "135371                    NaN            NaN  528191.986943   \n",
       "\n",
       "       Program_Description   Fund_Description Text_1  \n",
       "135371       Food Services  Central Cafeteria    NaN  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.FTE == 46.8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Aha, it's the cafeteria ladies....\n",
    "\n",
    "So a line item can represent more than one employee and in lots of places it does.  But mostly not the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123003"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df.FTE <= 1.0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3068"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df.FTE > 1.0).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### And that's why my histogram looks different than his..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### take a quick look at the dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Function                   object\n",
       "Use                        object\n",
       "Sharing                    object\n",
       "Reporting                  object\n",
       "Student_Type               object\n",
       "Position_Type              object\n",
       "Object_Type                object\n",
       "Pre_K                      object\n",
       "Operating_Status           object\n",
       "Object_Description         object\n",
       "Text_2                     object\n",
       "SubFund_Description        object\n",
       "Job_Title_Description      object\n",
       "Text_3                     object\n",
       "Text_4                     object\n",
       "Sub_Object_Description     object\n",
       "Location_Description       object\n",
       "FTE                       float64\n",
       "Function_Description       object\n",
       "Facility_or_Department     object\n",
       "Position_Extra             object\n",
       "Total                     float64\n",
       "Program_Description        object\n",
       "Fund_Description           object\n",
       "Text_1                     object\n",
       "dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode the labels as categorical variables\n",
    "Remember, your ultimate goal is to predict the probability that a certain label is attached to a budget line item. You just saw that many columns in your data are the inefficient object type. Does this include the labels you're trying to predict? Let's find out!\n",
    "\n",
    "There are 9 columns of labels in the dataset. Each of these columns is a category that has many possible values it can take). The 9 labels have been loaded into a list called LABELS. In the Shell, check out the type for these labels using df[LABELS].dtypes.\n",
    "\n",
    "You will notice that every label is encoded as an object datatype. Because category datatypes are much more efficient your task is to convert the labels to category types using the .astype() method.\n",
    "\n",
    "Note: .astype() only works on a pandas Series. Since you are working with a pandas DataFrame, you'll need to use the .apply() method and provide a lambda function called categorize_label that applies .astype() to each column, x.\n",
    "\n",
    "##### INSTRUCTIONS\n",
    "\n",
    "Define the lambda function categorize_label to convert column x into x.astype('category').\n",
    "Use the LABELS list provided to convert the subset of data df[LABELS] to categorical types using the .apply() method and categorize_label. Don't forget axis=0.\n",
    "Print the converted .dtypes attribute of df[LABELS]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function            category\n",
      "Object_Type         category\n",
      "Operating_Status    category\n",
      "Position_Type       category\n",
      "Pre_K               category\n",
      "Reporting           category\n",
      "Sharing             category\n",
      "Student_Type        category\n",
      "Use                 category\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "### bind variable LABELS - these are actually the targets and we're going to one-hot encode them...\n",
    "LABELS = ['Function',  'Use',  'Sharing',  'Reporting',  'Student_Type',  'Position_Type', \n",
    "          'Object_Type',  'Pre_K',  'Operating_Status']\n",
    "\n",
    "### This turns out to be key.  Submission requires the dummy versions of these vars to be in this order.\n",
    "LABELS.sort()\n",
    "\n",
    "# Define the lambda function: categorize_label\n",
    "categorize_label = lambda x: x.astype('category')\n",
    "\n",
    "# Convert df[LABELS] to a categorical type\n",
    "df[LABELS] = df[LABELS].apply(categorize_label, axis=0)\n",
    "\n",
    "# Print the converted dtypes\n",
    "print(df[LABELS].dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Counting unique labels\n",
    "As Peter mentioned in the video, there are over 100 unique labels. In this exercise, you will explore this fact by counting and plotting the number of unique values for each category of label.\n",
    "\n",
    "The dataframe df and the LABELS list have been loaded into the workspace; the LABELS columns of df have been converted to category types.\n",
    "\n",
    "pandas, which has been pre-imported as pd, provides a pd.Series.nunique method for counting the number of unique values in a Series.\n",
    "\n",
    "##### INSTRUCTIONS\n",
    "\n",
    "Create the DataFrame num_unique_labels by using the .apply() method on df[LABELS] with pd.Series.nunique as the argument.\n",
    "Create a bar plot of num_unique_labels using pandas' .plot(kind='bar') method.\n",
    "The axes have been labeled for you, so hit 'Submit Answer' to see the number of unique values for each label.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Function                  category\n",
       "Use                       category\n",
       "Sharing                   category\n",
       "Reporting                 category\n",
       "Student_Type              category\n",
       "Position_Type             category\n",
       "Object_Type               category\n",
       "Pre_K                     category\n",
       "Operating_Status          category\n",
       "Object_Description          object\n",
       "Text_2                      object\n",
       "SubFund_Description         object\n",
       "Job_Title_Description       object\n",
       "Text_3                      object\n",
       "Text_4                      object\n",
       "Sub_Object_Description      object\n",
       "Location_Description        object\n",
       "FTE                        float64\n",
       "Function_Description        object\n",
       "Facility_or_Department      object\n",
       "Position_Extra              object\n",
       "Total                      float64\n",
       "Program_Description         object\n",
       "Fund_Description            object\n",
       "Text_1                      object\n",
       "dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculate number of unique values for each label: num_unique_labels\n",
    "num_unique_labels = df[LABELS].apply(pd.Series.nunique, axis=0)\n",
    "\n",
    "# Plot number of unique values for each label\n",
    "num_unique_labels.plot(kind='bar')\n",
    "\n",
    "# Label the axes\n",
    "plt.xlabel('Labels')\n",
    "plt.ylabel('Number of unique values')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Function            37\n",
       "Object_Type         11\n",
       "Operating_Status     3\n",
       "Position_Type       25\n",
       "Pre_K                3\n",
       "Reporting            3\n",
       "Sharing              5\n",
       "Student_Type         9\n",
       "Use                  8\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_unique_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(num_unique_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's save the unique labels for each output (category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Function': ['Teacher Compensation',\n",
       "  'NO_LABEL',\n",
       "  'Substitute Compensation',\n",
       "  'Facilities & Maintenance',\n",
       "  'Instructional Materials & Supplies',\n",
       "  'Food Services',\n",
       "  'Security & Safety',\n",
       "  'Utilities',\n",
       "  'Student Transportation',\n",
       "  'Parent & Community Relations',\n",
       "  'Extended Time & Tutoring',\n",
       "  'Enrichment',\n",
       "  'Special Population Program Management & Support',\n",
       "  'School Supervision',\n",
       "  'Data Processing & Information Services',\n",
       "  'Aides Compensation',\n",
       "  'Physical Health & Services',\n",
       "  'Career & Academic Counseling',\n",
       "  'Library & Media',\n",
       "  'Professional Development',\n",
       "  'School Administration',\n",
       "  'Other Non-Compensation',\n",
       "  'Social & Emotional',\n",
       "  'Finance, Budget, Purchasing & Distribution',\n",
       "  'Human Resources',\n",
       "  'Curriculum Development',\n",
       "  'Legal',\n",
       "  'Other Compensation',\n",
       "  'Student Assignment',\n",
       "  'Governance',\n",
       "  'Development & Fundraising',\n",
       "  'Research & Accountability',\n",
       "  'Recruitment',\n",
       "  'Insurance',\n",
       "  'Untracked Budget Set-Aside',\n",
       "  'Communications',\n",
       "  'Facilities Planning'],\n",
       " 'Object_Type': ['NO_LABEL',\n",
       "  'Base Salary/Compensation',\n",
       "  'Benefits',\n",
       "  'Substitute Compensation',\n",
       "  'Supplies/Materials',\n",
       "  'Rent/Utilities',\n",
       "  'Other Compensation/Stipend',\n",
       "  'Contracted Services',\n",
       "  'Equipment & Equipment Lease',\n",
       "  'Other Non-Compensation',\n",
       "  'Travel & Conferences'],\n",
       " 'Operating_Status': ['PreK-12 Operating',\n",
       "  'Non-Operating',\n",
       "  'Operating, Not PreK-12'],\n",
       " 'Position_Type': ['Teacher',\n",
       "  'NO_LABEL',\n",
       "  'Substitute',\n",
       "  'Custodian',\n",
       "  'Non-Position',\n",
       "  'Coordinator/Manager',\n",
       "  'Other',\n",
       "  'Area Officers',\n",
       "  'TA',\n",
       "  'Sec/Clerk/Other Admin',\n",
       "  'Guidance Counselor',\n",
       "  'Vice Principal',\n",
       "  'Club Advisor/Coach',\n",
       "  'Physical Therapist',\n",
       "  'Psychologist',\n",
       "  'Instructional Coach',\n",
       "  'Speech Therapist',\n",
       "  'Social Worker',\n",
       "  'School Monitor/Security',\n",
       "  'Principal',\n",
       "  'Librarian',\n",
       "  'Nurse Aide',\n",
       "  'Nurse',\n",
       "  '(Exec) Director',\n",
       "  'Occupational Therapist'],\n",
       " 'Pre_K': ['NO_LABEL', 'Non PreK', 'PreK'],\n",
       " 'Reporting': ['School', 'NO_LABEL', 'Non-School'],\n",
       " 'Sharing': ['School Reported',\n",
       "  'NO_LABEL',\n",
       "  'School on Central Budgets',\n",
       "  'Leadership & Management',\n",
       "  'Shared Services'],\n",
       " 'Student_Type': ['NO_LABEL',\n",
       "  'Unspecified',\n",
       "  'Special Education',\n",
       "  'Poverty',\n",
       "  'ELL',\n",
       "  'At Risk',\n",
       "  'PreK',\n",
       "  'Gifted',\n",
       "  'Alternative'],\n",
       " 'Use': ['Instruction',\n",
       "  'NO_LABEL',\n",
       "  'O&M',\n",
       "  'Pupil Services & Enrichment',\n",
       "  'ISPD',\n",
       "  'Leadership',\n",
       "  'Business Services',\n",
       "  'Untracked Budget Set-Aside']}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{col : df[col].unique().tolist() for col in df[LABELS].columns}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing log loss with NumPy\n",
    "To see how the log loss metric handles the trade-off between accuracy and confidence, we will use some sample data generated with NumPy and compute the log loss using the provided function compute_log_loss(), which Peter showed you in the video.\n",
    "\n",
    "5 one-dimensional numeric arrays simulating different types of predictions have been pre-loaded: actual_labels, correct_confident, correct_not_confident, wrong_not_confident, and wrong_confident.\n",
    "\n",
    "Your job is to compute the log loss for each sample set provided using the compute_log_loss(predicted_values, actual_values). It takes the predicted values as the first argument and the actual values as the second argument.\n",
    "\n",
    "##### INSTRUCTIONS\n",
    "\n",
    "Using the compute_log_loss() function, compute the log loss for the following predicted values (in each case, the actual values are contained in actual_labels):\n",
    "correct_confident.\n",
    "correct_not_confident.\n",
    "wrong_not_confident.\n",
    "wrong_confident.\n",
    "actual_labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_confident = np.array([ 0.95,  0.95,  0.95,  0.95,  0.95,  0.05,  0.05,  0.05,  0.05,  0.05])\n",
    "correct_not_confident = np.array([ 0.65,  0.65,  0.65,  0.65,  0.65,  0.35,  0.35,  0.35,  0.35,  0.35])\n",
    "wrong_not_confident = np.array([ 0.35,  0.35,  0.35,  0.35,  0.35,  0.65,  0.65,  0.65,  0.65,  0.65])\n",
    "wrong_confident = np.array([ 0.05,  0.05,  0.05,  0.05,  0.05,  0.95,  0.95,  0.95,  0.95,  0.95])\n",
    "actual_labels = np.array([ 1.,  1.,  1.,  1.,  1.,  0.,  0.,  0.,  0.,  0.])                                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05129329438755058\n",
      "Log loss, correct and confident: 0.05129329438755058\n",
      "0.4307829160924542\n",
      "Log loss, correct and not confident: 0.4307829160924542\n",
      "1.049822124498678\n",
      "Log loss, wrong and not confident: 1.049822124498678\n",
      "2.9957322735539904\n",
      "Log loss, wrong and confident: 2.9957322735539904\n",
      "9.992007221626413e-16\n",
      "Log loss, actual labels: 9.99200722162646e-15\n"
     ]
    }
   ],
   "source": [
    "# Compute and print log loss for 1st case\n",
    "from log_loss import compute_log_loss\n",
    "from sklearn.metrics import log_loss\n",
    "# sklearn.metrics.log_loss\n",
    "print(log_loss(actual_labels, correct_confident))\n",
    "correct_confident = compute_log_loss(correct_confident, actual_labels)\n",
    "print(\"Log loss, correct and confident: {}\".format(correct_confident)) \n",
    "\n",
    "print(log_loss(actual_labels, correct_not_confident))\n",
    "# Compute log loss for 2nd case\n",
    "correct_not_confident = compute_log_loss(correct_not_confident, actual_labels)\n",
    "print(\"Log loss, correct and not confident: {}\".format(correct_not_confident)) \n",
    "\n",
    "print(log_loss(actual_labels, wrong_not_confident))\n",
    "# Compute and print log loss for 3rd case\n",
    "wrong_not_confident = compute_log_loss(wrong_not_confident, actual_labels)\n",
    "print(\"Log loss, wrong and not confident: {}\".format(wrong_not_confident)) \n",
    "\n",
    "# Compute and print log loss for 4th case\n",
    "print(log_loss(actual_labels, wrong_confident ))\n",
    "wrong_confident = compute_log_loss(wrong_confident ,actual_labels)\n",
    "print(\"Log loss, wrong and confident: {}\".format(wrong_confident)) \n",
    "\n",
    "print(log_loss(actual_labels, actual_labels))\n",
    "# Compute and print log loss for actual labels\n",
    "actual_labels = compute_log_loss(actual_labels, actual_labels)\n",
    "print(\"Log loss, actual labels: {}\".format(actual_labels)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### End of chapter 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up a train-test split in scikit-learn\n",
    "Alright, you've been patient and awesome. It's finally time to start training models!\n",
    "\n",
    "The first step is to split the data into a training set and a test set. Some labels don't occur very often, but we want to make sure that they appear in both the training and the test sets. We provide a function that will make sure at least min_count examples of each label appear in each split: multilabel_train_test_split.\n",
    "\n",
    "Feel free to check out the full code for multilabel_train_test_split here.\n",
    "\n",
    "You'll start with a simple model that uses just the numeric columns of your DataFrame when calling multilabel_train_test_split. The data has been read into a DataFrame df and a list consisting of just the numeric columns is available as NUMERIC_COLUMNS.\n",
    "\n",
    "##### INSTRUCTIONS\n",
    "\n",
    "Create a new DataFrame named numeric_data_only by applying the .fillna(-1000) method to the numeric columns (available in the list NUMERIC_COLUMNS) of df.\n",
    "Convert the labels (available in the list LABELS) to dummy variables. Save the result as label_dummies.\n",
    "In the call to multilabel_train_test_split(), set the size of your test set to be 0.2. Use a seed of 123.\n",
    "Fill in the .info() method calls for X_train, X_test, y_train, and y_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multilabel import multilabel_train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMERIC_COLUMNS = ['FTE', 'Total']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Function_Aides Compensation', 'Function_Career & Academic Counseling', 'Function_Communications', 'Function_Curriculum Development', 'Function_Data Processing & Information Services', 'Function_Development & Fundraising', 'Function_Enrichment', 'Function_Extended Time & Tutoring', 'Function_Facilities & Maintenance', 'Function_Facilities Planning', 'Function_Finance, Budget, Purchasing & Distribution', 'Function_Food Services', 'Function_Governance', 'Function_Human Resources', 'Function_Instructional Materials & Supplies', 'Function_Insurance', 'Function_Legal', 'Function_Library & Media', 'Function_NO_LABEL', 'Function_Other Compensation', 'Function_Other Non-Compensation', 'Function_Parent & Community Relations', 'Function_Physical Health & Services', 'Function_Professional Development', 'Function_Recruitment', 'Function_Research & Accountability', 'Function_School Administration', 'Function_School Supervision', 'Function_Security & Safety', 'Function_Social & Emotional', 'Function_Special Population Program Management & Support', 'Function_Student Assignment', 'Function_Student Transportation', 'Function_Substitute Compensation', 'Function_Teacher Compensation', 'Function_Untracked Budget Set-Aside', 'Function_Utilities', 'Object_Type_Base Salary/Compensation', 'Object_Type_Benefits', 'Object_Type_Contracted Services', 'Object_Type_Equipment & Equipment Lease', 'Object_Type_NO_LABEL', 'Object_Type_Other Compensation/Stipend', 'Object_Type_Other Non-Compensation', 'Object_Type_Rent/Utilities', 'Object_Type_Substitute Compensation', 'Object_Type_Supplies/Materials', 'Object_Type_Travel & Conferences', 'Operating_Status_Non-Operating', 'Operating_Status_Operating, Not PreK-12', 'Operating_Status_PreK-12 Operating', 'Position_Type_(Exec) Director', 'Position_Type_Area Officers', 'Position_Type_Club Advisor/Coach', 'Position_Type_Coordinator/Manager', 'Position_Type_Custodian', 'Position_Type_Guidance Counselor', 'Position_Type_Instructional Coach', 'Position_Type_Librarian', 'Position_Type_NO_LABEL', 'Position_Type_Non-Position', 'Position_Type_Nurse', 'Position_Type_Nurse Aide', 'Position_Type_Occupational Therapist', 'Position_Type_Other', 'Position_Type_Physical Therapist', 'Position_Type_Principal', 'Position_Type_Psychologist', 'Position_Type_School Monitor/Security', 'Position_Type_Sec/Clerk/Other Admin', 'Position_Type_Social Worker', 'Position_Type_Speech Therapist', 'Position_Type_Substitute', 'Position_Type_TA', 'Position_Type_Teacher', 'Position_Type_Vice Principal', 'Pre_K_NO_LABEL', 'Pre_K_Non PreK', 'Pre_K_PreK', 'Reporting_NO_LABEL', 'Reporting_Non-School', 'Reporting_School', 'Sharing_Leadership & Management', 'Sharing_NO_LABEL', 'Sharing_School Reported', 'Sharing_School on Central Budgets', 'Sharing_Shared Services', 'Student_Type_Alternative', 'Student_Type_At Risk', 'Student_Type_ELL', 'Student_Type_Gifted', 'Student_Type_NO_LABEL', 'Student_Type_Poverty', 'Student_Type_PreK', 'Student_Type_Special Education', 'Student_Type_Unspecified', 'Use_Business Services', 'Use_ISPD', 'Use_Instruction', 'Use_Leadership', 'Use_NO_LABEL', 'Use_O&M', 'Use_Pupil Services & Enrichment', 'Use_Untracked Budget Set-Aside']\n"
     ]
    }
   ],
   "source": [
    "print(label_dummies.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 320222 entries, 134338 to 415831\n",
      "Data columns (total 2 columns):\n",
      "FTE      320222 non-null float64\n",
      "Total    320222 non-null float64\n",
      "dtypes: float64(2)\n",
      "memory usage: 7.3 MB\n",
      "None\n",
      "\n",
      "X_test info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 80055 entries, 206341 to 413949\n",
      "Data columns (total 2 columns):\n",
      "FTE      80055 non-null float64\n",
      "Total    80055 non-null float64\n",
      "dtypes: float64(2)\n",
      "memory usage: 1.8 MB\n",
      "None\n",
      "\n",
      "y_train info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 320222 entries, 134338 to 415831\n",
      "Columns: 104 entries, Function_Aides Compensation to Use_Untracked Budget Set-Aside\n",
      "dtypes: uint8(104)\n",
      "memory usage: 34.2 MB\n",
      "None\n",
      "\n",
      "y_test info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 80055 entries, 206341 to 413949\n",
      "Columns: 104 entries, Function_Aides Compensation to Use_Untracked Budget Set-Aside\n",
      "dtypes: uint8(104)\n",
      "memory usage: 8.6 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Create the new DataFrame: numeric_data_only\n",
    "numeric_data_only = df[NUMERIC_COLUMNS].fillna(-1000)\n",
    "\n",
    "# Get labels and convert to dummy variables: label_dummies\n",
    "label_dummies = pd.get_dummies(df[LABELS])\n",
    "\n",
    "# Create training and test sets\n",
    "X_train, X_test, y_train, y_test = multilabel_train_test_split(numeric_data_only,\n",
    "                                                               label_dummies,\n",
    "                                                               size=0.2, \n",
    "                                                               seed=123)\n",
    "\n",
    "# Print the info\n",
    "print(\"X_train info:\")\n",
    "print(X_train.info())\n",
    "print(\"\\nX_test info:\")  \n",
    "print(X_test.info())\n",
    "print(\"\\ny_train info:\")  \n",
    "print(y_train.info())\n",
    "print(\"\\ny_test info:\")  \n",
    "print(y_test.info()) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400277, 2)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[NUMERIC_COLUMNS].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training a model\n",
    "With split data in hand, you're only a few lines away from training a model.\n",
    "\n",
    "In this exercise, you will import the logistic regression and one versus rest classifiers in order to fit a multi-class logistic regression model to the NUMERIC_COLUMNS of your feature data.\n",
    "\n",
    "Then you'll test and print the accuracy with the .score() method to see the results of training.\n",
    "\n",
    "Before you train! Remember, we're ultimately going to be using logloss to score our model, so don't worry too much about the accuracy here. Keep in mind that you're throwing away all of the text data in the dataset - that's by far most of the data! So don't get your hopes up for a killer performance just yet. We're just interested in getting things up and running at the moment.\n",
    "\n",
    "All data necessary to call multilabel_train_test_split() has been loaded into the workspace.\n",
    "\n",
    "##### INSTRUCTIONS\n",
    "\n",
    "Import LogisticRegression from sklearn.linear_model and OneVsRestClassifier from sklearn.multiclass.\n",
    "Instantiate the classifier clf by placing LogisticRegression() inside OneVsRestClassifier().\n",
    "Fit the classifier to the training data X_train and y_train.\n",
    "Compute and print the accuracy of the classifier using its .score() method, which accepts two arguments: X_test and y_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict time: 135.97455936049312 seconds\n",
      "Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Import classifiers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# Create the DataFrame: numeric_data_only\n",
    "numeric_data_only = df[NUMERIC_COLUMNS].fillna(-1000)\n",
    "\n",
    "# Get labels and convert to dummy variables: label_dummies\n",
    "label_dummies = pd.get_dummies(df[LABELS])\n",
    "\n",
    "# Create training and test sets\n",
    "X_train, X_test, y_train, y_test = multilabel_train_test_split(numeric_data_only,\n",
    "                                                               label_dummies,\n",
    "                                                               size=0.2, \n",
    "                                                               seed=123)\n",
    "\n",
    "# Instantiate the classifier: clf\n",
    "clf = OneVsRestClassifier(LogisticRegression(), n_jobs=-1)\n",
    "\n",
    "start = timer()\n",
    "# Fit the classifier to the training data\n",
    "clf.fit(X_train, y_train)\n",
    "end = timer()\n",
    "print('predict time: {} seconds'.format(end - start))\n",
    "\n",
    "# Print the accuracy\n",
    "print(\"Accuracy: {}\".format(clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use your model to predict values on holdout data\n",
    "You're ready to make some predictions! Remember, the train-test-split you've carried out so far is for model development. The original competition provides an additional test set, for which you'll never actually see the correct labels. This is called the \"holdout data.\"\n",
    "\n",
    "The point of the holdout data is to provide a fair test for machine learning competitions. If the labels aren't known by anyone but DataCamp, DrivenData, or whoever is hosting the competition, you can be sure that no one submits a mere copy of labels to artificially pump up the performance on their model.\n",
    "\n",
    "Remember that the original goal is to predict the probability of each label. In this exercise you'll do just that by using the .predict_proba() method on your trained model.\n",
    "\n",
    "First, however, you'll need to load the holdout data, which is available in the workspace as the file HoldoutData.csv.\n",
    "\n",
    "##### INSTRUCTIONS\n",
    "\n",
    "Read HoldoutData.csv into a DataFrame called holdout. Specify the keyword argument index_col=0 in your call to read_csv().\n",
    "Generate predictions using .predict_proba() on the numeric columns (available in the NUMERIC_COLUMNS list) of holdout. Make sure to fill in missing values with -1000!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit time: 130.5371198564142 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saus\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2785: DtypeWarning: Columns (5,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict time: 0.21749370137189317 seconds\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the classifier: clf\n",
    "clf = OneVsRestClassifier(LogisticRegression(), n_jobs=-1)\n",
    "\n",
    "start = timer()\n",
    "# Fit it to the training data\n",
    "clf.fit(X_train, y_train)\n",
    "end = timer()\n",
    "print('fit time: {} seconds'.format(end - start))\n",
    "\n",
    "# Load the holdout data: holdout\n",
    "### Over here the file is TestData.csv\n",
    "holdout = pd.read_csv('data/TestData.csv', index_col=0)\n",
    "\n",
    "start = timer()\n",
    "# Generate predictions: predictions\n",
    "predictions = clf.predict_proba(holdout[NUMERIC_COLUMNS].fillna(-1000))\n",
    "end = timer()\n",
    "print('predict time: {} seconds'.format(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### It takes a while to fit.  It's reasonable, since we're fitting 104 classifiers.  I've put in n_jobs=-1 in the One_Vs_Rest to get all the procs engaged.  It takes half the time with n_jobs=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50064, 104)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.03582155, 0.00656506, 0.00081719, 0.02396992, 0.0090299 ],\n",
       "       [0.03582447, 0.00656514, 0.00081721, 0.0239705 , 0.00902993],\n",
       "       [0.11990636, 0.01798188, 0.00133432, 0.02284447, 0.01621812],\n",
       "       [0.11963236, 0.01797478, 0.00133369, 0.02282747, 0.01621628],\n",
       "       [0.12028213, 0.01799161, 0.00133517, 0.02286775, 0.01622063]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[:5, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df = pd.DataFrame(columns=pd.get_dummies(df[LABELS], prefix_sep='__').columns, \n",
    "                             index=holdout.index,\n",
    "                             data=predictions)\n",
    "\n",
    "prediction_df.to_csv('predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",Function__Aides Compensation,Function__Career & Academic Counseling,Function__Communications,Function__Curriculum Development,Function__Data Processing & Information Services,Function__Development & Fundraising,Function__Enrichment,Function__Extended Time & Tutoring,Function__Facilities & Maintenance,Function__Facilities Planning,\"Function__Finance, Budget, Purchasing & Distribution\",Function__Food Services,Function__Governance,Function__Human Resources,Function__Instructional Materials & Supplies,Function__Insurance,Function__Legal,Function__Library & Media,Function__NO_LABEL,Function__Other Compensation,Function__Other Non-Compensation,Function__Parent & Community Relations,Function__Physical Health & Services,Function__Professional Development,Function__Recruitment,Function__Research & Accountability,Function__School Administration,Function__School Supervision,Function__Security & Safety,Function__Social & Emotional,Function__Special Population Program Management & Support,Function__Student Assignment,Function__Student Transportation,Function__Substitute Compensation,Function__Teacher Compensation,Function__Untracked Budget Set-Aside,Function__Utilities,Object_Type__Base Salary/Compensation,Object_Type__Benefits,Object_Type__Contracted Services,Object_Type__Equipment & Equipment Lease,Object_Type__NO_LABEL,Object_Type__Other Compensation/Stipend,Object_Type__Other Non-Compensation,Object_Type__Rent/Utilities,Object_Type__Substitute Compensation,Object_Type__Supplies/Materials,Object_Type__Travel & Conferences,Operating_Status__Non-Operating,\"Operating_Status__Operating, Not PreK-12\",Operating_Status__PreK-12 Operating,Position_Type__(Exec) Director,Position_Type__Area Officers,Position_Type__Club Advisor/Coach,Position_Type__Coordinator/Manager,Position_Type__Custodian,Position_Type__Guidance Counselor,Position_Type__Instructional Coach,Position_Type__Librarian,Position_Type__NO_LABEL,Position_Type__Non-Position,Position_Type__Nurse,Position_Type__Nurse Aide,Position_Type__Occupational Therapist,Position_Type__Other,Position_Type__Physical Therapist,Position_Type__Principal,Position_Type__Psychologist,Position_Type__School Monitor/Security,Position_Type__Sec/Clerk/Other Admin,Position_Type__Social Worker,Position_Type__Speech Therapist,Position_Type__Substitute,Position_Type__TA,Position_Type__Teacher,Position_Type__Vice Principal,Pre_K__NO_LABEL,Pre_K__Non PreK,Pre_K__PreK,Reporting__NO_LABEL,Reporting__Non-School,Reporting__School,Sharing__Leadership & Management,Sharing__NO_LABEL,Sharing__School Reported,Sharing__School on Central Budgets,Sharing__Shared Services,Student_Type__Alternative,Student_Type__At Risk,Student_Type__ELL,Student_Type__Gifted,Student_Type__NO_LABEL,Student_Type__Poverty,Student_Type__PreK,Student_Type__Special Education,Student_Type__Unspecified,Use__Business Services,Use__ISPD,Use__Instruction,Use__Leadership,Use__NO_LABEL,Use__O&M,Use__Pupil Services & Enrichment,Use__Untracked Budget Set-Aside\n",
      "180042,0.03582154840118523,0.006565057426977918,0.0008171937858314573,0.023969916091332484,0.00902989527766411,0.0001816042540260363,0.03217390689420817,0.024341733602958302,0.05192097490158243,6.478246128416034e-05,0.005921704813390721,0.03493862346619874,0.0014840400699692904,0.0033601908202371385,0.07196450622764251,0.00014601013524518145,0.00032288502873182093,0.01282477814292767,0.19985057700242403,0.0018034515259788256,0.006605364696279035,0.006292269905346289,0.015125232234790167,0.056627839449756544,0.001721009564074692,0.00398079317124112,0.03051031768215061,0.008071951094939167,0.006957703147349234,0.011145059108746995,0.010906868028010436,0.0009175499525061988,0.0311947290653084,0.09148771202102683,0.18525828606625022,0.0011202165343792805,0.006158918422615533,0.05815040629363042,0.2982520380289165,0.02695275340882467,0.016201303872750955,0.1918528138627406,0.1829896342588963,0.022502904278835653,0.010939500343831841,0.03728998945057638,0.1157949319426628,0.017177543015929372,0.16946994599288828,0.02024481374619685,0.8103213996860593,0.0007148082572710591,0.001106123842390993,0.012147525084143117,0.018378365676125063,0.021010313167021653,0.005981175026531115,0.014032694450009387,0.004783963628961503,0.337582381564974,0.06546611385423753,0.004975672205696496,2.5638042186836227e-06,0.0012151060821127251,0.08415216295455824,0.0007487004223097336,0.004960245663760772,0.00439647099967911,0.005139625991320132,0.019485317050759815,0.004259449067221657,0.005309269402421923,0.09074853354172406,0.04183633162873008,0.23907890928859718,0.004820630533143751,0.8310932040782517,0.14129723393663549,0.027584053930618822,0.18967860031633546,0.22672194106107954,0.5836124413344104,0.04569870530523044,0.19867827956402856,0.575919478130102,0.06782161635664521,0.11194406485680529,0.005980920963092192,0.008408303297020399,0.018514911580498606,0.004555938687342839,0.3023561569982255,0.046983806381953784,0.014892661166910902,0.08635141092321855,0.5176333482498872,0.016229355701697548,0.07376448523715413,0.4384729922997541,0.03867412586594982,0.25467528204236767,0.11412148322140814,0.06285765134586928,0.0011202165343792805\n",
      "28872,0.03582446807127531,0.006565142810346817,0.0008172062673472612,0.023970495772407785,0.00902992877726138,0.00018161381340612407,0.03217462099130063,0.02434218995642622,0.05192216312533699,6.478611356172143e-05,0.005921635904576616,0.0349398270138152,0.0014840284824065898,0.0033602365941138744,0.07196611924247186,0.00014601997258852073,0.00032290124554817647,0.012825083518889646,0.19984401172968508,0.0018034966871795632,0.0066053931915808495,0.006292425330675867,0.015125588527574674,0.056633219628046,0.0017210491928614827,0.003980756310881988,0.030510703190015812,0.008072098948515145,0.00695776036536184,0.011145235202551519,0.010906889042473016,0.0009175719506526234,0.031195379971179098,0.09248509542979785,0.18525595429174824,0.0011202593423575247,0.006158800599624935,0.05814945137494516,0.300780129303422,0.026952287754710022,0.01620144153178719,0.191820525880026,0.18424948682467399,0.022503382266101928,0.010939305750431795,0.03736573656389937,0.11581587341548685,0.017177971085404538,0.16946530664768394,0.020244445741065095,0.8103310081987607,0.0007147934695668476,0.0011063101294726493,0.012147812779574238,0.01837850620081871,0.021011092920293423,0.005981315808884531,0.014032915714782627,0.004783952764361595,0.3375745547451374,0.06546415923450194,0.004975776530005025,2.564138325291561e-06,0.0012151513268289752,0.08415680125291874,0.0007487191612132566,0.004960170579455835,0.004396496248738088,0.005139732608943606,0.019486029084596547,0.004259506587197518,0.005309321577249331,0.09176071359058192,0.041840275147915315,0.2390759853607383,0.00482056349432546,0.8310874903842627,0.14129847672785892,0.027584528137515153,0.1896689918042342,0.22672209799464035,0.583633277419319,0.04569947543965257,0.19867052944030436,0.5759375207561543,0.06782271345452703,0.11194586080358992,0.005990799775742667,0.008408474501685305,0.01851515873260036,0.004556049892809336,0.30232525433594526,0.046985772610033345,0.014893089933120692,0.08635319064786291,0.5176535397719695,0.016229227940702817,0.07376882335785527,0.4384769297269527,0.03867500687088978,0.2546625366430462,0.1141224947219584,0.0628628013406678,0.0011202593423575247\n",
      "186915,0.1199063613248147,0.017981884620624514,0.0013343162865332911,0.022844472600410542,0.016218120090047472,0.017472956982080484,0.02160286282761967,0.016364118792033346,0.08397061930852531,0.01196523463167049,0.010025940014585167,0.09372794155597394,0.003007438577268826,0.0070955501448505195,0.03485784084064621,0.03336300449772212,0.018185983928645216,0.02524164454969188,0.11426070457189344,0.004484808863660338,0.0009296323469820685,0.01204935617569998,0.034790110509721514,0.11326313199592156,0.002624427631145618,0.004527943736055554,0.06801501824830956,0.009130587725448346,0.012991889773341371,0.026250104325686106,0.019567503660039946,0.0023614886204920073,0.05164959270161499,0.24278705328661596,0.3457687298611978,0.016186235711971513,0.008863495492729537,0.6442729172341742,0.2406261331755581,0.019291213687917978,0.004067999366210143,0.14120861307651575,0.24536508259027942,0.02520268129329024,0.012341789744384388,0.14407286974174324,0.1453856658652924,0.015660571353459878,0.03928150867390482,0.04296652975117409,0.9537133241349104,0.006308821987308128,0.087240063627141,0.012344619955619566,0.026998685879851776,0.0652479183469555,0.023798159799364196,0.018337820287184455,0.0061691661947228275,0.09819463409384067,0.040583625028047396,0.007786558602692461,0.03869646842366223,0.013740389712191544,0.1408469001590618,0.0023274764677534677,0.010820878938500922,0.006750282108422782,0.00816700278879712,0.06593955085409348,0.008582631477641206,0.007159178085115634,0.23817766433967769,0.12695502223720925,0.3030725769426381,0.011881822106846962,0.5068138046714118,0.391247607596535,0.04630131855077092,0.04628668343204702,0.21371082503041502,0.7713029146022282,0.09440020682331934,0.07667879624943619,0.7497574927645296,0.10596548175538799,0.13413502320063897,0.07214453503078562,0.013064556580953773,0.01390381866353949,0.00860553892862107,0.1538772531674548,0.08334596777623517,0.025957573377237964,0.15658129474372826,0.6407035412457823,0.020359713861477422,0.12174674998084188,0.49973056860148757,0.09262011701797537,0.11120016702352031,0.13983787388875682,0.13919041056925766,0.016186235711971513\n",
      "412396,0.1196323554610486,0.01797477795604676,0.0013336901428632722,0.02282747278905222,0.016216283663671206,0.0174451889083231,0.021587964609670548,0.016354611564185665,0.08391354880898577,0.011944758347244719,0.010029513058089213,0.09363475951389741,0.0030081595794634575,0.007092589958845888,0.03483286641254902,0.03329625389104436,0.018158423071216706,0.025223403484370112,0.11438852455508275,0.004481366409081463,0.0009295083431980004,0.012040261432365328,0.034765424506526865,0.11295251136145962,0.002622571906362229,0.004529232337744581,0.06798962126853975,0.009125452421293784,0.012988625102483914,0.02623754963677942,0.01956635464924799,0.0023597510104952555,0.051617165778495636,0.1817378236136442,0.34587618884826843,0.01616751528705309,0.008868696415139547,0.6443957718534413,0.1794994906605326,0.01930154478576341,0.004066923637093991,0.14198693509693522,0.20070423166901175,0.025186272224499318,0.012348532173327079,0.13626336857292154,0.1446060936185478,0.015648558001881058,0.039319773816390825,0.04298999363372398,0.9536283973991945,0.006312813755192818,0.08682818493082273,0.012335635787037355,0.026992395024241632,0.06517686499577778,0.02378125160200177,0.01832897086141974,0.006169596390192623,0.09828997238752815,0.040621891067734006,0.007781554680458489,0.03854770189806132,0.013724864957645968,0.1406231356183843,0.0023256888870079916,0.010825886756371616,0.006749093062714282,0.00816181122682802,0.06586901026253657,0.00857908410343274,0.007157019263817883,0.1768280961499067,0.12662015676871843,0.30317696886480905,0.011886867674319311,0.5071266089689686,0.39117260083553296,0.0462773216936401,0.04637161017566234,0.21370620021149245,0.7708375472756046,0.0943537991161889,0.07678483481276757,0.7493310869666114,0.10591494524994578,0.13407052375519835,0.06880107268950766,0.013056418486517105,0.013898086427521262,0.008599109199766666,0.1544646660456738,0.08324287567991114,0.025934863312935683,0.15648971853754098,0.6401309630456408,0.0203646217788286,0.12153816625759566,0.49960764329996554,0.09255890300109676,0.11140438272728398,0.1398008755066951,0.1388686561759987,0.01616751528705309\n",
      "427740,0.12028212724288884,0.017991612247444413,0.0013351734360320133,0.022867751342210203,0.01622063297336767,0.017511018898010298,0.021623262480973245,0.01637713514491607,0.08404875883365501,0.01199330595152444,0.010021053508349934,0.0938555655495887,0.003006452399475441,0.007099602183508847,0.03489203818164811,0.033454540323133705,0.018223758517992843,0.02526662233102917,0.1140860260670379,0.004489523007737302,0.0009298020325878537,0.012061810367188447,0.03482391264800941,0.11368932216846825,0.0026269687085544286,0.0045261812951450505,0.06804977999893895,0.00913761835708914,0.01299635772503458,0.026267290802673097,0.019569075814570795,0.00236386804249031,0.05169398960047834,0.3463496702923306,0.34562173270979424,0.016211883065096216,0.008856384619324476,0.6441047998755108,0.34471514994961555,0.01927708772371836,0.004069471610575658,0.1401495114954053,0.31649332093191956,0.0252251486371465,0.012332570813446254,0.15535659244111208,0.1464579034554558,0.015677022420582113,0.039229213135071855,0.04293444687763852,0.9538292790151829,0.006303364614702357,0.08780644814822604,0.012356922335679414,0.027007295178176068,0.06534524847012635,0.023821312167599685,0.01834993463968652,0.006168577660494348,0.09806432816489434,0.040531326327432275,0.007793410014451964,0.03890089991427583,0.01376165834322438,0.1411535313096945,0.0023299244112641514,0.010814031080791745,0.00675190926765341,0.0081741110582,0.06603617644774329,0.00858748726081314,0.007162132776984792,0.3431622788857235,0.12741440248119956,0.30292978347947624,0.011874922317212003,0.5063858251773984,0.39135023826645887,0.04633416962230881,0.04617072854097019,0.21371715270834818,0.7719385406373064,0.09446373379350248,0.07653393450916231,0.7503401069682787,0.10603465895275943,0.13422331218473918,0.07696188167031316,0.013075699024576752,0.013911665139690946,0.008614343642030108,0.15307653211232833,0.0834872037956094,0.025988676071072195,0.15670665767810754,0.6414862791302799,0.020353000849876367,0.12203262840993886,0.49989875192870714,0.09270392737238388,0.11092129491544726,0.13988850741960016,0.1396316389578744,0.016211883065096216\n",
      "69847,0.035839453834986414,0.00656558096870748,0.0008172703186938991,0.02397347059550716,0.009030100680910272,0.00018166287486022823,0.03217828559021045,0.024344531845436033,0.051928260847698623,6.480485830285545e-05,0.005921282314563871,0.03494600359416511,0.0014839690227589093,0.0033604714905278704,0.07197439688187147,0.00014607046290981068,0.00032298447423876925,0.012826650655130901,0.19981032486660003,0.0018037284476564851,0.0066055394158911936,0.0062932229489419044,0.015127416958355141,0.056660835336075285,0.0017212525605708304,0.003980567168916409,0.030512681480907647,0.008072857694903596,0.00695805398443031,0.011146138863450726,0.010906996877894383,0.0009176848414640925,0.031198720272845565,0.09775811312005771,0.1852439892500638,0.0011204790353244727,0.006158196031363845,0.0581445514831227,0.3139421574471902,0.026949898394131297,0.016202147941181435,0.1916549067106907,0.19082001403162183,0.02250583519280909,0.010938307256799687,0.037756761692713976,0.11592338544500062,0.017180167870000634,0.16944150164190658,0.02024255744195972,0.8103803079946305,0.0007147175919214718,0.0011072665465704453,0.01214928918191394,0.018379227313697306,0.021015094635455988,0.0059820382791633975,0.014034051179350832,0.004783897013530796,0.33753439316305095,0.06545413004295059,0.004976311899850371,2.5658534637435473e-06,0.0012153835245747586,0.08418060612963711,0.0007488153264463889,0.004959785305434403,0.004396625815446055,0.00514027974720917,0.01948968325615565,0.0042598017605941065,0.005309589318107001,0.09711579183135961,0.041860516736353666,0.23906098173920137,0.004820219502169785,0.8310581683919416,0.1413048542000884,0.02758696163398252,0.18961969201144288,0.22672290329352046,0.5837401921710494,0.045703427545005726,0.19863076359778664,0.5760301025134885,0.06782834342298627,0.11195507702175034,0.006041748183717378,0.008409353085277854,0.01851642703367539,0.0045566205812828985,0.30216670684350017,0.04699586345375053,0.014895290319356986,0.0863623237492001,0.5177571507720712,0.016228572356332613,0.0737910879277531,0.4384971345835083,0.03867952801206469,0.254597140792628,0.11412768531546263,0.06288923450584821,0.0011204790353244727\n",
      "358824,0.03560030570185802,0.006558571019639293,0.0008162456749741743,0.023925896939750273,0.009027349473036876,0.000180879155612636,0.032119676163040754,0.02430707247692565,0.051830738141848146,6.450546851960248e-05,0.005926944618913667,0.03484726366289296,0.0014849210568228593,0.003356713556358664,0.07184200523876044,0.00014526437564788263,0.0003216548356820213,0.012801588777433693,0.20035004962370434,0.0018000223124746517,0.006603199244285999,0.006280467829796069,0.015098176073046568,0.05622031776277921,0.0017180002091373252,0.003983595768809401,0.030481030635733878,0.008060721217281948,0.006953355611419193,0.011131682998830241,0.010905270923341929,0.0009158795059859803,0.031145294866493933,0.03904514344985628,0.18543558090564533,0.0011169676552759686,0.006167880216352422,0.058223028760745116,0.14535796616970098,0.026988168740628357,0.016190844348168004,0.19431902450421723,0.10574031456612888,0.022466603469207543,0.010954300600544429,0.031945973105336575,0.11421286323076868,0.017145037890356856,0.1698228592113326,0.020272803465223043,0.8095900053830835,0.000715933104233631,0.0010920562485290195,0.012125678234541585,0.01836768806393642,0.02095113016168475,0.0059704843788882895,0.014015887096122259,0.004784789473849874,0.33817752574416093,0.06561483528341981,0.00496774931491674,2.5385371514543816e-06,0.0012116721389519045,0.08380030391752498,0.000747277526846512,0.004965955820110378,0.004394552346200737,0.005131528865036342,0.019431272885986364,0.004255079475858889,0.005305305303012797,0.03797040402067977,0.04153763925271144,0.23930121664048945,0.004825728590764434,0.8315270276321862,0.1412028012704919,0.02754803460882656,0.19040999457356575,0.22671001341891822,0.5820279080786841,0.045640206741779245,0.1992679996604795,0.5745475442995588,0.06773827865343948,0.11180763608782232,0.00527559589334828,0.008395300852699404,0.018496136047469927,0.004547494324886757,0.30471016420870944,0.04683459028702958,0.014860107985251303,0.08621623862132172,0.5160985109827736,0.0162390691785251,0.07343544909960704,0.43817374755480737,0.038607220687041095,0.25564523507073283,0.11404462612763426,0.062467371700840255,0.0011169676552759686\n",
      "254148,0.03579333252162875,0.00656423198803715,0.000817073122982795,0.02396431237590494,0.009029571406213886,0.00018151185882480007,0.032167003714884845,0.02433732195748036,0.05190948834872125,6.47471612775485e-05,0.005922371072512975,0.03492698949551625,0.001484152104462521,0.003359748305395314,0.07194891314216302,0.00014591506089092242,0.00032272828474430045,0.012821826104534147,0.1999140591279838,0.0018030149607196676,0.006605089206855688,0.006290767432833326,0.015121787982995722,0.05657584806363598,0.0017206264755667408,0.0039811495587899854,0.030506590792878156,0.008070521768778633,0.006957149981388467,0.011143356756283595,0.010906664859933732,0.0009173372991713258,0.031188436713376747,0.08233371427921242,0.1852808310886979,0.0011198027463794709,0.00616005766381031,0.05815963931158704,0.2744554976349193,0.02695725580757234,0.0161999730293317,0.19216519138529323,0.1711530290896975,0.022498283556730298,0.01094138187082936,0.036565231468518235,0.11559263712903436,0.017173404933501165,0.1695148049074057,0.020248371998268896,0.8102284842989345,0.0007149512421926984,0.0011043244134285898,0.012144743960015464,0.018377007122261636,0.02100277588655445,0.005979814097055856,0.01403055541513003,0.0047840686703779275,0.3376580567109147,0.06548501413242816,0.0049746636990057845,2.560576283360199e-06,0.0012146687379682444,0.08410733134524498,0.0007485192766902845,0.004960971645425712,0.004396226896437182,0.005138595313838003,0.019478434344074116,0.004258892997635588,0.005308764996735962,0.08147027844562878,0.04179822357698724,0.23910717941030482,0.0048212787196800656,0.8311484367605411,0.14128521896497948,0.027579469654510882,0.18977151569765632,0.22672042381406646,0.5834109803711769,0.045691260197928164,0.19875322034054446,0.5757450296014458,0.06781101034191427,0.11192670274375649,0.005886242657721639,0.0084066482496729,0.01851252225364432,0.004554863679957378,0.3026550196136027,0.04696480070570965,0.01488851643481946,0.08633420606182378,0.5174381314277803,0.01623059096009766,0.07372255591474115,0.4384349252190215,0.03866560920630206,0.2547985276224692,0.11411170432531033,0.0628078809231296,0.0011198027463794709\n",
      "296,0.0358348593615131,0.0065654466497475006,0.0008172506834503971,0.02397255863589162,0.009030047984119187,0.0001816478338393361,0.0321771621776845,0.024343813924418718,0.05192639154085788,6.479911159341638e-05,0.005921390704082091,0.03494411007379007,0.0014839872496476018,0.0033603994821360976,0.07197185930788612,0.00014605498342801752,0.0003229589585033216,0.012826170235054321,0.1998206510006783,0.0018036573991546743,0.006605494591012049,0.0062929784312483455,0.015126856436333094,0.05665236849487958,0.0017211902163799637,0.003980625148755529,0.030512075029732688,0.008072625096403767,0.006957963975251234,0.011145861841746658,0.010906963821221029,0.0009176502337162999,0.03119769627857156,0.09611381361201078,0.1852476570271737,0.0011204116846604054,0.006158381353324878,0.058146053486193366,0.30987415000492413,0.02695063082214762,0.01620193139069791,0.19170566507423534,0.18878701279773755,0.02250508322916995,0.010938613331857841,0.037636478314137654,0.1158904188045996,0.017179494424233024,0.16944879869804444,0.02024313627493689,0.8103651963625849,0.000714740851085719,0.0011069732725908098,0.012148836577240633,0.018379006256182225,0.02101386784477668,0.005981816799274123,0.014033703097137893,0.004783914103679105,0.3375467043027694,0.0654572043037343,0.004976147778054652,2.565327572305895e-06,0.0012153123404795654,0.08417330818030573,0.000748785846027658,0.004959903406637547,0.004396586096832824,0.005140112017779709,0.019488563011601602,0.004259711274084335,0.0053095072416808956,0.0954452254153977,0.04185431075493484,0.23906558097688205,0.00482032494913367,0.8310671573744304,0.14130289918331224,0.027586215632206843,0.189634803642545,0.22672265643182493,0.5837074186746444,0.045702216006711927,0.198642953037706,0.5760017224750736,0.06782661753030979,0.11195225175268322,0.006026084526842957,0.008409083748957522,0.01851603823147432,0.0045564456312174074,0.30221530399729707,0.04699276992287251,0.014894615764826473,0.08635952393681255,0.5177253893538385,0.016228773320599344,0.07378426214653247,0.4384909408355868,0.03867814201855569,0.25461718648047776,0.11412609413579874,0.06288113041205383,0.0011204116846604054\n"
     ]
    }
   ],
   "source": [
    "!head predictions.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Submitted this prediction file (after fixing the ordering problem) and it scored 1.33, good enough for  28th on the leader board.  We're shooting for better than 0.37 (log_loss).\n",
    "\n",
    "##### This using only the numeric data as predictors.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing out your results to a csv for submission\n",
    "At last, you're ready to submit some predictions for scoring. In this exercise, you'll write your predictions to a .csv using the .to_csv() method on a pandas DataFrame. Then you'll evaluate your performance according to the LogLoss metric discussed earlier!\n",
    "\n",
    "You'll need to make sure your submission obeys the correct format.\n",
    "\n",
    "To do this, you'll use your predictions values to create a new DataFrame, prediction_df.\n",
    "\n",
    "Interpreting LogLoss & Beating the Benchmark:\n",
    "\n",
    "When interpreting your log loss score, keep in mind that the score will change based on the number of samples tested. To get a sense of how this very basic model performs, compare your score to the DrivenData benchmark model performance: 2.0455, which merely submitted uniform probabilities for each class.\n",
    "\n",
    "Remember, the lower the log loss the better. Is your model's log loss lower than 2.0455?\n",
    "\n",
    "##### INSTRUCTIONS\n",
    "\n",
    "Create the prediction_df DataFrame by specifying the following arguments to the provided parameters pd.DataFrame():\n",
    "pd.get_dummies(df[LABELS]).columns.\n",
    "holdout.index.\n",
    "predictions.\n",
    "Save prediction_df to a csv file called 'predictions.csv' using the .to_csv() method.\n",
    "Submit the predictions for scoring by using the score_submission() function with pred_path set to 'predictions.csv'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Already did this so just commented out..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate predictions: predictions\n",
    "# predictions = clf.predict_proba(holdout[NUMERIC_COLUMNS].fillna(-1000))\n",
    "\n",
    "# # Format predictions in DataFrame: prediction_df\n",
    "# prediction_df = pd.DataFrame(columns=pd.get_dummies(df[LABELS]).columns,\n",
    "#                              index=holdout.index,\n",
    "#                              data=predictions)\n",
    "\n",
    "\n",
    "# # Save prediction_df to csv\n",
    "# prediction_df.to_csv('predictions.csv')\n",
    "\n",
    "# # Submit the predictions for scoring: score\n",
    "# score = score_submission(pred_path='predictions.csv')\n",
    "\n",
    "# # Print score\n",
    "# print('Your model, trained with numeric data only, yields logloss score: {}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a bag-of-words in scikit-learn\n",
    "In this exercise, you'll study the effects of tokenizing in different ways by comparing the bag-of-words representations resulting from different token patterns.\n",
    "\n",
    "You will focus on one feature only, the Position_Extra column, which describes any additional information not captured by the Position_Type label.\n",
    "\n",
    "For example, in the Shell you can check out the budget item in row 8960 of the data using df.loc[8960]. Looking at the output reveals that this Object_Description is overtime pay. For who? The Position Type is merely \"other\", but the Position Extra elaborates: \"BUS DRIVER\". Explore the column further to see more instances. It has a lot of NaN values.\n",
    "\n",
    "Your task is to turn the raw text in this column into a bag-of-words representation by creating tokes that contain only alphanumeric characters.\n",
    "\n",
    "For comparison purposes, the first 15 tokens of vec_basic, which splits df.Position_Extra into tokens when it encounters only whitespace characters, have been printed along with the length of the representation.\n",
    "\n",
    "##### INSTRUCTIONS\n",
    "Import CountVectorizer from sklearn.feature_extraction.text.\n",
    "Fill missing values in df.Position_Extra using .fillna('') to replace NaNs with empty strings. Specify the additional keyword argument inplace=True so that you don't have to assign the result back to df.\n",
    "Instantiate the CountVectorizer as vec_alphanumeric by specifying the token_pattern to be TOKENS_ALPHANUMERIC.\n",
    "Fit vec_alphanumeric to df.Position_Extra.\n",
    "Hit 'Submit Answer' to see the len of the fitted representation as well as the first 15 elements, and compare to vec_basic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 385 tokens in Position_Extra if we split on non-alpha numeric\n",
      "['1st', '2nd', '3rd', '4th', '56', '5th', '9th', 'a', 'ab', 'accountability', 'adaptive', 'addit', 'additional', 'adm', 'admin']\n"
     ]
    }
   ],
   "source": [
    "# Import CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create the token pattern: TOKENS_ALPHANUMERIC\n",
    "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'\n",
    "\n",
    "# Fill missing values in df.Position_Extra\n",
    "df.Position_Extra.fillna('', inplace=True)\n",
    "\n",
    "# Instantiate the CountVectorizer: vec_alphanumeric\n",
    "vec_alphanumeric = CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC)\n",
    "\n",
    "# Fit to the data\n",
    "vec_alphanumeric.fit(df.Position_Extra)\n",
    "\n",
    "# Print the number of tokens and first 15 tokens\n",
    "msg = \"There are {} tokens in Position_Extra if we split on non-alpha numeric\"\n",
    "print(msg.format(len(vec_alphanumeric.get_feature_names())))\n",
    "print(vec_alphanumeric.get_feature_names()[:15])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is a major model design decision.  It may be worth rethinking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combining text columns for tokenization\n",
    "In order to get a bag-of-words representation for all of the text data in our DataFrame, you must first convert the text data in each row of the DataFrame into a single string.\n",
    "\n",
    "In the previous exercise, this wasn't necessary because you only looked at one column of data, so each row was already just a single string. CountVectorizer expects each row to just be a single string, so in order to use all of the text columns, you'll need a method to turn a list of strings into a single string.\n",
    "\n",
    "In this exercise, you'll complete the function definition combine_text_columns(). When completed, this function will convert all training text data in your DataFrame to a single string per row that can be passed to the vectorizer object and made into a bag-of-words using the .fit_transform() method.\n",
    "\n",
    "Note that the function uses NUMERIC_COLUMNS and LABELS to determine which columns to drop. These lists have been loaded into the workspace.\n",
    "\n",
    "##### INSTRUCTIONS\n",
    "\n",
    "Use the .drop() method on data_frame with to_drop and axis= as arguments to drop the non-text data. Save the result as text_data.\n",
    "Fill in missing values (inplace) in text_data with blanks (\"\"), using the .fillna() method.\n",
    "Complete the .apply() method by writing a lambda function that uses the .join() method to join all the items in a row with a space in between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define combine_text_columns()\n",
    "def combine_text_columns(data_frame, to_drop=NUMERIC_COLUMNS + LABELS):\n",
    "    \"\"\" converts all text in each row of data_frame to single vector \"\"\"\n",
    "    \n",
    "    # Drop non-text columns that are in the df\n",
    "    to_drop = set(to_drop) & set(data_frame.columns.tolist())\n",
    "    text_data = data_frame.drop(to_drop, axis=1)\n",
    "    \n",
    "    # Replace nans with blanks\n",
    "    text_data.fillna('', inplace=True)\n",
    "    \n",
    "    # Join all text items in a row that have a space in between\n",
    "    return text_data.apply(lambda x: \" \".join(x), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What's in a token?\n",
    "Now you will use combine_text_columns to convert all training text data in your DataFrame to a single vector that can be passed to the vectorizer object and made into a bag-of-words using the .fit_transform() method.\n",
    "\n",
    "You'll compare the effect of tokenizing using any non-whitespace characters as a token and using only alphanumeric characters as a token.\n",
    "\n",
    "##### INSTRUCTIONS\n",
    "\n",
    "Import CountVectorizer from sklearn.feature_extraction.text.\n",
    "Instantiate vec_basic and vec_alphanumeric using, respectively, the TOKENS_BASIC and TOKENS_ALPHANUMERIC patterns.\n",
    "Create the text vector by using the combine_text_columns() function on df.\n",
    "Using the .fit_transform() method with text_vector, fit and transform first vec_basic and then vec_alphanumeric. Print the number of tokens they contain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4757 tokens in the dataset\n",
      "There are 3284 alpha-numeric tokens in the dataset\n"
     ]
    }
   ],
   "source": [
    "# Import the CountVectorizer\n",
    "from  sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create the basic token pattern; separates based on whitespace\n",
    "TOKENS_BASIC = '\\\\S+(?=\\\\s+)'\n",
    "\n",
    "# Create the alphanumeric token pattern\n",
    "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'\n",
    "\n",
    "# Instantiate basic CountVectorizer: vec_basic\n",
    "vec_basic = CountVectorizer(token_pattern=TOKENS_BASIC)\n",
    "\n",
    "# Instantiate alphanumeric CountVectorizer: vec_alphanumeric\n",
    "vec_alphanumeric = CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC)\n",
    "\n",
    "# Create the text vector\n",
    "text_vector = combine_text_columns(df)\n",
    "\n",
    "# Fit and transform vec_basic\n",
    "vec_basic.fit_transform(text_vector)\n",
    "\n",
    "# Print number of tokens of vec_basic\n",
    "print(\"There are {} tokens in the dataset\".format(len(vec_basic.get_feature_names())))\n",
    "\n",
    "\n",
    "# Fit and transform vec_alphanumeric\n",
    "vec_alphanumeric.fit_transform(text_vector)\n",
    "\n",
    "# Print number of tokens of vec_alphanumeric\n",
    "print(\"There are {} alpha-numeric tokens in the dataset\".format(len(vec_alphanumeric.get_feature_names())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"(year',\n",
       " '\"building',\n",
       " '\"guidance,',\n",
       " '\"human',\n",
       " '\"maintenance,',\n",
       " '\"multilingual',\n",
       " '\"performing',\n",
       " '\"software,',\n",
       " '\"technology',\n",
       " '\"title',\n",
       " '%',\n",
       " '&',\n",
       " '&materials',\n",
       " '&program',\n",
       " '&wildlife',\n",
       " '(']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_basic.get_feature_names()[:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00a',\n",
       " '12',\n",
       " '1st',\n",
       " '2nd',\n",
       " '3rd',\n",
       " '4th',\n",
       " '5',\n",
       " '56',\n",
       " '5th',\n",
       " '6',\n",
       " '60',\n",
       " '60hrs',\n",
       " '6th',\n",
       " '70',\n",
       " '70h',\n",
       " '70hr']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_alphanumeric.get_feature_names()[:16]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate pipeline\n",
    "In order to make your life easier as you start to work with all of the data in your original DataFrame, df, it's time to turn to one of scikit-learn's most useful objects: the Pipeline.\n",
    "\n",
    "For the next few exercises, you'll reacquaint yourself with pipelines and train a classifier on some synthetic (sample) data of multiple datatypes before using the same techniques on the main dataset.\n",
    "\n",
    "The sample data is stored in the DataFrame, sample_df, which has three kinds of feature data: numeric, text, and numeric with missing values. It also has a label column with two classes, a and b.\n",
    "\n",
    "In this exercise, your job is to instantiate a pipeline that trains using the numeric column of the sample data.\n",
    "\n",
    "##### INSTRUCTIONS\n",
    "\n",
    "Import Pipeline from sklearn.pipeline.\n",
    "Create training and test sets using the numeric data only. Do this by specifying sample_df[['numeric']] in train_test_split().\n",
    "Instantiate a pipeline as pl by adding the classifier step. Use a name of 'clf' and the same classifier from Chapter 2: OneVsRestClassifier(LogisticRegression()).\n",
    "Fit your pipeline to the training data and compute its accuracy to see it in action! Since this is toy data, you'll use the default scoring method for now. In the next chapter, you'll return to log loss scoring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### I don't have the sample_df  so the code is commented out."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Here's what the sample data looks like\n",
    "\n",
    "     numeric     text  with_missing label\n",
    "0 -10.856306               4.433240     b\n",
    "1   9.973454      foo      4.310229     b\n",
    "2   2.829785  foo bar      2.469828     a\n",
    "3 -15.062947               2.852981     b\n",
    "4  -5.786003  foo bar      1.826475     a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import Pipeline\n",
    "# from sklearn.pipeline import Pipeline\n",
    "\n",
    "# # Import other necessary modules\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "### Notice here that he does one-hot on the label to get the multiple target columns\n",
    "\n",
    "# # Split and select numeric data only, no nans \n",
    "# X_train, X_test, y_train, y_test = train_test_split(sample_df[['numeric']],\n",
    "#                                                     pd.get_dummies(sample_df['label']), \n",
    "#                                                     random_state=22)\n",
    "\n",
    "# # Instantiate Pipeline object: pl\n",
    "# pl = Pipeline([\n",
    "#         ('clf', OneVsRestClassifier(LogisticRegression()))\n",
    "#     ])\n",
    "\n",
    "# # Fit the pipeline to the training data\n",
    "# pl.fit(X_train, y_train)\n",
    "\n",
    "# # Compute and print accuracy\n",
    "# accuracy = pl.score(X_test, y_test)\n",
    "# print(\"\\nAccuracy on sample data - numeric, no nans: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing numeric features\n",
    "What would have happened if you had included the with 'with_missing' column in the last exercise? Without imputing missing values, the pipeline would not be happy (try it and see). So, in this exercise you'll improve your pipeline a bit by using the Imputer() imputation transformer from scikit-learn to fill in missing values in your sample data.\n",
    "\n",
    "By default, the imputer transformer replaces NaNs with the mean value of the column. That's a good enough imputation strategy for the sample data, so you won't need to pass anything extra to the imputer.\n",
    "\n",
    "After importing the transformer, you will edit the steps list used in the previous exercise by inserting a (name, transform) tuple. Recall that steps are processed sequentially, so make sure the new tuple encoding your preprocessing step is put in the right place.\n",
    "\n",
    "The sample_df is in the workspace, in case you'd like to take another look. Make sure to select both numeric columns- in the previous exercise we couldn't use with_missing because we had no preprocessing step!\n",
    "\n",
    "##### INSTRUCTIONS\n",
    "\n",
    "Import Imputer from sklearn.preprocessing.\n",
    "Create training and test sets by selecting the correct subset of sample_df: 'numeric' and 'with_missing'.\n",
    "Add the tuple ('imp', Imputer()) to the correct position in the pipeline. Pipeline processes steps sequentially, so the imputation step should come before the classifier step.\n",
    "Complete the .fit() and .score() methods to fit the pipeline to the data and compute the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### No data, so sorry..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import the Imputer object\n",
    "# from sklearn.preprocessing import Imputer\n",
    "\n",
    "# # Create training and test sets using only numeric data\n",
    "# X_train, X_test, y_train, y_test = train_test_split(sample_df[['numeric', 'with_missing']],\n",
    "#                                                     pd.get_dummies(sample_df['label']), \n",
    "#                                                     random_state=456)\n",
    "\n",
    "# # Insantiate Pipeline object: pl\n",
    "# pl = Pipeline([\n",
    "#         ('imp', Imputer()),\n",
    "#         ('clf', OneVsRestClassifier(LogisticRegression()))\n",
    "#     ])\n",
    "\n",
    "# # Fit the pipeline to the training data\n",
    "# pl.fit(X_train, y_train)\n",
    "\n",
    "# # Compute and print accuracy\n",
    "# accuracy = pl.score(X_test, y_test)\n",
    "# print(\"\\nAccuracy on sample data - all numeric, incl nans: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing text features\n",
    "Here, you'll perform a similar preprocessing pipeline step, only this time you'll use the text column from the sample data.\n",
    "\n",
    "To preprocess the text, you'll turn to CountVectorizer() to generate a bag-of-words representation of the data, as in Chapter 2. Using the default arguments, add a (step, transform) tuple to the steps list in your pipeline.\n",
    "\n",
    "Make sure you select only the text column for splitting your training and test sets.\n",
    "\n",
    "As usual, your sample_df is ready and waiting in the workspace.\n",
    "\n",
    "##### INSTRUCTIONS\n",
    "\n",
    "Import CountVectorizer from sklearn.feature_extraction.text.\n",
    "Create training and test sets by selecting the correct subset of sample_df: 'text'.\n",
    "Add the 'CountVectorizer' step (with the name 'vec') to the correct position in the pipeline.\n",
    "Fit the pipeline to the training data and compute its accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### still no data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import the CountVectorizer\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# # Split out only the text data\n",
    "# X_train, X_test, y_train, y_test = train_test_split(sample_df['text'],\n",
    "#                                                     pd.get_dummies(sample_df['label']), \n",
    "#                                                     random_state=456)\n",
    "\n",
    "# # Instantiate Pipeline object: pl\n",
    "# pl = Pipeline([\n",
    "#         ('vec', CountVectorizer()),\n",
    "#         ('clf', OneVsRestClassifier(LogisticRegression()))\n",
    "#     ])\n",
    "\n",
    "# # Fit to the training data\n",
    "# pl.fit(X_train, y_train)\n",
    "\n",
    "# # Compute and print accuracy\n",
    "# accuracy = pl.score(X_test, y_test)\n",
    "# print(\"\\nAccuracy on sample data - just text data: \", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple types of processing: FunctionTransformer\n",
    "The next two exercises will introduce new topics you'll need to make your pipeline truly excel.\n",
    "\n",
    "Any step in the pipeline must be an object that implements the fit and transform methods. The FunctionTransformer creates an object with these methods out of any Python function that you pass to it. We'll use it to help select subsets of data in a way that plays nicely with pipelines.\n",
    "\n",
    "You are working with numeric data that needs imputation, and text data that needs to be converted into a bag-of-words. You'll create functions that separate the text from the numeric variables and see how the .fit() and .transform() methods work.\n",
    "\n",
    "##### INSTRUCTIONS\n",
    "\n",
    "Compute the selector get_text_data by using a lambda function and FunctionTransformer() to obtain all 'text' columns.\n",
    "Compute the selector get_numeric_data by using a lambda function and FunctionTransformer() to obtain all the numeric columns (including missing data). These are 'numeric' and 'with_missing'.\n",
    "Fit and transform get_text_data using the .fit_transform() method with sample_df as the argument.\n",
    "Fit and transform get_numeric_data using the same approach as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import FunctionTransformer\n",
    "# from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# # Obtain the text data: get_text_data\n",
    "# get_text_data = FunctionTransformer(lambda x: x['text'], validate=False)\n",
    "\n",
    "# # Obtain the numeric data: get_numeric_data\n",
    "# get_numeric_data = FunctionTransformer(lambda x: x[['numeric', 'with_missing']], validate=False)\n",
    "\n",
    "# # Fit and transform the text data: just_text_data\n",
    "# just_text_data = get_text_data.fit_transform(sample_df)\n",
    "\n",
    "# # Fit and transform the numeric data: just_numeric_data\n",
    "# just_numeric_data = get_numeric_data.fit_transform(sample_df)\n",
    "\n",
    "# # Print head to check results\n",
    "# print('Text Data')\n",
    "# print(just_text_data.head())\n",
    "# print('\\nNumeric Data')\n",
    "# print(just_numeric_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple types of processing: FeatureUnion\n",
    "Now that you can separate text and numeric data in your pipeline, you're ready to perform separate steps on each by nesting pipelines and using FeatureUnion().\n",
    "\n",
    "These tools will allow you to streamline all preprocessing steps for your model, even when multiple datatypes are involved. Here, for example, you don't want to impute our text data, and you don't want to create a bag-of-words with our numeric data. Instead, you want to deal with these separately and then join the results together using FeatureUnion().\n",
    "\n",
    "In the end, you'll still have only two high-level steps in your pipeline: preprocessing and model instantiation. The difference is that the first preprocessing step actually consists of a pipeline for numeric data and a pipeline for text data. The results of those pipelines are joined using FeatureUnion().\n",
    "\n",
    "##### INSTRUCTIONS\n",
    "\n",
    "In the process_and_join_features:\n",
    "Add the steps ('selector', get_numeric_data) and ('imputer', Imputer()) to the 'numeric_features' preprocessing step.\n",
    "Add the equivalent steps for the text_features preprocessing step. That is, use get_text_data and a CountVectorizer step with the name 'vectorizer.\n",
    "Add the transform step process_and_join_features to 'union' in the main pipeline, pl.\n",
    "Hit 'Submit Answer' to see the pipeline in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import FeatureUnion\n",
    "# from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "# # Split using ALL data in sample_df\n",
    "# X_train, X_test, y_train, y_test = train_test_split(sample_df[['numeric', 'with_missing', 'text']],\n",
    "#                                                     pd.get_dummies(sample_df['label']), \n",
    "#                                                     random_state=22)\n",
    "\n",
    "# # Create a FeatureUnion with nested pipeline: process_and_join_features\n",
    "# process_and_join_features = FeatureUnion(\n",
    "#             transformer_list = [\n",
    "#                 ('numeric_features', Pipeline([\n",
    "#                     ('selector', get_numeric_data),\n",
    "#                     ('imputer', Imputer())\n",
    "#                 ])),\n",
    "#                 ('text_features', Pipeline([\n",
    "#                     ('selector', get_text_data),\n",
    "#                     ('vectorizer', CountVectorizer())\n",
    "#                 ]))\n",
    "#              ]\n",
    "#         )\n",
    "\n",
    "# # Instantiate nested pipeline: pl\n",
    "# pl = Pipeline([\n",
    "#         ('union', process_and_join_features),\n",
    "#         ('clf', OneVsRestClassifier(LogisticRegression()))\n",
    "#     ])\n",
    "\n",
    "\n",
    "# # Fit pl to the training data\n",
    "# pl.fit(X_train, y_train)\n",
    "\n",
    "# # Compute and print accuracy\n",
    "# accuracy = pl.score(X_test, y_test)\n",
    "# print(\"\\nAccuracy on sample data - all data: \", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using FunctionTransformer on the main dataset\n",
    "In this exercise you're going to use FunctionTransformer on the primary budget data, before instantiating a multiple-datatype pipeline in the next exercise.\n",
    "\n",
    "Recall from Chapter 2 that you used a custom function combine_text_columns to select and properly format text data for tokenization; it is loaded into the workspace and ready to be put to work in a function transformer!\n",
    "\n",
    "Concerning the numeric data, you can use NUMERIC_COLUMNS, preloaded as usual, to help design a subset-selecting lambda function.\n",
    "\n",
    "You're all finished with sample data. The original df is back in the workspace, ready to use.\n",
    "\n",
    "##### INSTRUCTIONS\n",
    "\n",
    "Complete the call to multilabel_train_test_split() by selecting df[NON_LABELS].\n",
    "Compute get_text_data by using FunctionTransformer() and passing in combine_text_columns. Be sure to also specify validate=False.\n",
    "Use FunctionTransformer() to compute get_numeric_data. In the lambda function, select out the NUMERIC_COLUMNS of x. Like you did when computing get_text_data, also specify validate=False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import FunctionTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# Get the dummy encoding of the labels\n",
    "dummy_labels = pd.get_dummies(df[LABELS])\n",
    "\n",
    "# Get the columns that are features in the original df\n",
    "NON_LABELS = [c for c in df.columns if c not in LABELS]\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = multilabel_train_test_split(df[NON_LABELS],\n",
    "                                                               dummy_labels,\n",
    "                                                               0.2, \n",
    "                                                               seed=123)\n",
    "\n",
    "# Preprocess the text data: get_text_data\n",
    "get_text_data = FunctionTransformer(combine_text_columns, validate=False)\n",
    "\n",
    "# Preprocess the numeric data: get_numeric_data\n",
    "get_numeric_data = FunctionTransformer(lambda x: x[NUMERIC_COLUMNS], validate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add a model to the pipeline\n",
    "You're about to take everything you've learned so far and implement it in a Pipeline that works with the real, DrivenData budget line item data you've been exploring.\n",
    "\n",
    "Surprise! The structure of the pipeline is exactly the same as earlier in this chapter:\n",
    "\n",
    "the preprocessing step uses FeatureUnion to join the results of nested pipelines that each rely on FunctionTransformer to select multiple datatypes\n",
    "the model step stores the model object\n",
    "You can then call familiar methods like .fit() and .score() on the Pipeline object pl.\n",
    "\n",
    "##### INSTRUCTIONS\n",
    "\n",
    "Complete the 'numeric_features' transform with the following steps:\n",
    "get_numeric_data, with the name 'selector'.\n",
    "Imputer(), with the name 'imputer'.\n",
    "Complete the 'text_features' transform with the following steps:\n",
    "get_text_data, with the name 'selector'.\n",
    "CountVectorizer(), with the name 'vectorizer'.\n",
    "Fit the pipeline to the training data.\n",
    "Hit 'Submit Answer' to compute the accuracy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Before this will work, I have to go back to the commented code and find the pieces I need.  \n",
    "\n",
    "##### Is it simpler to just create a dataset to run all the cells?  It might be because if I do it correctly, any problems can be worked out...\n",
    "\n",
    "##### Eh, just forge ahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "# for the selectors\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "# for gluing preprocessed text and numbers together\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "# for nans in the numeric data\n",
    "from sklearn.preprocessing import Imputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The selectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the text data: get_text_data\n",
    "get_text_data = FunctionTransformer(combine_text_columns, validate=False)\n",
    "\n",
    "# Preprocess the numeric data: get_numeric_data\n",
    "get_numeric_data = FunctionTransformer(lambda x: x[NUMERIC_COLUMNS], validate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit time: 552.9759218740909 seconds\n",
      "\n",
      "Accuracy on budget dataset:  0.34734869777028293\n"
     ]
    }
   ],
   "source": [
    "# Complete the pipeline: pl\n",
    "pl = Pipeline([\n",
    "        ('union', FeatureUnion(\n",
    "            transformer_list = [\n",
    "                ('numeric_features', Pipeline([('selector', get_numeric_data),\n",
    "                                               ('imputer', Imputer())])),\n",
    "                ('text_features', Pipeline([('selector', get_text_data),\n",
    "                                            ('vectorizer', CountVectorizer())]))\n",
    "             ])),\n",
    "        ('clf', OneVsRestClassifier(LogisticRegression(), n_jobs=-1))\n",
    "    ])\n",
    "\n",
    "start = timer()\n",
    "# Fit to the training data\n",
    "pl.fit(X_train, y_train)\n",
    "end = timer()\n",
    "print('fit time: {} seconds'.format(end - start))\n",
    "\n",
    "# Compute and print accuracy\n",
    "accuracy = pl.score(X_test, y_test)\n",
    "print(\"\\nAccuracy on budget dataset: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### predict and write submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict time: 2.811005188764966 seconds\n"
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "# Generate predictions: predictions\n",
    "predictions = pl.predict_proba(holdout)\n",
    "end = timer()\n",
    "print('predict time: {} seconds'.format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict time: 2.4578736818721154 seconds\n"
     ]
    }
   ],
   "source": [
    "# start = timer()\n",
    "# # Generate predictions: predictions\n",
    "# predictions = pl.predict_proba(holdout)\n",
    "# end = timer()\n",
    "# print('predict time: {} seconds'.format(end - start))\n",
    "\n",
    "# prediction_df = pd.DataFrame(columns=pd.get_dummies(df[LABELS], prefix_sep='__').columns, \n",
    "#                              index=holdout.index,\n",
    "#                              data=predictions)\n",
    "\n",
    "# prediction_df.to_csv('sub2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50064, 104)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.50438762e-02, 9.03329930e-03, 2.02679431e-05, 5.15634199e-03,\n",
       "        4.11514734e-03],\n",
       "       [4.33409116e-02, 4.67055310e-03, 1.58356138e-04, 7.05264179e-03,\n",
       "        4.72590971e-03],\n",
       "       [6.80352893e-02, 2.36865149e-02, 1.32713221e-05, 3.77470952e-03,\n",
       "        2.45368811e-03],\n",
       "       [6.78844838e-02, 2.35319987e-02, 1.31912911e-05, 3.76189937e-03,\n",
       "        2.44281742e-03],\n",
       "       [5.20167050e-02, 2.11305834e-01, 2.46700519e-04, 2.94294671e-03,\n",
       "        6.81839512e-03]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[:5, :5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submitted and moved up to 16th with log-loss of 0.75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now get train and test set losses to compare with scored loss of predictions on holdout set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try a different class of model\n",
    "Now you're cruising. One of the great strengths of pipelines is how easy they make the process of testing different models.\n",
    "\n",
    "Until now, you've been using the model step ('clf', OneVsRestClassifier(LogisticRegression())) in your pipeline.\n",
    "\n",
    "But what if you want to try a different model? Do you need to build an entirely new pipeline? New nests? New FeatureUnions? Nope! You just have a simple one-line change, as you'll see in this exercise.\n",
    "\n",
    "In particular, you'll swap out the logistic-regression model and replace it with a random forest classifier, which uses the statistics of an ensemble of decision trees to generate predictions.\n",
    "\n",
    "##### INSTRUCTIONS\n",
    "Import the RandomForestClassifier from sklearn.ensemble.\n",
    "Add a RandomForestClassifier() step named 'clf' to the pipeline.\n",
    "Hit 'Submit Answer' to fit the pipeline to the training data and compute its accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ~~They're cheating here because it's definitely not apples to apples.  Before we used a one-vs-rest scheme to predict 104 columns.  Here the RFC isn't predicting 104 columns, only one.  And when it scores, it's only checking one column~~\n",
    "\n",
    "##### Revision: there *ARE* 104_ columns of predictions, they're just coming out in a different form (list of 104 2-d arrays with a probability for both 1 and 0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Further, it's not clear to me that this is the way to do this problem given sklearn docs say every classifier can do  multiclass and some can do multi-class-multi-label.\n",
    "\n",
    "http://scikit-learn.org/stable/modules/multiclass.html\n",
    "\n",
    "I quote: 'The set of labels can be different for each output variable. For instance, a sample could be assigned pear for an output variable that takes possible values in a finite set of species such as pear, apple; and blue or green for a second output variable that takes possible values in a finite set of colors such as green, red, blue, yellow'\n",
    "\n",
    "Seems like what I need is a compact multi-multi dataset for experiments.  This is not so easy to find."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit time: 170.5594423003348 seconds\n",
      "\n",
      "Accuracy on budget dataset:  0.9055524327025171\n"
     ]
    }
   ],
   "source": [
    "# Import random forest classifer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Edit model step in pipeline\n",
    "pl = Pipeline([\n",
    "        ('union', FeatureUnion(\n",
    "            transformer_list = [\n",
    "                ('numeric_features', Pipeline([\n",
    "                    ('selector', get_numeric_data),\n",
    "                    ('imputer', Imputer())\n",
    "                ])),\n",
    "                ('text_features', Pipeline([\n",
    "                    ('selector', get_text_data),\n",
    "                    ('vectorizer', CountVectorizer())\n",
    "                ]))\n",
    "             ]\n",
    "        )),\n",
    "        ('clf', RandomForestClassifier(n_jobs=-1))\n",
    "    ])\n",
    "start = timer()\n",
    "# Fit to the training data\n",
    "pl.fit(X_train, y_train)\n",
    "end = timer()\n",
    "print('fit time: {} seconds'.format(end - start))\n",
    "\n",
    "# Compute and print accuracy\n",
    "accuracy = pl.score(X_test, y_test)\n",
    "print(\"\\nAccuracy on budget dataset: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict time: 6.232642497095412 seconds\n"
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "# Generate predictions: predictions\n",
    "rf_predictions = pl.predict_proba(holdout)\n",
    "end = timer()\n",
    "print('predict time: {} seconds'.format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(rf_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       ...,\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50064, 2)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_predictions[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rf_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We have 104 elements in predictions, each of which is an array (50064, 2).  That looks a lot like it predicted all the columns, like they suggested .\n",
    "\n",
    "##### questions:\n",
    "* Why does the code below complain?\n",
    "    * wrong shape\n",
    "* What does one-vs-all with logreg output for probalities?\n",
    "    * array num_samples, num_target_columns\n",
    "* How are they different??\n",
    "\n",
    "RFC probas are list with num_target_cols elements, each of which is array (num_samples, num_labels).  RFC (along with most other classifiers) is happy to predict any number of labels in a target column (but they're mutually exclusive).\n",
    "    \n",
    "* Can I make a submissions file with this output from RFC?\n",
    "   * yes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50064, 16)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "holdout.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab the probabilities for 1 from the probas; transpose goes from rows to columns\n",
    "pred_rf = pd.DataFrame(data=[x[:, 1] for x in rf_predictions]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50064, 104)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the shape\n",
    "pred_rf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_rf.columns = pd.get_dummies(df[LABELS], prefix_sep='__').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_rf.index = holdout.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Function__Aides Compensation</th>\n",
       "      <th>Function__Career &amp; Academic Counseling</th>\n",
       "      <th>Function__Communications</th>\n",
       "      <th>Function__Curriculum Development</th>\n",
       "      <th>Function__Data Processing &amp; Information Services</th>\n",
       "      <th>Function__Development &amp; Fundraising</th>\n",
       "      <th>Function__Enrichment</th>\n",
       "      <th>Function__Extended Time &amp; Tutoring</th>\n",
       "      <th>Function__Facilities &amp; Maintenance</th>\n",
       "      <th>Function__Facilities Planning</th>\n",
       "      <th>Function__Finance, Budget, Purchasing &amp; Distribution</th>\n",
       "      <th>Function__Food Services</th>\n",
       "      <th>Function__Governance</th>\n",
       "      <th>Function__Human Resources</th>\n",
       "      <th>Function__Instructional Materials &amp; Supplies</th>\n",
       "      <th>Function__Insurance</th>\n",
       "      <th>Function__Legal</th>\n",
       "      <th>Function__Library &amp; Media</th>\n",
       "      <th>Function__NO_LABEL</th>\n",
       "      <th>Function__Other Compensation</th>\n",
       "      <th>Function__Other Non-Compensation</th>\n",
       "      <th>Function__Parent &amp; Community Relations</th>\n",
       "      <th>Function__Physical Health &amp; Services</th>\n",
       "      <th>Function__Professional Development</th>\n",
       "      <th>Function__Recruitment</th>\n",
       "      <th>Function__Research &amp; Accountability</th>\n",
       "      <th>Function__School Administration</th>\n",
       "      <th>Function__School Supervision</th>\n",
       "      <th>Function__Security &amp; Safety</th>\n",
       "      <th>Function__Social &amp; Emotional</th>\n",
       "      <th>...</th>\n",
       "      <th>Position_Type__Teacher</th>\n",
       "      <th>Position_Type__Vice Principal</th>\n",
       "      <th>Pre_K__NO_LABEL</th>\n",
       "      <th>Pre_K__Non PreK</th>\n",
       "      <th>Pre_K__PreK</th>\n",
       "      <th>Reporting__NO_LABEL</th>\n",
       "      <th>Reporting__Non-School</th>\n",
       "      <th>Reporting__School</th>\n",
       "      <th>Sharing__Leadership &amp; Management</th>\n",
       "      <th>Sharing__NO_LABEL</th>\n",
       "      <th>Sharing__School Reported</th>\n",
       "      <th>Sharing__School on Central Budgets</th>\n",
       "      <th>Sharing__Shared Services</th>\n",
       "      <th>Student_Type__Alternative</th>\n",
       "      <th>Student_Type__At Risk</th>\n",
       "      <th>Student_Type__ELL</th>\n",
       "      <th>Student_Type__Gifted</th>\n",
       "      <th>Student_Type__NO_LABEL</th>\n",
       "      <th>Student_Type__Poverty</th>\n",
       "      <th>Student_Type__PreK</th>\n",
       "      <th>Student_Type__Special Education</th>\n",
       "      <th>Student_Type__Unspecified</th>\n",
       "      <th>Use__Business Services</th>\n",
       "      <th>Use__ISPD</th>\n",
       "      <th>Use__Instruction</th>\n",
       "      <th>Use__Leadership</th>\n",
       "      <th>Use__NO_LABEL</th>\n",
       "      <th>Use__O&amp;M</th>\n",
       "      <th>Use__Pupil Services &amp; Enrichment</th>\n",
       "      <th>Use__Untracked Budget Set-Aside</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>180042</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28872</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186915</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412396</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427740</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  104 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Function__Aides Compensation  Function__Career & Academic Counseling  \\\n",
       "180042                           0.0                                     0.0   \n",
       "28872                            0.0                                     0.1   \n",
       "186915                           0.0                                     0.1   \n",
       "412396                           0.0                                     0.1   \n",
       "427740                           0.1                                     0.0   \n",
       "\n",
       "        Function__Communications  Function__Curriculum Development  \\\n",
       "180042                       0.0                               0.0   \n",
       "28872                        0.0                               0.1   \n",
       "186915                       0.0                               0.0   \n",
       "412396                       0.0                               0.0   \n",
       "427740                       0.0                               0.1   \n",
       "\n",
       "        Function__Data Processing & Information Services  \\\n",
       "180042                                               0.0   \n",
       "28872                                                0.0   \n",
       "186915                                               0.0   \n",
       "412396                                               0.0   \n",
       "427740                                               0.0   \n",
       "\n",
       "        Function__Development & Fundraising  Function__Enrichment  \\\n",
       "180042                                  0.0                   0.0   \n",
       "28872                                   0.0                   0.2   \n",
       "186915                                  0.0                   0.0   \n",
       "412396                                  0.0                   0.0   \n",
       "427740                                  0.0                   0.0   \n",
       "\n",
       "        Function__Extended Time & Tutoring  \\\n",
       "180042                                 0.0   \n",
       "28872                                  0.0   \n",
       "186915                                 0.0   \n",
       "412396                                 0.0   \n",
       "427740                                 0.0   \n",
       "\n",
       "        Function__Facilities & Maintenance  Function__Facilities Planning  \\\n",
       "180042                                 0.0                            0.0   \n",
       "28872                                  0.0                            0.0   \n",
       "186915                                 0.1                            0.0   \n",
       "412396                                 0.1                            0.0   \n",
       "427740                                 0.1                            0.0   \n",
       "\n",
       "        Function__Finance, Budget, Purchasing & Distribution  \\\n",
       "180042                                                0.1      \n",
       "28872                                                 0.0      \n",
       "186915                                                0.0      \n",
       "412396                                                0.0      \n",
       "427740                                                0.0      \n",
       "\n",
       "        Function__Food Services  Function__Governance  \\\n",
       "180042                      0.0                   0.0   \n",
       "28872                       0.0                   0.0   \n",
       "186915                      0.0                   0.0   \n",
       "412396                      0.0                   0.0   \n",
       "427740                      0.1                   0.0   \n",
       "\n",
       "        Function__Human Resources  \\\n",
       "180042                        0.0   \n",
       "28872                         0.0   \n",
       "186915                        0.0   \n",
       "412396                        0.0   \n",
       "427740                        0.0   \n",
       "\n",
       "        Function__Instructional Materials & Supplies  Function__Insurance  \\\n",
       "180042                                           0.2                  0.0   \n",
       "28872                                            0.0                  0.0   \n",
       "186915                                           0.1                  0.0   \n",
       "412396                                           0.1                  0.0   \n",
       "427740                                           0.0                  0.0   \n",
       "\n",
       "        Function__Legal  Function__Library & Media  Function__NO_LABEL  \\\n",
       "180042              0.0                        0.0                 0.0   \n",
       "28872               0.0                        0.0                 0.0   \n",
       "186915              0.0                        0.0                 0.2   \n",
       "412396              0.0                        0.0                 0.2   \n",
       "427740              0.0                        0.0                 0.2   \n",
       "\n",
       "        Function__Other Compensation  Function__Other Non-Compensation  \\\n",
       "180042                           0.0                               0.3   \n",
       "28872                            0.0                               0.0   \n",
       "186915                           0.0                               0.1   \n",
       "412396                           0.0                               0.1   \n",
       "427740                           0.0                               0.0   \n",
       "\n",
       "        Function__Parent & Community Relations  \\\n",
       "180042                                     0.0   \n",
       "28872                                      0.1   \n",
       "186915                                     0.0   \n",
       "412396                                     0.0   \n",
       "427740                                     0.0   \n",
       "\n",
       "        Function__Physical Health & Services  \\\n",
       "180042                                   0.0   \n",
       "28872                                    0.0   \n",
       "186915                                   0.1   \n",
       "412396                                   0.1   \n",
       "427740                                   0.0   \n",
       "\n",
       "        Function__Professional Development  Function__Recruitment  \\\n",
       "180042                                 0.1                    0.0   \n",
       "28872                                  0.0                    0.0   \n",
       "186915                                 0.1                    0.0   \n",
       "412396                                 0.1                    0.0   \n",
       "427740                                 0.0                    0.0   \n",
       "\n",
       "        Function__Research & Accountability  Function__School Administration  \\\n",
       "180042                                  0.0                              0.2   \n",
       "28872                                   0.0                              0.1   \n",
       "186915                                  0.0                              0.1   \n",
       "412396                                  0.0                              0.1   \n",
       "427740                                  0.0                              0.3   \n",
       "\n",
       "        Function__School Supervision  Function__Security & Safety  \\\n",
       "180042                           0.0                          0.0   \n",
       "28872                            0.0                          0.0   \n",
       "186915                           0.0                          0.0   \n",
       "412396                           0.0                          0.0   \n",
       "427740                           0.0                          0.0   \n",
       "\n",
       "        Function__Social & Emotional               ...                 \\\n",
       "180042                           0.1               ...                  \n",
       "28872                            0.2               ...                  \n",
       "186915                           0.0               ...                  \n",
       "412396                           0.0               ...                  \n",
       "427740                           0.0               ...                  \n",
       "\n",
       "        Position_Type__Teacher  Position_Type__Vice Principal  \\\n",
       "180042                     0.0                            0.0   \n",
       "28872                      0.3                            0.1   \n",
       "186915                     0.1                            0.0   \n",
       "412396                     0.1                            0.0   \n",
       "427740                     0.1                            0.0   \n",
       "\n",
       "        Pre_K__NO_LABEL  Pre_K__Non PreK  Pre_K__PreK  Reporting__NO_LABEL  \\\n",
       "180042              0.5              0.5          0.0                  0.0   \n",
       "28872               0.6              0.4          0.0                  0.0   \n",
       "186915              0.8              0.2          0.0                  0.2   \n",
       "412396              0.8              0.2          0.0                  0.2   \n",
       "427740              0.5              0.5          0.0                  0.2   \n",
       "\n",
       "        Reporting__Non-School  Reporting__School  \\\n",
       "180042                    0.2                0.8   \n",
       "28872                     0.2                0.8   \n",
       "186915                    0.2                0.6   \n",
       "412396                    0.2                0.6   \n",
       "427740                    0.2                0.6   \n",
       "\n",
       "        Sharing__Leadership & Management  Sharing__NO_LABEL  \\\n",
       "180042                               0.2                0.0   \n",
       "28872                                0.1                0.0   \n",
       "186915                               0.2                0.2   \n",
       "412396                               0.2                0.2   \n",
       "427740                               0.1                0.2   \n",
       "\n",
       "        Sharing__School Reported  Sharing__School on Central Budgets  \\\n",
       "180042                       0.7                                 0.1   \n",
       "28872                        0.7                                 0.0   \n",
       "186915                       0.6                                 0.0   \n",
       "412396                       0.6                                 0.0   \n",
       "427740                       0.5                                 0.0   \n",
       "\n",
       "        Sharing__Shared Services  Student_Type__Alternative  \\\n",
       "180042                       0.0                        0.0   \n",
       "28872                        0.2                        0.1   \n",
       "186915                       0.0                        0.0   \n",
       "412396                       0.0                        0.0   \n",
       "427740                       0.2                        0.0   \n",
       "\n",
       "        Student_Type__At Risk  Student_Type__ELL  Student_Type__Gifted  \\\n",
       "180042                    0.0                0.0                   0.1   \n",
       "28872                     0.0                0.0                   0.0   \n",
       "186915                    0.0                0.2                   0.0   \n",
       "412396                    0.0                0.2                   0.0   \n",
       "427740                    0.0                0.0                   0.0   \n",
       "\n",
       "        Student_Type__NO_LABEL  Student_Type__Poverty  Student_Type__PreK  \\\n",
       "180042                     0.0                    0.0                 0.0   \n",
       "28872                      0.0                    0.0                 0.0   \n",
       "186915                     0.3                    0.0                 0.0   \n",
       "412396                     0.3                    0.0                 0.0   \n",
       "427740                     0.4                    0.0                 0.0   \n",
       "\n",
       "        Student_Type__Special Education  Student_Type__Unspecified  \\\n",
       "180042                              0.0                        0.9   \n",
       "28872                               0.0                        0.9   \n",
       "186915                              0.0                        0.5   \n",
       "412396                              0.0                        0.5   \n",
       "427740                              0.1                        0.5   \n",
       "\n",
       "        Use__Business Services  Use__ISPD  Use__Instruction  Use__Leadership  \\\n",
       "180042                     0.1        0.1               0.5              0.2   \n",
       "28872                      0.0        0.1               0.2              0.1   \n",
       "186915                     0.0        0.1               0.3              0.1   \n",
       "412396                     0.0        0.1               0.3              0.1   \n",
       "427740                     0.0        0.1               0.2              0.2   \n",
       "\n",
       "        Use__NO_LABEL  Use__O&M  Use__Pupil Services & Enrichment  \\\n",
       "180042            0.0       0.0                               0.1   \n",
       "28872             0.0       0.0                               0.6   \n",
       "186915            0.3       0.1                               0.1   \n",
       "412396            0.3       0.1                               0.1   \n",
       "427740            0.3       0.2                               0.0   \n",
       "\n",
       "        Use__Untracked Budget Set-Aside  \n",
       "180042                              0.0  \n",
       "28872                               0.0  \n",
       "186915                              0.0  \n",
       "412396                              0.0  \n",
       "427740                              0.0  \n",
       "\n",
       "[5 rows x 104 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_rf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Function__Aides Compensation</th>\n",
       "      <th>Function__Career &amp; Academic Counseling</th>\n",
       "      <th>Function__Communications</th>\n",
       "      <th>Function__Curriculum Development</th>\n",
       "      <th>Function__Data Processing &amp; Information Services</th>\n",
       "      <th>Function__Development &amp; Fundraising</th>\n",
       "      <th>Function__Enrichment</th>\n",
       "      <th>Function__Extended Time &amp; Tutoring</th>\n",
       "      <th>Function__Facilities &amp; Maintenance</th>\n",
       "      <th>Function__Facilities Planning</th>\n",
       "      <th>Function__Finance, Budget, Purchasing &amp; Distribution</th>\n",
       "      <th>Function__Food Services</th>\n",
       "      <th>Function__Governance</th>\n",
       "      <th>Function__Human Resources</th>\n",
       "      <th>Function__Instructional Materials &amp; Supplies</th>\n",
       "      <th>Function__Insurance</th>\n",
       "      <th>Function__Legal</th>\n",
       "      <th>Function__Library &amp; Media</th>\n",
       "      <th>Function__NO_LABEL</th>\n",
       "      <th>Function__Other Compensation</th>\n",
       "      <th>Function__Other Non-Compensation</th>\n",
       "      <th>Function__Parent &amp; Community Relations</th>\n",
       "      <th>Function__Physical Health &amp; Services</th>\n",
       "      <th>Function__Professional Development</th>\n",
       "      <th>Function__Recruitment</th>\n",
       "      <th>Function__Research &amp; Accountability</th>\n",
       "      <th>Function__School Administration</th>\n",
       "      <th>Function__School Supervision</th>\n",
       "      <th>Function__Security &amp; Safety</th>\n",
       "      <th>Function__Social &amp; Emotional</th>\n",
       "      <th>...</th>\n",
       "      <th>Position_Type__Teacher</th>\n",
       "      <th>Position_Type__Vice Principal</th>\n",
       "      <th>Pre_K__NO_LABEL</th>\n",
       "      <th>Pre_K__Non PreK</th>\n",
       "      <th>Pre_K__PreK</th>\n",
       "      <th>Reporting__NO_LABEL</th>\n",
       "      <th>Reporting__Non-School</th>\n",
       "      <th>Reporting__School</th>\n",
       "      <th>Sharing__Leadership &amp; Management</th>\n",
       "      <th>Sharing__NO_LABEL</th>\n",
       "      <th>Sharing__School Reported</th>\n",
       "      <th>Sharing__School on Central Budgets</th>\n",
       "      <th>Sharing__Shared Services</th>\n",
       "      <th>Student_Type__Alternative</th>\n",
       "      <th>Student_Type__At Risk</th>\n",
       "      <th>Student_Type__ELL</th>\n",
       "      <th>Student_Type__Gifted</th>\n",
       "      <th>Student_Type__NO_LABEL</th>\n",
       "      <th>Student_Type__Poverty</th>\n",
       "      <th>Student_Type__PreK</th>\n",
       "      <th>Student_Type__Special Education</th>\n",
       "      <th>Student_Type__Unspecified</th>\n",
       "      <th>Use__Business Services</th>\n",
       "      <th>Use__ISPD</th>\n",
       "      <th>Use__Instruction</th>\n",
       "      <th>Use__Leadership</th>\n",
       "      <th>Use__NO_LABEL</th>\n",
       "      <th>Use__O&amp;M</th>\n",
       "      <th>Use__Pupil Services &amp; Enrichment</th>\n",
       "      <th>Use__Untracked Budget Set-Aside</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>180042</th>\n",
       "      <td>0.035044</td>\n",
       "      <td>0.009033</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.005156</td>\n",
       "      <td>0.004115</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.023341</td>\n",
       "      <td>0.012113</td>\n",
       "      <td>0.017187</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.002960</td>\n",
       "      <td>0.004691</td>\n",
       "      <td>0.000383</td>\n",
       "      <td>0.000736</td>\n",
       "      <td>0.106760</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.005736</td>\n",
       "      <td>0.119529</td>\n",
       "      <td>0.006688</td>\n",
       "      <td>0.025591</td>\n",
       "      <td>0.003441</td>\n",
       "      <td>0.005528</td>\n",
       "      <td>0.019680</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000735</td>\n",
       "      <td>0.024214</td>\n",
       "      <td>0.003384</td>\n",
       "      <td>0.001597</td>\n",
       "      <td>0.011827</td>\n",
       "      <td>...</td>\n",
       "      <td>0.066468</td>\n",
       "      <td>0.001552</td>\n",
       "      <td>0.685863</td>\n",
       "      <td>0.268437</td>\n",
       "      <td>0.006203</td>\n",
       "      <td>0.102979</td>\n",
       "      <td>0.131416</td>\n",
       "      <td>0.810076</td>\n",
       "      <td>0.024852</td>\n",
       "      <td>0.103729</td>\n",
       "      <td>0.841351</td>\n",
       "      <td>0.034106</td>\n",
       "      <td>0.086982</td>\n",
       "      <td>0.000772</td>\n",
       "      <td>0.000623</td>\n",
       "      <td>0.005840</td>\n",
       "      <td>0.000681</td>\n",
       "      <td>0.255563</td>\n",
       "      <td>0.011720</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.076170</td>\n",
       "      <td>0.517811</td>\n",
       "      <td>0.004018</td>\n",
       "      <td>0.034925</td>\n",
       "      <td>0.548877</td>\n",
       "      <td>0.028157</td>\n",
       "      <td>0.208611</td>\n",
       "      <td>0.048271</td>\n",
       "      <td>0.076109</td>\n",
       "      <td>0.003594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28872</th>\n",
       "      <td>0.043341</td>\n",
       "      <td>0.004671</td>\n",
       "      <td>0.000158</td>\n",
       "      <td>0.007053</td>\n",
       "      <td>0.004726</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.346247</td>\n",
       "      <td>0.032432</td>\n",
       "      <td>0.066243</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.009442</td>\n",
       "      <td>0.006800</td>\n",
       "      <td>0.000579</td>\n",
       "      <td>0.001067</td>\n",
       "      <td>0.062761</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000237</td>\n",
       "      <td>0.004635</td>\n",
       "      <td>0.012955</td>\n",
       "      <td>0.000677</td>\n",
       "      <td>0.000803</td>\n",
       "      <td>0.007210</td>\n",
       "      <td>0.015402</td>\n",
       "      <td>0.023446</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.005893</td>\n",
       "      <td>0.069195</td>\n",
       "      <td>0.003190</td>\n",
       "      <td>0.002421</td>\n",
       "      <td>0.012070</td>\n",
       "      <td>...</td>\n",
       "      <td>0.165565</td>\n",
       "      <td>0.003492</td>\n",
       "      <td>0.764101</td>\n",
       "      <td>0.253937</td>\n",
       "      <td>0.004298</td>\n",
       "      <td>0.011162</td>\n",
       "      <td>0.393182</td>\n",
       "      <td>0.693085</td>\n",
       "      <td>0.071155</td>\n",
       "      <td>0.023945</td>\n",
       "      <td>0.677781</td>\n",
       "      <td>0.085284</td>\n",
       "      <td>0.183946</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>0.001791</td>\n",
       "      <td>0.014778</td>\n",
       "      <td>0.001482</td>\n",
       "      <td>0.047052</td>\n",
       "      <td>0.013795</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.055897</td>\n",
       "      <td>0.833639</td>\n",
       "      <td>0.047163</td>\n",
       "      <td>0.030864</td>\n",
       "      <td>0.304865</td>\n",
       "      <td>0.123692</td>\n",
       "      <td>0.058561</td>\n",
       "      <td>0.112773</td>\n",
       "      <td>0.426434</td>\n",
       "      <td>0.000275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186915</th>\n",
       "      <td>0.068035</td>\n",
       "      <td>0.023687</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.003775</td>\n",
       "      <td>0.002454</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.014611</td>\n",
       "      <td>0.004399</td>\n",
       "      <td>0.014048</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.002093</td>\n",
       "      <td>0.001688</td>\n",
       "      <td>0.000408</td>\n",
       "      <td>0.000297</td>\n",
       "      <td>0.076740</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.002104</td>\n",
       "      <td>0.027777</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.005738</td>\n",
       "      <td>0.002662</td>\n",
       "      <td>0.014151</td>\n",
       "      <td>0.008969</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000181</td>\n",
       "      <td>0.033915</td>\n",
       "      <td>0.001949</td>\n",
       "      <td>0.000319</td>\n",
       "      <td>0.008079</td>\n",
       "      <td>...</td>\n",
       "      <td>0.116599</td>\n",
       "      <td>0.000777</td>\n",
       "      <td>0.882500</td>\n",
       "      <td>0.049357</td>\n",
       "      <td>0.006004</td>\n",
       "      <td>0.018001</td>\n",
       "      <td>0.110613</td>\n",
       "      <td>0.822952</td>\n",
       "      <td>0.015549</td>\n",
       "      <td>0.025805</td>\n",
       "      <td>0.774871</td>\n",
       "      <td>0.027887</td>\n",
       "      <td>0.058084</td>\n",
       "      <td>0.000391</td>\n",
       "      <td>0.000875</td>\n",
       "      <td>0.023295</td>\n",
       "      <td>0.000513</td>\n",
       "      <td>0.188035</td>\n",
       "      <td>0.008833</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>0.095172</td>\n",
       "      <td>0.384363</td>\n",
       "      <td>0.006559</td>\n",
       "      <td>0.016105</td>\n",
       "      <td>0.712228</td>\n",
       "      <td>0.035126</td>\n",
       "      <td>0.085500</td>\n",
       "      <td>0.022647</td>\n",
       "      <td>0.071729</td>\n",
       "      <td>0.000102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412396</th>\n",
       "      <td>0.067884</td>\n",
       "      <td>0.023532</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.003762</td>\n",
       "      <td>0.002443</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.014568</td>\n",
       "      <td>0.004390</td>\n",
       "      <td>0.014010</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.002088</td>\n",
       "      <td>0.001686</td>\n",
       "      <td>0.000406</td>\n",
       "      <td>0.000296</td>\n",
       "      <td>0.076481</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.002098</td>\n",
       "      <td>0.027708</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.005717</td>\n",
       "      <td>0.002656</td>\n",
       "      <td>0.014106</td>\n",
       "      <td>0.008953</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000180</td>\n",
       "      <td>0.033843</td>\n",
       "      <td>0.001943</td>\n",
       "      <td>0.000319</td>\n",
       "      <td>0.008061</td>\n",
       "      <td>...</td>\n",
       "      <td>0.118218</td>\n",
       "      <td>0.000775</td>\n",
       "      <td>0.882676</td>\n",
       "      <td>0.049284</td>\n",
       "      <td>0.005990</td>\n",
       "      <td>0.017923</td>\n",
       "      <td>0.110500</td>\n",
       "      <td>0.823951</td>\n",
       "      <td>0.015511</td>\n",
       "      <td>0.025785</td>\n",
       "      <td>0.777487</td>\n",
       "      <td>0.027844</td>\n",
       "      <td>0.058031</td>\n",
       "      <td>0.000391</td>\n",
       "      <td>0.000873</td>\n",
       "      <td>0.023209</td>\n",
       "      <td>0.000512</td>\n",
       "      <td>0.190932</td>\n",
       "      <td>0.008823</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>0.094927</td>\n",
       "      <td>0.382252</td>\n",
       "      <td>0.006515</td>\n",
       "      <td>0.016055</td>\n",
       "      <td>0.713104</td>\n",
       "      <td>0.035043</td>\n",
       "      <td>0.085294</td>\n",
       "      <td>0.022625</td>\n",
       "      <td>0.071502</td>\n",
       "      <td>0.000102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427740</th>\n",
       "      <td>0.052017</td>\n",
       "      <td>0.211306</td>\n",
       "      <td>0.000247</td>\n",
       "      <td>0.002943</td>\n",
       "      <td>0.006818</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.016244</td>\n",
       "      <td>0.002897</td>\n",
       "      <td>0.044838</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.006023</td>\n",
       "      <td>0.004567</td>\n",
       "      <td>0.000197</td>\n",
       "      <td>0.000621</td>\n",
       "      <td>0.046854</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>0.006088</td>\n",
       "      <td>0.018502</td>\n",
       "      <td>0.000166</td>\n",
       "      <td>0.000454</td>\n",
       "      <td>0.002349</td>\n",
       "      <td>0.002549</td>\n",
       "      <td>0.012234</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000349</td>\n",
       "      <td>0.424999</td>\n",
       "      <td>0.001536</td>\n",
       "      <td>0.001715</td>\n",
       "      <td>0.003781</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027036</td>\n",
       "      <td>0.009271</td>\n",
       "      <td>0.676855</td>\n",
       "      <td>0.242153</td>\n",
       "      <td>0.004491</td>\n",
       "      <td>0.010051</td>\n",
       "      <td>0.162554</td>\n",
       "      <td>0.874431</td>\n",
       "      <td>0.041169</td>\n",
       "      <td>0.021105</td>\n",
       "      <td>0.868192</td>\n",
       "      <td>0.016074</td>\n",
       "      <td>0.103978</td>\n",
       "      <td>0.000220</td>\n",
       "      <td>0.000620</td>\n",
       "      <td>0.006931</td>\n",
       "      <td>0.000304</td>\n",
       "      <td>0.101587</td>\n",
       "      <td>0.005021</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.036306</td>\n",
       "      <td>0.784040</td>\n",
       "      <td>0.032007</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.288827</td>\n",
       "      <td>0.436190</td>\n",
       "      <td>0.064791</td>\n",
       "      <td>0.078117</td>\n",
       "      <td>0.055109</td>\n",
       "      <td>0.000177</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  104 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Function__Aides Compensation  Function__Career & Academic Counseling  \\\n",
       "180042                      0.035044                                0.009033   \n",
       "28872                       0.043341                                0.004671   \n",
       "186915                      0.068035                                0.023687   \n",
       "412396                      0.067884                                0.023532   \n",
       "427740                      0.052017                                0.211306   \n",
       "\n",
       "        Function__Communications  Function__Curriculum Development  \\\n",
       "180042                  0.000020                          0.005156   \n",
       "28872                   0.000158                          0.007053   \n",
       "186915                  0.000013                          0.003775   \n",
       "412396                  0.000013                          0.003762   \n",
       "427740                  0.000247                          0.002943   \n",
       "\n",
       "        Function__Data Processing & Information Services  \\\n",
       "180042                                          0.004115   \n",
       "28872                                           0.004726   \n",
       "186915                                          0.002454   \n",
       "412396                                          0.002443   \n",
       "427740                                          0.006818   \n",
       "\n",
       "        Function__Development & Fundraising  Function__Enrichment  \\\n",
       "180042                             0.000051              0.023341   \n",
       "28872                              0.000033              0.346247   \n",
       "186915                             0.000007              0.014611   \n",
       "412396                             0.000007              0.014568   \n",
       "427740                             0.000011              0.016244   \n",
       "\n",
       "        Function__Extended Time & Tutoring  \\\n",
       "180042                            0.012113   \n",
       "28872                             0.032432   \n",
       "186915                            0.004399   \n",
       "412396                            0.004390   \n",
       "427740                            0.002897   \n",
       "\n",
       "        Function__Facilities & Maintenance  Function__Facilities Planning  \\\n",
       "180042                            0.017187                       0.000016   \n",
       "28872                             0.066243                       0.000027   \n",
       "186915                            0.014048                       0.000007   \n",
       "412396                            0.014010                       0.000007   \n",
       "427740                            0.044838                       0.000008   \n",
       "\n",
       "        Function__Finance, Budget, Purchasing & Distribution  \\\n",
       "180042                                           0.002960      \n",
       "28872                                            0.009442      \n",
       "186915                                           0.002093      \n",
       "412396                                           0.002088      \n",
       "427740                                           0.006023      \n",
       "\n",
       "        Function__Food Services  Function__Governance  \\\n",
       "180042                 0.004691              0.000383   \n",
       "28872                  0.006800              0.000579   \n",
       "186915                 0.001688              0.000408   \n",
       "412396                 0.001686              0.000406   \n",
       "427740                 0.004567              0.000197   \n",
       "\n",
       "        Function__Human Resources  \\\n",
       "180042                   0.000736   \n",
       "28872                    0.001067   \n",
       "186915                   0.000297   \n",
       "412396                   0.000296   \n",
       "427740                   0.000621   \n",
       "\n",
       "        Function__Instructional Materials & Supplies  Function__Insurance  \\\n",
       "180042                                      0.106760             0.000100   \n",
       "28872                                       0.062761             0.000037   \n",
       "186915                                      0.076740             0.000010   \n",
       "412396                                      0.076481             0.000010   \n",
       "427740                                      0.046854             0.000016   \n",
       "\n",
       "        Function__Legal  Function__Library & Media  Function__NO_LABEL  \\\n",
       "180042         0.000050                   0.005736            0.119529   \n",
       "28872          0.000237                   0.004635            0.012955   \n",
       "186915         0.000044                   0.002104            0.027777   \n",
       "412396         0.000044                   0.002098            0.027708   \n",
       "427740         0.000206                   0.006088            0.018502   \n",
       "\n",
       "        Function__Other Compensation  Function__Other Non-Compensation  \\\n",
       "180042                      0.006688                          0.025591   \n",
       "28872                       0.000677                          0.000803   \n",
       "186915                      0.000098                          0.005738   \n",
       "412396                      0.000098                          0.005717   \n",
       "427740                      0.000166                          0.000454   \n",
       "\n",
       "        Function__Parent & Community Relations  \\\n",
       "180042                                0.003441   \n",
       "28872                                 0.007210   \n",
       "186915                                0.002662   \n",
       "412396                                0.002656   \n",
       "427740                                0.002349   \n",
       "\n",
       "        Function__Physical Health & Services  \\\n",
       "180042                              0.005528   \n",
       "28872                               0.015402   \n",
       "186915                              0.014151   \n",
       "412396                              0.014106   \n",
       "427740                              0.002549   \n",
       "\n",
       "        Function__Professional Development  Function__Recruitment  \\\n",
       "180042                            0.019680               0.000008   \n",
       "28872                             0.023446               0.000046   \n",
       "186915                            0.008969               0.000004   \n",
       "412396                            0.008953               0.000004   \n",
       "427740                            0.012234               0.000013   \n",
       "\n",
       "        Function__Research & Accountability  Function__School Administration  \\\n",
       "180042                             0.000735                         0.024214   \n",
       "28872                              0.005893                         0.069195   \n",
       "186915                             0.000181                         0.033915   \n",
       "412396                             0.000180                         0.033843   \n",
       "427740                             0.000349                         0.424999   \n",
       "\n",
       "        Function__School Supervision  Function__Security & Safety  \\\n",
       "180042                      0.003384                     0.001597   \n",
       "28872                       0.003190                     0.002421   \n",
       "186915                      0.001949                     0.000319   \n",
       "412396                      0.001943                     0.000319   \n",
       "427740                      0.001536                     0.001715   \n",
       "\n",
       "        Function__Social & Emotional               ...                 \\\n",
       "180042                      0.011827               ...                  \n",
       "28872                       0.012070               ...                  \n",
       "186915                      0.008079               ...                  \n",
       "412396                      0.008061               ...                  \n",
       "427740                      0.003781               ...                  \n",
       "\n",
       "        Position_Type__Teacher  Position_Type__Vice Principal  \\\n",
       "180042                0.066468                       0.001552   \n",
       "28872                 0.165565                       0.003492   \n",
       "186915                0.116599                       0.000777   \n",
       "412396                0.118218                       0.000775   \n",
       "427740                0.027036                       0.009271   \n",
       "\n",
       "        Pre_K__NO_LABEL  Pre_K__Non PreK  Pre_K__PreK  Reporting__NO_LABEL  \\\n",
       "180042         0.685863         0.268437     0.006203             0.102979   \n",
       "28872          0.764101         0.253937     0.004298             0.011162   \n",
       "186915         0.882500         0.049357     0.006004             0.018001   \n",
       "412396         0.882676         0.049284     0.005990             0.017923   \n",
       "427740         0.676855         0.242153     0.004491             0.010051   \n",
       "\n",
       "        Reporting__Non-School  Reporting__School  \\\n",
       "180042               0.131416           0.810076   \n",
       "28872                0.393182           0.693085   \n",
       "186915               0.110613           0.822952   \n",
       "412396               0.110500           0.823951   \n",
       "427740               0.162554           0.874431   \n",
       "\n",
       "        Sharing__Leadership & Management  Sharing__NO_LABEL  \\\n",
       "180042                          0.024852           0.103729   \n",
       "28872                           0.071155           0.023945   \n",
       "186915                          0.015549           0.025805   \n",
       "412396                          0.015511           0.025785   \n",
       "427740                          0.041169           0.021105   \n",
       "\n",
       "        Sharing__School Reported  Sharing__School on Central Budgets  \\\n",
       "180042                  0.841351                            0.034106   \n",
       "28872                   0.677781                            0.085284   \n",
       "186915                  0.774871                            0.027887   \n",
       "412396                  0.777487                            0.027844   \n",
       "427740                  0.868192                            0.016074   \n",
       "\n",
       "        Sharing__Shared Services  Student_Type__Alternative  \\\n",
       "180042                  0.086982                   0.000772   \n",
       "28872                   0.183946                   0.000476   \n",
       "186915                  0.058084                   0.000391   \n",
       "412396                  0.058031                   0.000391   \n",
       "427740                  0.103978                   0.000220   \n",
       "\n",
       "        Student_Type__At Risk  Student_Type__ELL  Student_Type__Gifted  \\\n",
       "180042               0.000623           0.005840              0.000681   \n",
       "28872                0.001791           0.014778              0.001482   \n",
       "186915               0.000875           0.023295              0.000513   \n",
       "412396               0.000873           0.023209              0.000512   \n",
       "427740               0.000620           0.006931              0.000304   \n",
       "\n",
       "        Student_Type__NO_LABEL  Student_Type__Poverty  Student_Type__PreK  \\\n",
       "180042                0.255563               0.011720            0.000045   \n",
       "28872                 0.047052               0.013795            0.000074   \n",
       "186915                0.188035               0.008833            0.000131   \n",
       "412396                0.190932               0.008823            0.000131   \n",
       "427740                0.101587               0.005021            0.000013   \n",
       "\n",
       "        Student_Type__Special Education  Student_Type__Unspecified  \\\n",
       "180042                         0.076170                   0.517811   \n",
       "28872                          0.055897                   0.833639   \n",
       "186915                         0.095172                   0.384363   \n",
       "412396                         0.094927                   0.382252   \n",
       "427740                         0.036306                   0.784040   \n",
       "\n",
       "        Use__Business Services  Use__ISPD  Use__Instruction  Use__Leadership  \\\n",
       "180042                0.004018   0.034925          0.548877         0.028157   \n",
       "28872                 0.047163   0.030864          0.304865         0.123692   \n",
       "186915                0.006559   0.016105          0.712228         0.035126   \n",
       "412396                0.006515   0.016055          0.713104         0.035043   \n",
       "427740                0.032007   0.037037          0.288827         0.436190   \n",
       "\n",
       "        Use__NO_LABEL  Use__O&M  Use__Pupil Services & Enrichment  \\\n",
       "180042       0.208611  0.048271                          0.076109   \n",
       "28872        0.058561  0.112773                          0.426434   \n",
       "186915       0.085500  0.022647                          0.071729   \n",
       "412396       0.085294  0.022625                          0.071502   \n",
       "427740       0.064791  0.078117                          0.055109   \n",
       "\n",
       "        Use__Untracked Budget Set-Aside  \n",
       "180042                         0.003594  \n",
       "28872                          0.000275  \n",
       "186915                         0.000102  \n",
       "412396                         0.000102  \n",
       "427740                         0.000177  \n",
       "\n",
       "[5 rows x 104 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all(prediction_df.columns == pred_rf.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all(prediction_df.index == pred_rf.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_rf.to_csv('sub_rf_defaults.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Can you adjust the model or parameters to improve accuracy?\n",
    "You just saw a substantial improvement in accuracy by swapping out the model. Pipelines are amazing!\n",
    "\n",
    "Can you make it better? Try changing the parameter n_estimators of RandomForestClassifier(), whose default value is 10, to 15.\n",
    "\n",
    "##### INSTRUCTIONS\n",
    "\n",
    "Import the RandomForestClassifier from sklearn.ensemble.\n",
    "Add a RandomForestClassifier() step with n_estimators=15 to the pipeline with a name of 'clf'.\n",
    "Hit 'Submit Answer' to fit the pipeline to the training data and compute its accuracy.\n",
    "\n",
    "##### Note: \n",
    "\n",
    "This model not submitted because the last one (similar accuracy) has high log_loss (i.e. score).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy on budget dataset:  0.9141215414402598\n"
     ]
    }
   ],
   "source": [
    "# Import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Add model step to pipeline: pl\n",
    "pl = Pipeline([\n",
    "        ('union', FeatureUnion(\n",
    "            transformer_list = [\n",
    "                ('numeric_features', Pipeline([\n",
    "                    ('selector', get_numeric_data),\n",
    "                    ('imputer', Imputer())\n",
    "                ])),\n",
    "                ('text_features', Pipeline([\n",
    "                    ('selector', get_text_data),\n",
    "                    ('vectorizer', CountVectorizer())\n",
    "                ]))\n",
    "             ]\n",
    "        )),\n",
    "        ('clf', RandomForestClassifier(n_estimators=15))\n",
    "    ])\n",
    "\n",
    "# Fit to the training data\n",
    "pl.fit(X_train, y_train)\n",
    "\n",
    "# Compute and print accuracy\n",
    "accuracy = pl.score(X_test, y_test)\n",
    "print(\"\\nAccuracy on budget dataset: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submitted 28may2018\n",
    "\n",
    "##### Log-loss on holdout set is 2.1101. MUCH worse log-loss.\n",
    "\n",
    "I could have known that if I'd measured it.  I have the tool to do it.\n",
    "\n",
    "I've learned that estimators can used in at least 2 ways.  The first is as they've done, one-hot encoding the targets.  The second is letting (capable) estimators work with multiple labels for each output.  That way, you just encode the multilabel target column as category.  If you then predict probas you get a list of outputs which are arrays of the \n",
    "probabilities for each label.\n",
    "\n",
    "Here the RFC is providing a list of arrays of probalities for 0 and 1, with one list element for each column.  I wrangled that into the appropriate shape for submission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deciding what's a word\n",
    "Before you build up to the winning pipeline, it will be useful to look a little deeper into how the text features will be processed.\n",
    "\n",
    "In this exercise, you will use CountVectorizer on the training data X_train (preloaded into the workspace) to see the effect of tokenization on punctuation.\n",
    "\n",
    "Remember, since CountVectorizer expects a vector, you'll need to use the preloaded function, combine_text_columns before fitting to the training data.\n",
    "\n",
    "##### INSTRUCTIONS\n",
    "\n",
    "Create text_vector by preprocessing X_train using combine_text_columns. This is important, or else you won't get any tokens!\n",
    "Instantiate CountVectorizer as text_features. Specify the keyword argument token_pattern=TOKENS_ALPHANUMERIC.\n",
    "Fit text_features to the text_vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00a', '12', '1st', '2nd', '3rd', '4th', '5', '56', '5th', '6']\n"
     ]
    }
   ],
   "source": [
    "# Import the CountVectorizer\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create the text vector\n",
    "text_vector = combine_text_columns(X_train)\n",
    "\n",
    "# Create the token pattern: TOKENS_ALPHANUMERIC\n",
    "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'\n",
    "\n",
    "# Instantiate the CountVectorizer: text_features\n",
    "text_features = CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC)\n",
    "\n",
    "# Fit text_features to the text vector\n",
    "text_features.fit(text_vector)\n",
    "\n",
    "# Print the first 10 tokens\n",
    "print(text_features.get_feature_names()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3186"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many features - that's a lotta features\n",
    "len(text_features.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  See what happens to the size of features if you look at bigrams too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29287\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the CountVectorizer: text_features\n",
    "__text_features = CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC, ngram_range=(1, 2))\n",
    "\n",
    "# Fit text_features to the text vector\n",
    "__text_features.fit(text_vector)\n",
    "\n",
    "# Print the first 10 tokens\n",
    "print(len(__text_features.get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### N-gram range in scikit-learn\n",
    "In this exercise you'll insert a CountVectorizer instance into your pipeline for the main dataset, and compute multiple n-gram features to be used in the model.\n",
    "\n",
    "In order to look for ngram relationships at multiple scales, you will use the ngram_range parameter as Peter discussed in the video.\n",
    "\n",
    "Special functions: You'll notice a couple of new steps provided in the pipeline in this and many of the remaining exercises. Specifically, the dim_red step following the vectorizer step , and the scale step preceeding the clf (classification) step.\n",
    "\n",
    "These have been added in order to account for the fact that you're using a reduced-size sample of the full dataset in this course. To make sure the models perform as the expert competition winner intended, we have to apply a dimensionality reduction technique, which is what the dim_red step does, and we have to scale the features to lie between -1 and 1, which is what the scale step does.\n",
    "\n",
    "The dim_red step uses a scikit-learn function called SelectKBest(), applying something called the chi-squared test to select the K \"best\" features. The scale step uses a scikit-learn function called MaxAbsScaler() in order to squash the relevant features into the interval -1 to 1.\n",
    "\n",
    "You won't need to do anything extra with these functions here, just complete the vectorizing pipeline steps below. However, notice how easy it was to add more processing steps to our pipeline!\n",
    "\n",
    "##### Instructions\n",
    "Import CountVectorizer from sklearn.feature_extraction.text.\n",
    "Add a CountVectorizer step to the pipeline with the name 'vectorizer'.\n",
    "Set the token pattern to be TOKENS_ALPHANUMERIC.\n",
    "Set the ngram_range to be (1, 2).\n",
    "\n",
    "##### Comment\n",
    "\n",
    "There's more going on here than they described above.\n",
    "\n",
    "The pull in from sklearn.feature_selection chi2 and  SelectKBest.  I bet they're trying to downsize the feature set without losing information.  Also, MaxAbsScaler from sklearn preprocessing.  They scale everything right before the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pipeline\n",
    "# from sklearn.pipeline import Pipeline\n",
    "\n",
    "# # Import classifiers\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# # Import CountVectorizer\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Import other preprocessing modules\n",
    "# from sklearn.preprocessing import Imputer\n",
    "from sklearn.feature_selection import chi2, SelectKBest\n",
    "\n",
    "# Select 300 best features\n",
    "chi_k = 300\n",
    "\n",
    "# Import functional utilities\n",
    "from sklearn.preprocessing import FunctionTransformer, MaxAbsScaler\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "# Perform preprocessing\n",
    "get_text_data = FunctionTransformer(combine_text_columns, validate=False)\n",
    "get_numeric_data = FunctionTransformer(lambda x: x[NUMERIC_COLUMNS], validate=False)\n",
    "\n",
    "# Create the token pattern: TOKENS_ALPHANUMERIC\n",
    "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'\n",
    "\n",
    "# Instantiate pipeline: pl\n",
    "pl = Pipeline([\n",
    "        ('union', FeatureUnion(\n",
    "            transformer_list = [\n",
    "                ('numeric_features', Pipeline([\n",
    "                    ('selector', get_numeric_data),\n",
    "                    ('imputer', Imputer())\n",
    "                ])),\n",
    "                ('text_features', Pipeline([\n",
    "                    ('selector', get_text_data),\n",
    "                    # soup up vectorizer a bit\n",
    "                    ('vectorizer', CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC,\n",
    "                                                   ngram_range=(1, 2))),\n",
    "                    ('dim_red', SelectKBest(chi2, chi_k))\n",
    "                ]))\n",
    "             ]\n",
    "        )),\n",
    "        ('scale', MaxAbsScaler()),\n",
    "        ('clf', OneVsRestClassifier(LogisticRegression()))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### They haven't fitted or predicted with this model in the course.  It seems natural to do that.  Here we go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit time: 557.132529835839 seconds\n",
      "\n",
      "Accuracy on budget dataset:  0.5482480794453813\n"
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "# Fit to the training data\n",
    "pl.fit(X_train, y_train)\n",
    "end = timer()\n",
    "print('fit time: {} seconds'.format(end - start))\n",
    "\n",
    "# Compute and print accuracy\n",
    "accuracy = pl.score(X_test, y_test)\n",
    "print(\"\\nAccuracy on budget dataset: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict time: 3.058608183896922 seconds\n"
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "# Generate predictions: predictions\n",
    "predictions = pl.predict_proba(holdout)\n",
    "end = timer()\n",
    "print('predict time: {} seconds'.format(end - start))\n",
    "\n",
    "prediction_df = pd.DataFrame(columns=pd.get_dummies(df[LABELS], prefix_sep='__').columns, \n",
    "                             index=holdout.index,\n",
    "                             data=predictions)\n",
    "\n",
    "prediction_df.to_csv('sub2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "yhat_train = pl.predict(X_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Unlikely that we're overfit with this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score on training set: 0.5489379243150064\n"
     ]
    }
   ],
   "source": [
    "print('accuracy score on training set: {}'.format(accuracy_score(y_train, yhat_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submitted model with bigrams, dimension reduction and scaling - score is 0.8174.  Not as good as first model, but it's actually pretty close.\n",
    "\n",
    "It may be that tossing all those features (there are 29k of them) is overdoing it.  Since it's all wrapped in the pipeline, it's hard to know.  Also, maybe not the best choice of dimension reduction.  \n",
    "\n",
    "It's pretty hard to know what helps without a local scorer (hint, hint)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement interaction modeling in scikit-learn\n",
    "It's time to add interaction features to your model. The PolynomialFeatures object in scikit-learn does just that, but here you're going to a custom interaction object, SparseInteractions. Interaction terms are a statistical tool that lets your model express what happens if two features appear together in the same row.\n",
    "\n",
    "SparseInteractions does the same thing as PolynomialFeatures, but it uses sparse matrices to do so. You can get the code for SparseInteractions at this GitHub Gist.\n",
    "\n",
    "PolynomialFeatures and SparseInteractions both take the argument degree, which tells them what polynomial degree of interactions to compute.\n",
    "\n",
    "You're going to consider interaction terms of degree=2 in your pipeline. You will insert these steps after the preprocessing steps you've built out so far, but before the classifier steps.\n",
    "\n",
    "Pipelines with interaction terms take a while to train (since you're making n features into n-squared features!), so as long as you set it up right, we'll do the heavy lifting and tell you what your score is!\n",
    "\n",
    "##### INSTRUCTIONS\n",
    "\n",
    "Add the interaction terms step using SparseInteractions() with degree=2. Give it a name of 'int', and make sure it is after the preprocessing step but before scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparse_interactions import SparseInteractions\n",
    "\n",
    "# Instantiate pipeline: pl\n",
    "pl = Pipeline([\n",
    "        ('union', FeatureUnion(\n",
    "            transformer_list = [\n",
    "                ('numeric_features', Pipeline([\n",
    "                    ('selector', get_numeric_data),\n",
    "                    ('imputer', Imputer())\n",
    "                ])),\n",
    "                ('text_features', Pipeline([\n",
    "                    ('selector', get_text_data),\n",
    "                    ('vectorizer', CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC,\n",
    "                                                   ngram_range=(1, 2))),  \n",
    "                    ('dim_red', SelectKBest(chi2, chi_k))\n",
    "                ]))\n",
    "             ]\n",
    "        )),\n",
    "        # Now add the interaction features to the selected feature set\n",
    "        ('int', SparseInteractions(degree=2)),\n",
    "        ('scale', MaxAbsScaler()),\n",
    "        ('clf', OneVsRestClassifier(LogisticRegression()))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing the hashing trick in scikit-learn\n",
    "In this exercise you will check out the scikit-learn implementation of HashingVectorizer before adding it to your pipeline later.\n",
    "\n",
    "As you saw in the video, HashingVectorizer acts just like CountVectorizer in that it can accept token_pattern and ngram_range parameters. The important difference is that it creates hash values from the text, so that we get all the computational advantages of hashing!\n",
    "\n",
    "##### INSTRUCTIONS\n",
    "\n",
    "Import HashingVectorizer from sklearn.feature_extraction.text.\n",
    "Instantiate the HashingVectorizer as hashing_vec using the TOKENS_ALPHANUMERIC pattern.\n",
    "Fit and transform hashing_vec using text_data. Save the result as hashed_text.\n",
    "Hit 'Submit Answer' to see some of the resulting hash values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0\n",
      "0  0.377964\n",
      "1  0.755929\n",
      "2  0.377964\n",
      "3  0.377964\n",
      "4  0.235702\n"
     ]
    }
   ],
   "source": [
    "# Import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "# Get text data: text_data\n",
    "text_data = combine_text_columns(X_train)\n",
    "\n",
    "# Create the token pattern: TOKENS_ALPHANUMERIC\n",
    "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)' \n",
    "\n",
    "# Instantiate the HashingVectorizer: hashing_vec\n",
    "hashing_vec = HashingVectorizer(token_pattern=TOKENS_ALPHANUMERIC)\n",
    "\n",
    "# Fit and transform the Hashing Vectorizer\n",
    "hashed_text = hashing_vec.fit_transform(text_data)\n",
    "\n",
    "# Create DataFrame and print the head\n",
    "hashed_df = pd.DataFrame(hashed_text.data)\n",
    "print(hashed_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1048576"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### that's a lot of features\n",
    "hashing_vec.n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4473948,)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashed_text.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the winning model\n",
    "You have arrived! This is where all of your hard work pays off. It's time to build the model that won DrivenData's competition.\n",
    "\n",
    "You've constructed a robust, powerful pipeline capable of processing training and testing data. Now that you understand the data and know all of the tools you need, you can essentially solve the whole problem in a relatively small number of lines of code. Wow!\n",
    "\n",
    "All you need to do is add the HashingVectorizer step to the pipeline to replace the CountVectorizer step.\n",
    "\n",
    "The parameters non_negative=True, norm=None, and binary=False make the HashingVectorizer perform similarly to the default settings on the CountVectorizer so you can just replace one with the other.\n",
    "\n",
    "##### INSTRUCTIONS\n",
    "\n",
    "Import HashingVectorizer from sklearn.feature_extraction.text.\n",
    "Add a HashingVectorizer step to the pipeline.\n",
    "Name the step 'vectorizer'.\n",
    "Use the TOKENS_ALPHANUMERIC token pattern.\n",
    "Specify the ngram_range to be (1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the hashing vectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "# Instantiate the winning model pipeline: pl\n",
    "pl = Pipeline([\n",
    "        ('union', FeatureUnion(\n",
    "            transformer_list = [\n",
    "                ('numeric_features', Pipeline([\n",
    "                    ('selector', get_numeric_data),\n",
    "                    ('imputer', Imputer())\n",
    "                ])),\n",
    "                ('text_features', Pipeline([\n",
    "                    ('selector', get_text_data),\n",
    "                    ('vectorizer', HashingVectorizer(token_pattern=TOKENS_ALPHANUMERIC,\n",
    "                                                     non_negative=True, norm=None, binary=False,\n",
    "                                                     ngram_range=(1,2))),\n",
    "                    ('dim_red', SelectKBest(chi2, chi_k))\n",
    "                ]))\n",
    "             ]\n",
    "        )),\n",
    "        ('int', SparseInteractions(degree=2)),\n",
    "        ('scale', MaxAbsScaler()),\n",
    "        ('clf', OneVsRestClassifier(LogisticRegression(), n_jobs=-1))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### That's the final model in the course.  Fit and see how it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saus\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n",
      "C:\\Users\\saus\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit time: 3437.3036388351634 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saus\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy on budget dataset:  0.7799762663169071\n"
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "# Fit to the training data\n",
    "pl.fit(X_train, y_train)\n",
    "end = timer()\n",
    "print('fit time: {} seconds'.format(end - start))\n",
    "\n",
    "# Compute and print accuracy\n",
    "accuracy = pl.score(X_test, y_test)\n",
    "print(\"\\nAccuracy on budget dataset: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3484 seconds is 58.06666666666667 minutes.\n"
     ]
    }
   ],
   "source": [
    "print('3484 seconds is {} minutes.'.format(3484/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### That took a long time.  Accuracy is better.  Now submit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saus\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict time: 34.867701128551744 seconds\n"
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "# Generate predictions: predictions\n",
    "predictions = pl.predict_proba(holdout)\n",
    "end = timer()\n",
    "print('predict time: {} seconds'.format(end - start))\n",
    "\n",
    "prediction_df = pd.DataFrame(columns=pd.get_dummies(df[LABELS], prefix_sep='__').columns, \n",
    "                             index=holdout.index,\n",
    "                             data=predictions)\n",
    "\n",
    "prediction_df.to_csv('subm4.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Final model as presented in course scores 0.8893.  This is considerably worse than much simpler early model.  Disappointing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thoughts\n",
    "\n",
    "* I don't know if my random forest is overfitted.  It's worth finding out.  My thinking is the scoring metric is hyper-sensitive to wrong answers (especially confident ones) and I bet RFC is damn confident.  Hyper-parameter tuning can do a lot, especially if I have a good metric.\n",
    "\n",
    "* I want a local loss metric that works like the scoring metric.  Turning around submissions is too time consuming (although if the fit time is on the order of an hour it going to be tough to make progress no matter what).\n",
    "\n",
    "* The best scoring model was the 2nd submission and I think improving that is perhaps the quickest way to progress.  Presented techniques do not improve scored performance.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
